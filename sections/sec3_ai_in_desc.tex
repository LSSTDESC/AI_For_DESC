\newpage
\section{The Current Landscape of ML Across DESC Science}
\label{sec3:use_case_for_aiml}

The science goals of \acrshort{desc} place unusually stringent demands on statistical methodology. Extracting percent-level constraints on dark energy, dark matter, and tests of gravity from Rubin \acrshort{lsst} data requires not only exquisite control of observational systematics, but also analysis pipelines that can efficiently exploit information distributed across billions of galaxies, multiple probes, and heterogeneous data modalities (images, catalogs, time series, simulations). While \acrshort{ai} (in the sense of \acrshortpl{llm} and agents) has not yet significantly started to impact DESC, \acrshort{ml} is already embedded in many of these workflows and its importance will only grow as analyses become more ambitious and data volumes increase.

In this section, we survey existing intersections of ML methods with DESC science, organized by application area, including photometric redshifts, strong and weak lensing, and galaxy clusters; supernovae and transients; cosmological theory and simulations; deblending; and shape measurement. Each subsection highlights both current capabilities and open challenges, to clarify where targeted methodological investment will yield the most significant scientific returns for DESC.

\textit{A note on reading this section:} In the subsections that follow, each DESC science application is accompanied by a summary box highlighting the ML methodologies it employs and the challenges it faces. As you read through these applications, pay attention to how often the same methods (\acrshort{acr:sbi}, differentiable programming, \acrshort{nde}) and the same challenges (covariate shifts, uncertainty quantification, scalability, data sparsity, metrics) recur across seemingly disparate topics. This reveals a fundamental pattern: a small set of transversal methodologies and challenges cuts across the entirety of DESC's ML applications, as illustrated in Figure~\ref{fig:chord-diagram}. This pervasive transversality has direct implications for how DESC should organize its AI/ML efforts, which we synthesize at the end of this section and develop further in Section~\ref{sec4:aiml_research}.


\subsection{Photometric redshifts} \label{sec3:photo-z}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{gaussian-process}, \meth{neural-density-estimation}, \meth{som}, \meth{transformer}, \meth{hierarchical-bayes}, \meth{emulator}, \meth{neural-surrogate}, \meth{diffusion-model} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{uq}, \challenge{scalability}, \challenge{metrics} \\
\themebullet \themekey{Opportunities.} Multi-survey training, simulation infrastructure, hierarchical inference
\end{ThemeBoxA}

The inference of \acrshortpl{photoz} represents a foundational challenge for \acrshort{lsst}, where the vast majority of tens of billions of detected galaxies will lack spectroscopic redshift measurements due to both observational time constraints and the intrinsic faintness of the sample. Photometric redshifts are derived by establishing empirical or physically motivated mappings between broadband photometry (including colors, magnitudes), morphology, and redshift. This process is fundamentally limited by our incomplete knowledge of galaxy \acrfullpl{sed}, the spatial and temporal variations of stellar populations in galaxies, and the nature and distribution of attenuating dust. However, the accuracy and reliability of photo-$z$ estimation is critical across virtually all extragalactic LSST science cases, including weak gravitational lensing, large-scale structure, galaxy cluster cosmology, and supernova surveys. To achieve the \acrshort{desc} goals for constraining the dark energy equation of state, calibration of photo-$z$ estimates must reach the $0.002 \times (1+z)$ level for the first year of LSST data \citep{v1DESC-SRD}. Achieving these benchmarks necessitates not only accurate point predictions but also \textit{well-calibrated uncertainty quantification}, motivating an emphasis on probabilistic methods. In addition, model benchmarking requires a unified framework for per-galaxy photo-$z$ algorithms, ensemble calibration algorithms, mock data generation, and performance evaluation. These tasks are fulfilled by the \acrlong{rail} \citep[\acrshort{rail};][]{RAIL_2025}, a photo-$z$ library developed by \acrshort{lincc} and DESC. 


\paragraph{Supervised Photo-$z$ Estimation} The empirical approach to  photo-$z$ inference is to learn a mapping from observed broadband photometry or imaging to redshift, leveraging spectroscopic training samples. From catalog-level photometry, empirical regressors such as random forests in \acrlong{tpz}~\citep[\acrshort{tpz};][]{carrasco2013tpz}, gradient boosting machines \citep[\href{https://github.com/rizbicki/FlexCoDE}{\tt FlexZBoost};][]{izbicki2017converting,2020Dalmasso_FlexZBoost}, and Gaussian processes~\citep[\acrshort{gpz};][]{almosallam2016gpz} have demonstrated competitive performance by constructing mappings from color-magnitude space to redshift. Neural networks, including both fully connected and specialized architectures, have proven particularly adept at capturing complex relationships in high-dimensional photometric data \citep{Collister2004}. On the other hand, nearest-neighbor approaches such as \acrlong{knn} and \acrlong{cmnn}~\citep[\acrshort{knn} and \acrshort{cmnn};][]{graham2017photometric} take the training as a reference sample and compute the average redshift of neighbors of the target galaxy. In a similar way, \acrlong{dnf}~\citep[\acrshort{dnf};][]{2016DeVicente_DNF} performs a regression on the neighborhood, which allows the construction of a local linear model for each galaxy. \\ 
Contemporary approaches have evolved from point estimators to full probabilistic models capable of capturing the full conditional distribution $p(z \mid \text{photometry})$, with \acrshort{nde} techniques \citep[e.g., \href{https://github.com/jfcrenshaw/pzflow}{\tt PZFlow};][]{2024Crenshaw_PZFlow} enabling flexible, well-calibrated redshift \acrfullpl{pdf} via maximum likelihood training. Complementing catalog-based methods, image-based inference circumvents the information bottleneck imposed by aperture photometry by operating directly on multi-band pixel data, delegating feature extraction to deep neural networks that leverage morphology and spatial structure inaccessible to catalogs. The \acrfull{deepdisc} framework \citep{Merz23DeepDISC,Merz25DeepDISCpz} exemplifies this approach, integrating object detection, segmentation, and redshift estimation into a unified pipeline using \acrlong{mvit} \citep[\acrshort{mvit};][]{mvitv2} as the backbone feature extractor coupled with mixture density networks for probabilistic PDF estimation. Beyond redshift estimation, tomographic bin assignment for 3$\times$2pt analyses is itself amenable to supervised learning; the DESC Tomography Optimization Challenge \citep{Zuntz:2021} benchmarked multiple algorithms for this task, and subsequent work has shown that neural network classifiers can identify galaxies likely to be correctly binned, improving cosmological constraints \citep{Moskowitz:2023}. \\
Despite their sophistication, \textit{all supervised \acrshort{ml} methods remain fundamentally limited by the quality and representativeness of their spectroscopic training samples}: spectroscopic incompleteness, magnitude-limited surveys, and selection biases induce systematic offsets and distortions in the learned photo-$z$ mapping, particularly at faint magnitudes and high redshifts where spectroscopic follow-up is most incomplete \citep{newman2022}. This represents the main challenge for photo-$z$ today and has motivated dedicated calibration strategies.
A further problem is the ``implicit prior'' imposed by each photo-$z$ method \citep{schmidt2020}.
These priors, which have a large impact on photo-$z$ estimates, are opaque and difficult to quantify, making it difficult to compare and combine photo-$z$ posteriors provided by different methods. %All of the aforementioned algorithms are implemented in the Redshift Infrastructure Assessment Layers (RAIL) \citep{RAIL_2025}. Six supervised learning algorithms in RAIL (FlexZBoost, kNN, CMNN, GPz, DNF, and TPz) were applied to Rubin Data Preview 1 \citep{Zhang2025_dp1photoz} and achieved promising results on its test set. 


\paragraph{Calibration Strategies to Account for Covariate Shifts}  \Acrfullpl{acr:som} have emerged as the preeminent unsupervised learning technique for diagnosing and mitigating the biases caused by covariate shifts by performing non-linear dimensionality reduction of photometric feature vectors onto a discrete two-dimensional grid \citep{Masters2015}. SOM-based calibration approaches, such as those deployed in \acrfull{des} Year 3 \citep{Myles2021} and \acrfull{kids} analyses \citep{wright2020a, wright2020b, wright2025, vanDenBusch2022}, directly assign photometric galaxies the empirical redshift distribution of spectroscopic galaxies in their SOM cell while down-weighting or even rejecting regions of color space poorly represented in the spectroscopic catalog. More sophisticated SOM-guided data augmentation strategies selectively populate under-represented SOM cells with simulated galaxies from mock catalogs improving ML model performance where spectroscopic coverage is deficient \citep{Moskowitz2024, Zhang2025}. An alternative approach to SOM for covariate shift mitigation (and without using data augmentation) is represented by stratification by the propensity score (defined as the probability of a covariate vector to be admitted as part of the training set) of both training and target data. Within each propensity score group, supervised photo-$z$ can proceed with any method of choice. This \acrfull{stratlearn} approach is theoretically guaranteed (under some conditions) to cancel covariate shift~\citep{Autenrieth_2023}. It has demonstrated state-of-the-art performance in the \acrshort{plasticc} \citep{PLAsTiCC1810.00001} \acrshort{snia} classification challenge, a factor of $\sim 2$ improvement in photo-$z$ calibration from the cosmic shear KiDS+VIKING-450 dataset~\citep{Autenrieth_2024} and a reduced fraction of catastrophic errors and one order of magnitude improvement of the bias for simulated photo-$z$ reconstruction~\citep{Moretti_2025}.

\paragraph{Hybrid Template-Based Estimators}
In contrast to empirical photo-$z$ estimators, a broad class of ``template-based'' photo-$z$ estimators \citep[e.g.,][]{eazy,lephare} attempt to circumvent the problem of covariate shift using physical models of galaxy SEDs.
These estimators trade the problem of covariate shift for the problem of model misspecification.
Hybrid methods, however, attempt to combine the strengths of empirical and template-based estimators by deriving SED templates in a physics-informed, data-driven manner \citep{budavari2000,Csabai2000}.
These models have been shown to deliver higher-quality photo-$z$ estimates than traditional template-based estimators while suffering less from covariate shift than pure empirical methods \citep{crenshaw2020,li2025}.
They do not perform as well in-distribution as pure empirical methods, however, and still rely on spectroscopic calibration sets.
It may be possible to remedy these defects by implementing hybrid, physics-informed models in deep learning frameworks to enable self-supervised learning without reliance on spectroscopic data sets \citep{2021Boone_ParSNIP}. 



\paragraph{Population-Level Hierarchical Forward Modeling} Traditional photo-$z$ workflows estimate individual galaxy redshifts and aggregate these posteriors to derive population-level quantities such as ensemble redshift distributions $n(z)$ -- a computationally expensive bottom-up approach prone to biases when combining noisy individual posteriors \citep{Leistedt2016, Malz2021, Malz2022, Alsing2023}. Population-level inference inverts this paradigm by directly targeting the population distribution $P(\boldsymbol{\theta})$ over redshift and physical galaxy parameters (stellar mass, star formation rate, metallicity) as the primary inference objective, leveraging the collective constraining power of the entire photometric dataset while naturally incorporating physical priors on galaxy evolution. These methods rely on forward modeling: generating synthetic photometry from physical parameters via \acrlong{sps} \citep[\acrshort{sps}; for reviews, see, e.g.,][]{Conroy:2013, Iyer:2026} models and comparing the distribution of model photometry to observed data. Classical SPS calculations---as implemented by, e.g., \acrlong{fsps} (\acrshort{fsps}; \citealp{Conroy:2009, Conroy:2010, ConroyGunn:2010}) and the \href{https://github.com/bd-j/prospector/}{\tt prospector} model family \citep{Leja:2017, Johnson:2021, Wang:2023}---are computationally prohibitive for large samples, motivating neural network emulators like \href{https://github.com/justinalsing/speculator/tree/master}{\tt speculator} \citep{SPECULATOR} that achieve $\sim10^{3}$--$10^{4}\times$ speedups with negligible accuracy loss. The \href{https://github.com/Cosmo-Pop/pop-cosmos}{\tt pop-cosmos} framework \citep{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025} exemplifies this approach: it defines a probability distribution over a 16-dimensional SPS parameterization using a score-based diffusion model calibrated on $\sim$420,000 galaxies from COSMOS2020 \citep{Weaver:2022} with 26-band photometry spanning deep \acrfull{uv} to mid-\acrfull{ir}. 
%The \href{pop-cosmos}{https://github.com/Cosmo-Pop/pop-cosmos} 
This model enables direct estimation of tomographic redshift distributions, and, when used as a data-driven prior in SED fitting, highly accurate individual galaxy redshift inference.

\paragraph{Benchmarking and Evaluation Frameworks}
As \acrshort{ai} methods become increasingly central to cosmological analyses, it is critical to develop robust frameworks for testing and validation that ensure reproducibility and enable systematic comparison of different approaches.
For this purpose, the DESC has developed RAIL, an open-source, Python-based framework to support large-scale photometric-redshift workflows for LSST.
RAIL is a library that hosts many per-galaxy algorithms (e.g., \href{https://github.com/rizbicki/FlexCoDE}{\tt FlexZBoost}, \href{https://github.com/ltoribiosc/DNF_photoz}{\tt DNF}, \href{https://github.com/jfcrenshaw/pzflow}{\tt PZFlow}, \href{https://github.com/grantmerz/deepdisc}{\tt DeepDISC}), ensemble calibration algorithms, (e.g., self-organizing maps). RAIL also provides comprehensive infrastructure that
(i) supplies a unified \acrshort{api} and modular pipeline stages to train, apply, and compare a broad range of redshift estimators (catalog-based, image-based, probabilistic),
(ii) embeds evaluation modules and metrics for both individual-galaxy redshift and ensemble PDFs ~\citep{RAIL_2025}, 
(iii) generates realistic mock data for supervised learning algorithms by applying photometric noise, reference redshift selection, and error to an input catalog, and (iv) enables data challenges to test the robustness of photo-z estimators to a wide array of systematic errors. RAIL's standardized framework facilitates reproducible results and fair benchmarking across different methods, essential for validating AI techniques in preparation for LSST data.


\subsection{Strong Lensing}
% \note{Text written by Stefan Schuldt, with help from Clecio de Bom, Sydney Erickson, Martin Millon and Padma Venkatraman. Comments are welcome!}
\label{sec:strong_lensing}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{gaussian-process}, \meth{cnn}, \meth{rnn}, \meth{transformer}, \meth{sbi}, \meth{diffusion-model}, \meth{variational-inference}  \\
\themebullet \themekey{Challenges.} \challenge{data-sparsity}, \challenge{covariate-shift}, \challenge{uq} \\
\themebullet \themekey{Opportunities.} Multi-survey cross-matching (Roman+LSST+Euclid), population-level inference, automated discovery, subhalo constraints from anomalous flux ratios
\end{ThemeBoxA}

Strong gravitational lensing is a rare astrophysical phenomenon where the light of a distant object, the source, is deflected by the gravity of an intervening structure, the lens, forming multiple images of the background source. In galaxy-galaxy strong lensing, both the source and the lens are individual galaxies, while on larger scales, the lens could range from a group to an entire galaxy cluster. Despite their rarity (a result of the stringent alignment required between source, lens, and observer), lensed systems are powerful cosmological probes that can constrain dark energy and probe dark matter on sub-galactic scales. Since elliptical galaxies dominate the deflector population, strong lenses also enable studies of their mass profiles, stellar content, and dark matter halos. Furthermore, lensing magnification enables the study of high-redshift sources, offering insights into early galaxy evolution \citep{1992grle.book.....S}.

\acrshort{lsst} will be transformative for strong lensing science. Forecasts predict the discovery of ${\sim}170{,}000$ systems \citep{collett15}, two orders of magnitude more than currently known. This large sample will provide the statistical power required for precision cosmology: a combination of time-delay lenses and large samples of static lenses has been shown to enable competitive dark energy measurements \citep{shajib_SL_2025}. The sample will also include significant numbers of currently rare systems, such as double-source-plane lenses, lensed supernovae, and cluster-scale lenses with multiple background sources. \acrshort{lsst}'s six-filter imaging (see \autoref{sec3:photo-z}) will enable photo-$z$ estimation for both lens and source populations, as well as classification of time-delay lenses.

One particularly powerful application is time-delay cosmography, which uses strongly lensed transients to measure the Hubble constant ($H_0$)~\citep[e.g.,][]{tdcosmo2025}. This approach yields a geometrical measurement independent of both the local distance ladder \citep[e.g., SH0ES;][]{riess22} and early-universe measurements from the \acrlong{cmb} \citep[\acrshort{cmb}; see, e.g.][]{planck20}. \acrshort{lsst} will provide time-domain coverage for ${\sim}100\times$ more systems than current surveys \citep{wojtak2019, goldstein2019, arendse2024, erickson_de_2025, abe_2025}, dramatically increasing the sample of lensed \acrfullpl{sne} and \acrfullpl{agn} available for cosmography. \acrshort{ai}/\acrshort{ml} models have been proposed for time-delay estimation from light curves, particularly kernel-based methods \citep[e.g.][]{cuevas06,cuevas2010uncovering,otaibi16}.

\paragraph{Supervised Detection in the Low Data Regime} Given the rarity of strong lensing events, identifying them among billions of cutouts is inherently challenging for visual inspection in wide-field surveys. While early automated detection approaches relied on curvature-based features \citep{estrada2007systematic} and arc-characterizing descriptors \citep{de2012metodo, bom2017neural}, the advent of \acrfullpl{acr:cnn} led to state-of-the-art performance in lens finding \citep{2017MNRAS.472.1129P, 2018A&A...611A...2S, 2019MNRAS.482..807P, 2018MNRAS.473.3895L}. Building on this progress, \citet{metcalf19} launched a lens-finding challenge using Bologna Lens Factory simulations based on the Millennium data \citep{Lemson2006}, showing that LSST-like ground-based multi-band images are well suited for this task. A subsequent Euclid-like challenge produced a winning algorithm combining multi-resolution CNNs \citep{bom2022developing}, later validated on real data by \citet{melo25} using Legacy Survey and \acrfull{hst} images to mimic LSST--\textit{Euclid} synergy. Leveraging multiple networks classifications as ensemble lens classifiers showed improved results over a single network classifier \citep[e.g.,][]{andika23, schuldt23b, gonzalez25}, while \cite{holloway_2024} incorporated citizen science annotations to enhance the performance. Because too few real lenses exist for supervised training, realistic mock datasets are essential. Works such as \citet{2017MNRAS.472.1129P} or \citet{schuldt21} proposed simulating only the lensing effect on real galaxy images, a practice now standard in the ongoing LSST \acrshort{desc} and \acrshort{slsc} challenge (Bom et al., in prep.). In preparation for LSST, \acrfull{hsc} data (\citealp{aihara18}; with similar filters and pixel scale) have been used to develop and compare models \citep[see e.g.,][]{shu22, andika23, canameras24, jaelani24, more24}. While early efforts focused on simple CNN or \acrfull{resnet} architectures, more advanced architectures such as vision transformers have also been applied \citep{2025Gonzalez_ViTLenses}. Foundation models such as Zoobot \citep{ZoobotRelease2023} have also achieved strong results on \textit{Euclid} imaging \citep{walmsley25, lines25}, and will soon be adopted for LSST (see Sect.~\ref{sec:foundation_models}). Finally, ML methods are now expanding beyond galaxy-scale lenses to systems involving entire clusters \citep[][Bazzanini et al., in prep.]{schuldt25} and galaxy–galaxy lenses within clusters \citep{angora23}. There has also been work on detecting time-variable strong lenses: \citet{morgan2021deepzipper} developed DeepZipper, which integrates \acrshort{lstm} networks with \acrshortpl{acr:cnn} to jointly process temporal and spatial information for identifying strongly lensed supernovae in time-domain surveys.

\paragraph{Simulation-Based Inference (SBI)} Beyond lens finding, \citet{hezaveh17} pioneered the use machine learning models to predict characteristics of strong lensing systems. Specifically, \citet{hezaveh17} showed that simple CNNs can be used to predict parameters of the lens (Einstein radius, position, ellipticity and angle) from images from \textit{HST} with a precision comparable to that of traditional methods. \citet{Perreault:2017} proposed using approximate \acrfullpl{bnn} to obtain calibrated estimations of the marginal posterior of these lens parameters, an approach applied to \acrfull{alma} observations in~\citet{Morningstar2018}. Such ML estimates can also be used as starting points for classical model fitting methods \citep[e.g.][]{2025arXiv250315329E}, providing significant speedups. \citet{Legin2021, Legin2023} compared this approach to \acrfull{nle}, demonstrating potential for better calibration in 2-stage \acrshort{acr:sbi} methods. In~\cite{Poh2025}, it was demonstrated that BNNs and \acrfull{acr:npe} can be used to infer parameters that describe the lens system even in ground-based \acrshort{des}-like imagining. In subsequent years, significant progress was made in using HSC images to prepare for LSST \citep[e.g.,][]{pearson19, schuldt21, gentile23, schuldt23a, schuldt23b, gawade25}. Following earlier work by \citet{Park2021,Wagner-Carena2021}, \citet{erickson25} applied (sequential) NPE within a hierarchical framework to model strongly lensed quasars, testing on real systems discovered by DES and followed-up with \textit{HST} high-resolution imaging, and \citet{venkatraman_2025} applied hierarchical NPE modeling to simulated LSST $i$-band coadds.  Ongoing DESC work examines how modeling an uncertain sample of static galaxy-galaxy lenses with ML enables new cosmological constraints (Holloway et al. in prep.), leveraging the method demonstrated by \citet{li_gg_SL}. And in~\cite{Jarugula2024}, authors presented a scalable approach for inferring the dark energy equation-of-state parameter from a population of strong gravitational lens images using \acrfull{nre}.
%Furthermore, \citet{schuldt23b} made a first direct comparison between the model predicted quantities obtained in the classical way without machine learning using real HSC images. The classical procedure requires dedicated software that fits typically profiles to the observed image by minimizing the difference though Monte Carlo Markov Chain sampling, or similar sampling techniques. This makes it computationally very expensive and requires also significant human input time, such that it is not scalable to the full sample expected by LSST. 
%Finally, \citet{filipp25} applied a sequential NPE approach to detect dark-matter clumps in strong-lensing systems, exploiting the sensitivity of lensing to total mass.
\citet{filipp25} investigated the robustness of neural ratio and neural posterior estimators to distributional shifts for dark matter substructure inference from strong lensing, finding that these methods can exhibit significant biases when the test data deviates from the training distributions. Initial tests of using domain adaptation~\citep{Farahani2020ABR} for improving robustness of CNNs and neural posterior estimators when predicting characteristics of strong lens systems in the presence of distributional shift were performed in~\cite{Swierc2023,Swierz2024} and~\cite{Agarwal2024}.


 
\paragraph{High Dimensional Inverse Problem for Lens Modeling}  The task of lens modelling, that is, predicting surface brightness of background sources and density maps of foreground lenses is, in its simplest form, a non-linear inverse problem involving a handful of parameters ($\sim 10-20$). However, as the quality and resolution of data increases, such parametric description of lensed object becomes too simplistic, and more complex parametrization become necessary to avoid biases. An example of such a parametrization that is particularly well-adapted to ML applications are pixelated images of sources surface brightness and projected densities of lenses. Traditionally, it has been difficult to characterize appropriate priors analytically on such high-dimensional spaces~\citep[see, e.g.,][]{Suyu2006, WarrenDye2005, Birrer2015, Delaunay2009, Nightingale2018}, however, recent advances in high-dimensional inference with deep learning has made progress on this front possible. \\
An initial attempt at solving the source reconstruction problem was presented in~\cite{Morningstar2019}, and extended in ~\cite{AlexAdam2023} to enable joint modeling of generalized pixellated lens densities and sources surface brightness. However, while these models provided high-fidelity \acrfull{map} estimates, they lacked the crucial ability to quantify uncertainties. Approaches based on \acrfull{vi} have also been proposed in~\citet{Chianese2020, Karchev2022GP, Mishra-Sharma2022}.\\
Advances of, e.g., \cite{Song_2019, Ho_2020, Song:2020, Song_2021, 2022Yang_Diffusion}, have shown that generative models used as expressive, data-driven priors are a promising alternative to address this problem. \cite{adam2022posterior, Karchev2022Diffusion} used \acrfullpl{sbm} as flexible priors in an explicit inference framework to produce posterior samples of background galaxy sources. In \cite{Barco2025blindinversion}, this method was extended to allow joint sampling the source and lens parameters for smooth, parametric lenses. Such methods have been shown to alleviate known biases in lens parameters induced by misspecified traditional priors, and methods have been proposed to empirically adapt initially biased SBM priors to correct for, e.g., population-level evolution of galaxy morphologies~\citep{Barco_2025}, and to empirically extend misspecified physical models~\citep{Payot2025}. More recently, \cite{RonanSLACLenses} has shown that SBM priors can be leveraged in a Gibbs sampling scheme to reanalyze \textit{HST} data from lenses observed by \acrfull{slacs}. Ongoing challenges include increasing the sampling efficiency of these methods to allow modelling a large fraction of the strong lenses expected with LSST.

\paragraph{Leveraging LSST time-series data} LSST will generate an overwhelming number of transient alerts, making the discovery and characterization of strongly lensed short-lived transients (e.g., supernovae) both difficult and time-critical. Kernel-based methods and probabilistic machine learning models such as Gaussian processes will likely play a major role in time-delay inference for lensed quasars \citep[e.g.,][]{cuevas06, cuevas2010uncovering, hojjati14, otaibi16, tak17} and supernovae \citep[e.g.,][]{hayes24, hayes25} discovered by LSST. Temporal deep learning models will also be essential in this area. For instance, \citet{bag24} developed a model using unresolved light-curve data from difference imaging, while \citet{bag25} extended this to full multi-band time series with a 2D convolutional \acrfull{lstm} network. \citet{huber24} instead used an LSTM network to predict time delays between such transients directly from their light curves, whilst \citet{goncalves25} used an ensemble of CNNs to directly estimate $H_0$ from time series of lensed supernova images and \citet{Campeau2023} demonstrated the potential of NREs to infer $H_0$ from time delays and lens models. Beyond short-lived transients, \citet{jimenez25} modeled microlensing in lensed quasar light curves, and \citet{fagin_light_curves} introduced a latent \acrfull{sde} framework to jointly model AGN variability and transfer functions, potentially extendable to lensed AGNs for joint inference of time delays and disk parameters.

\subsection{Weak Lensing}
\label{sec3:wlss}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{sbi}, \meth{neural-compression}, \meth{differentiable-programming}, \meth{hierarchical-bayes}, \meth{diffusion-model} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{uq}, \challenge{scalability} \\
\themebullet \themekey{Opportunities.} Multi-resolution joint processing (Roman+LSST), probabilistic deblending, DeepDISC instance segmentation, physics-informed priors for galaxy morphology
\end{ThemeBoxA}

As light from background galaxies travels through the Universe, its path is deflected by the gravitational potential of foreground matter, inducing subtle shape distortions of observed galaxies that can be statistically measured. This effect, referred to as weak gravitational lensing, provides a direct probe of the total matter distribution in the Universe, making it a powerful tool for constraining cosmological parameters such as the matter density $\Omega_m$, the amplitude of matter fluctuations $\sigma_8$, and the dark energy equation of state $w$ in a \acrfull{wcdm} model.
With its unprecedented depth, image quality, and sky coverage, \acrshort{lsst} will provide the most precise mapping of the large-scale structure of the Universe to date. This level of precision has two key implications:
(1) It  presents a major opportunity to refine cosmological constraints, motivating the development of advanced inference methods that can fully exploit this high-quality data; (2) It demands rigorous control of systematic uncertainties to ensure unbiased cosmological constraints.

\paragraph{Systematics modeling / mitigation}
Systematic errors, such as imperfect shear calibration, \acrshort{photoz} uncertainties, spatially varying selection effects, and \acrfull{psf} residuals, must be well characterized. To date, the precision of current cosmological surveys has permitted the use of state-of-the-art prescriptions that capture the dominant effects of these systematics (e.g., \citealp{Mandelbaum:2018}). However, as forthcoming large-scale structure and weak-lensing data from LSST achieve substantially higher statistical precision, a more accurate and detailed characterization of these systematics will be required, at a level of complexity that renders purely analytical treatments intractable.  
\acrshort{ml} methods offer a complementary pathway by learning complex nonlinear mappings from observational features---e.g., properties of individual galaxies, local image quality metrics, PSF residuals, depth maps, shape measurement parameters--- to the resulting systematic bias or residual error \citep{Tewes:2018she,Rezaie2020,Pujol:2020wrk}. Neural network or other ML algorithms can be trained on simulated or calibration data for which the true systematic shifts are known, and can then be tuned to predict, flag or correct for the systematic effect when applied to real survey data \citep{Fluri:2022rvb}. In doing so, these methods enable rapid and flexible removal of systematic contamination from the cosmological signal, thereby yielding a cleaner, more robustly inferred signal.

\paragraph{Clean catalog construction} Systematics such as \acrlong{ia} \citep[\acrshort{ia}; e.g.,][]{Mandelbaum:2006, Mandelbaum:2011, Troxel:2015, Joachimi:2015} can be mitigated by constructing clean source and/or lens catalogs from galaxy populations where these effects are known to be negligible. Scalable inference of galaxy properties that correlate with active galaxy populations (such as \acrlong{ssfr}, \acrshort{ssfr}) can enable the construction of IA-mitigated galaxy samples. For instance, machine-learned generative priors can be leveraged (e.g., \href{https://github.com/Cosmo-Pop/pop-cosmos}{\tt pop-cosmos}; \citealp{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}) to estimate per-galaxy sSFR and construct clean catalogs of star-forming galaxies with conservative cuts based on this parameter. This approach, especially when combined with amortized \acrshort{acr:npe}, is scalable to LSST-sized datasets and is expected to outperform color-based selections, which are affected by contamination. Moreover, generative models of the galaxy population can be applied in a weak lensing context to directly infer the redshift distributions of source catalogs subject to tomographic binning and sample selection criteria, provided that the color-redshift relation is realistic and robust. This provides an alternative to approaches such as \acrshort{acr:som} calibration.

% Although morphology-based selection remains central in most photometric surveys due its robustness and interpretability, the growing availability of multi-wavelength, multi-survey data also opens the door for \acrshort{ai}-driven methods to achieve higher completeness and purity in star/galaxy/quasar discrimination. Numerous studies demonstrate that machine learning approaches can outperform traditional selection cuts, especially when color information is included \citep[e.g.][]{sevilla18, baqui21}. Other work emphasizes the value of probabilistic outputs, which enable weighted sampling instead of binary selection \citep[e.g.][]{fadely12,lopez19}. Looking ahead, fully exploiting the depth of LSST data alongside shallower surveys will require joint analysis frameworks that operate at the pixel level rather than relying solely on cross-matched catalogs.

\paragraph{Neural Compression and Simulation-Based Inference}
Traditional weak-lensing analyses follow a two-step pipeline: compress high-dimensional shear or convergence fields into summary statistics, then perform Bayesian inference on these summaries to obtain posteriors over cosmological parameters. The matter power spectrum and shear–shear correlation functions remain workhorse statistics (\acrshort{kids}-1000 cosmic shear: \citealt{Asgari2021}; \acrshort{des} Y3 cosmic shear: \citealt{Amon_2022,Secco_2022}), but in the LSST era significant non-Gaussian information will become available. This has motivated the use of higher-order moments such as the bispectrum and trispectrum \citep[e.g.,][]{desy3_moments}, as well as peak counts \citep[e.g.,][]{Marques_2024}, persistent homology \citep{prat2025}, and Minkowski functionals \citep[e.g.,][]{PhysRevD.85.103513}. While powerful, these handcrafted summaries are not guaranteed to capture all cosmological information. An alternative enabled by ML is to train neural networks that compress the maps directly into low-dimensional summaries. Several strategies have been explored in the weak lensing literature for training such networks \citep[see][for a comparison]{neural_summary_lanzieri_2025}; among these, information-theoretic criteria (such as those used in Information-Maximizing Neural Networks \citep[IMNNs;][]{2018PhRvD..97h3004C,Makinen:2021} and Variational Mutual Information Maximization \citep[VMIM;][]{Jeffrey:2021}) can yield summaries that closely approximate sufficient statistics, achieving near-optimal compression. Hybrid methods that combine physics-based summaries (e.g., power spectra) with learned summaries can leverage the strengths of both \citep{Makinen2024}. Because the likelihood of these learned summaries is unknown, \acrshort{nde} techniques such as normalizing flows are then used to approximate the posterior within a \acrshort{acr:sbi} framework \citep{Alsing:2019}. This strategy was first demonstrated on survey data in \citet{Jeffrey:2021} and subsequently applied to recent surveys \citep[e.g.,][]{jeffrey2024darkenergysurveyyear, kramsta2025}, with \citet{jeffrey2024darkenergysurveyyear} reporting more than a factor of two improvement in dark energy parameter precision compared to power spectrum inference. However, the practical limit to \acrshort{acr:sbi} is not the ability to extract information from the data through learned summaries, but rather the difficulty of producing simulations realistic enough to be compared to observations without incurring biases from model misspecification. In addition, neural summaries are notoriously difficult to interrogate: monitoring them for covariate shifts, unmodeled systematics, and failures in specific regions of the data space is challenging, thereby complicating the construction of robust null tests and diagnostic pipelines. That said, posterior predictive checks against conventional summary statistics (e.g., the power spectrum or higher-order moments) can help detect some forms of model misspecification, though they are not guaranteed to catch all issues. 
 
\paragraph{Hierarchical Bayesian Field-Level Inference} With the advent of \acrshort{gpu}-accelerated probabilistic programming, it has become feasible to model the full weak-lensing field in a hierarchical framework that links Gaussian initial conditions of the matter density to observed shear maps through an explicit forward simulation model. Proof-of-concept studies have demonstrated this approach in simplified weak-lensing settings \citep[e.g.,][]{porqueres2023fieldlevelinferencecosmicshear}, showing substantial gains in constraining power relative to power-spectrum analyses. DESC members have contributed key building blocks for such end-to-end pipelines, including differentiable lensing lightcone constructions \citep{Lanzieri_2023} and accurate, differentiable ray-tracing schemes \citep{Zhou2024}. However, scaling these methods to a full LSST analysis remains extremely challenging: the survey volume and the resolution required for the forward model place stringent demands on memory, compute, and algorithmic efficiency. Ongoing work aims at lifting this bottleneck through distributed simulations across multiple GPUs \citep{Kabalan_jaxDecomp_2025}. In parallel with full forward modeling of the large-scale structure, \acrshort{desc} members have also explored map-based hierarchical inference using lognormal fields \citep{boruah2022mapbasedcosmologyinferencelognormal, Zhou2024Prd}, which is far less computationally demanding but whose ultimate accuracy is constrained by the limitations of the lognormal approximation. As an alternative approach, DESC members have proposed using diffusion models to learn the forward model of the density field implicitly from simulations and combine this learned prior with an explicit likelihood to constrain observed shear data \citep{remy2023}, enabling fast reconstruction of high-fidelity mass maps. %Another method reconstructs tomographic mass maps directly from simulated LSST images using neural posterior estimation (White et al., in preparation). While this approach requires extending the implicit forward model to multi-band images, it is computationally feasible because it uses deep neural networks, marginalizes over nuisance variables, and avoids evaluating the likelihood.


%\newpage
%\subsubsection{Dark Matter}
%\begin{ThemeBoxA}[]
%\themebullet \themekey{Methodology.} TODO\\
%\themebullet \themekey{Challenges.} TODO\\
%\themebullet \themekey{Opportunities.} TODO
%\end{ThemeBoxA}

%ADW: Brainstorming by ADW
%star/galaxy classification (old-school, but still important!), simulation-based inference and graph neural networks for stellar stream modeling and measurement, emulators for accelerating SSI


% \newpage
\subsection{Galaxy Clusters}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{sbi}, \meth{object-detection}, \meth{cnn}, \meth{hierarchical-bayes}\\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{uq}, \challenge{scalability} \\
\themebullet \themekey{Opportunities.} Combination of imaging and catalog data, Hierarchical Modeling
\end{ThemeBoxA}

% Galaxy clusters probe the densest peaks of the universe, and formed relatively late in the history of the universe.  The evolution of their number count is therefore sensitive to dark energy.  Measurements of the galaxy cluster mass function have been the traditional approach to constrain cosmology. AI can facilitate cluster-cosmology in several stages.  
Galaxy clusters trace the most massive peaks of the matter density field and form relatively late in cosmic history, making their abundance and internal properties highly sensitive to the growth of structure and to dark energy. Cosmological constraints from clusters have traditionally relied on measurements of the cluster mass function and its redshift evolution, anchored by calibrated relations between mass and observable proxies. \acrshort{ml} methods are now entering this pipeline at multiple stages (cluster finding, mass–observable calibration, and population-level inference) offering new ways to combine imaging, catalog, and multi-wavelength data while retaining control over systematics and uncertainties.


% The first step to performing a cluster cosmology analysis is `cluster-finding'. Several non-AI algorithms exist to find galaxy clusters, including RedMaPPer \citep{rykoff2014, rykoff2016} and WAZP \citep{aguena2021} for optically selected galaxy clusters. Several deep learning-based methods for cluster finding have emerged in the SZ \citep{bonjean2020, lin2021, hurier2021, Meshcheryakov2022}, X-ray [???], and optical \citep{chan2019, grishin2023, grishin2025, tian2025}.%[Lin 2022, Grishin+23; Grishin+25; Tian+25].  
% A benefit of deep learning-based methods is that cluster-finding can operate directly in image spaces, as opposed to only the information in processed catalogs.  These models can potentially learn features in the image that are not captured in the catalog data. In one example, the You Only Look Once (YOLO; \citealp{redmon2015, redmon2016, redmon2018}) architecture was trained to identify clusters with a training sample defined by the RedMaPPer cluster catalog, but also successfully detected false negative cases, verified as true clusters in external X-ray catalogs \citep{grishin2023}. Ongoing work within DESC \citep[e.g.][]{grishin2025} will enable the application of YOLO to LSST data and joint catalogs.
\paragraph{Cluster Finding from Images and Catalogs}
The first step in cluster cosmology is robust identification of cluster candidates. Non-ML algorithms, such as \acrlong{redmapper} \citep[\acrshort{redmapper};][]{rykoff2014, rykoff2016} and the \acrfull{wazp} cluster finder \citep{aguena2021}, have been widely used to identify optically selected clusters in galaxy catalogs. In parallel, deep-learning–based cluster finders have emerged in the \acrlong{sz} \citep[\acrshort{sz}; e.g.,][]{bonjean2020, lin2021, hurier2021, Meshcheryakov2022} and optical domains \citep{chan2019, grishin2023, grishin2025, tian2025}. A key advantage of these approaches is that they can operate directly on images, rather than on pre-processed catalogs, and thus potentially exploit features (e.g., diffuse emission, subtle color–magnitude structure, environment) that are not captured in standard catalog-level summaries. For example, a \acrlong{yolo} (\acrshort{yolo}; \citealp{redmon2015, redmon2016, redmon2018}) architecture trained on RedMaPPer clusters was shown to recover not only the training sample but also previously missed systems that were later confirmed in external X-ray catalogs \citep{grishin2023}. Ongoing \acrshort{desc} work \citep[e.g.,][]{grishin2025} aims to deploy such models on \acrshort{lsst} imaging, with particular attention to domain adaptation and calibration across surveys.


% Another stage of cluster-cosmology relies on our ability to constrain galaxy cluster masses from observables, often referred to as the ``mass-observable'' relation.  This task has also seen an emergence of deep neural network based approaches, with methodology inferring masses from X-ray signatures \citep{ntampaka2019, krippendorf2024, iqbal2025}, dynamics of cluster member galaxies \citep{ho2019, ho2021, ho2022, wangthiele2025}, and the SZ signature \citep{deandres2022}.  
% A particular contribution of photometric galaxy data to cluster-based cosmology is in the weak lensing mass estimates, which is used to anchor the mass-observable relation.  The DESC tool, Cluster Mass Modeling \citep[CLMM;][]{aguena2021clmm}, currently uses traditional inference methods with MCMC to infer weak lensing masses from radial profiles that assume an underlying mass distribution model, such as the NFW profile.  Ongoing in DESC are the development and testing of likelihood implicit approaches to infer weak lensing masses, specifically an approach called Simulation Based Inference (SBI). 
\paragraph{Mass-Observable Relations and Weak-Lensing Mass Calibration}
Cosmological analyses require accurate and precise relations between cluster mass and observables (richness, SZ signal, X-ray luminosity/temperature, velocity dispersion). Deep neural networks are being explored as flexible mass estimators across multiple wavebands, including X-ray signatures \citep{ntampaka2019, krippendorf2024, iqbal2025}, the dynamics of member galaxies \citep{ho2019, ho2021, ho2022, wangthiele2025}, and SZ measurements \citep{deandres2022}. For LSST, photometric galaxy data contribute primarily through weak-lensing mass estimates that anchor mass–observable relations. The DESC \acrlong{clmm} (\acrshort{clmm}; \citealp{aguena2021clmm}) currently infers weak-lensing masses from radial shear profiles using traditional likelihoods and \acrfull{mcmc}, on the assumption of parametric mass models such as \acrlong{nfw} (\acrshort{nfw}; \citealp{nfw97}). Ongoing work within DESC explores alternative \acrshort{acr:sbi} approaches at this level, which can in principle incorporate more realistic shear profiles, complex noise, and selection effects without requiring an explicit closed-form likelihood.

% Simulation-based inference enables a computationally efficient approach to derive posteriors for relevant parameters, such as the cluster mass.  Computational efficiency becomes particularly important for analyses where we might want to simultaneously account for models of individual galaxy clusters and the broader galaxy cluster population (e.g.\ Bayesian Hierarchical Modeling).  Recent numerical experiments within indicate consistency between MCMC and SBI posteriors, provided that the SBI does not suffer from model misspecification more than MCMC (Gill et al., in prep.).
% SBI has also been recently used for direct cosmology parameter estimation from observed data vectors, such as from the CMB \citep{cole2022, lemos2023cmb}, shear two-point statistics \citep{kramsta2025, modi2025}, void lensing analysis \citep{su2025}, topological summaries \citep{prat2025}, and galaxy cluster number counts \citep{reza2022, reza2024}.  The latter of these is now being applied to DESC simulations, with a goal of constructing alternative approaches to cluster-based cosmology that can be incorporated into the DESC Cluster Cosmology Pipeline.  The DESC cluster cosmology analysis will need to incorporate more astrophysical effects and probe a larger parameter space than stage-III experiments, which is increasingly inefficient to perform with a Likelihood-based method. The SBI approach will bring improved computation flexibilities and in some cases, efficiencies, for a DESC analysis.
\paragraph{Simulation-Based Inference for Cluster Cosmology}
SBI provides a computationally efficient method for deriving posteriors for cluster- and population-level parameters directly from simulated data vectors. This is particularly attractive for analyses that must jointly model individual clusters and the cluster population via hierarchical frameworks, where traditional likelihood-based methods become increasingly costly and brittle as the parameter space and model complexity grow. Ongoing work within DESC indicates that SBI can recover cluster weak-lensing mass posteriors consistent with those from MCMC, provided that model misspecification is not worse than in the explicit-likelihood case (Gill et al., in prep.). In addition, SBI has been shown to derive relevant constraints directly from cluster counts \citep{reza2022, reza2024, zubeldia25}, and this approach is now being developed on DESC simulations as an alternative, simulation-native pathway to cluster cosmology that can be integrated into the DESC Cluster Cosmology Pipeline. Compared with Stage-III experiments, DESC cluster analyses will require richer astrophysical modeling and the exploration of larger parameter spaces; SBI offers the algorithmic flexibility and, in many regimes, the computational efficiency required to meet these demands.

% \newpage
\subsection{Supernova Cosmology and Transients}
\label{sec3:td}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{hierarchical-bayes}, \meth{rnn}, \meth{transformer}, \meth{active-learning}, \meth{ensembles}, \meth{sbi}, \meth{anomaly-detection}, \meth{vae}, \meth{gaussian-process}\\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{data-sparsity}, \challenge{scalability}, \challenge{uq} \\
\themebullet \themekey{Opportunities.} PLAsTiCC/ELAsTiCC simulation infrastructure, DESC leadership in alert broker integration (ALERCE, Fink, ANTARES)
\end{ThemeBoxA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - methodology
% - challenges
% - unique opportunities
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% BROAD MOTIVATION/SCIENCE %%%% Photometric classification of SNe Ia necessary to do cosmological inference with Rubin data. LSST will drive discovery rates, population-level studies of rare transients. Methods for large-scale inference are sorely needed. Triaging is still necessary for follow-up classification/science.

%%%% TECHNIQUES %%%%
% - Partial-phase classification and active learning (e.g., RESSPECT) can help for real-time triaging.
% RNNs/transformers 
% ensemble methods for UQ
%   - Challenges for photometric SN~Ia cosmology with AI: distribution shifts/selection bias, use of simulations, uq

%%%% opportunities %%%%

% - DESC's leadership in PLAsTiCC/ELAsTiCC drove the infrastructure to model these observations, now we have good simulations we can take advantage of.

% - Many alert brokers in various stages of AI integration; they're very involved in DESC, and so DESC can uniquely connect data streams to algorithms. 

\acrshort{lsst} is expected to detect $\sim$10 million transient and variable objects each night, a thousand-fold increase over current surveys. The sheer volume and cadence of detections renders traditional spectroscopic classification infeasible for most events. This presents a major bottleneck to the identification of pure \acrshort{snia} samples for cosmological distance measurements, and to constraining the explosion physics of populations of rare and novel phenomena for the first time. To achieve reliable cosmological constraints and meet \acrshort{desc} goals of reducing the systematic uncertainties from light curve modeling below 3\% of those obtained from SALT2 \citep{2007Guy_SALT2}, analysis techniques demand \textit{well-calibrated uncertainty quantification}, \textit{adaptive and scalable performance}, and \textit{robustness to covariate shifts and data corruption}.


\paragraph{Spectrophotometric Modeling} SNe~Ia are broadly homogeneous and viable standard candles, but diversity in their spectro-temporal properties and persistent host-dependent effects \citep{2006ApJ...648..868S,2010MNRAS.406..782S,2010ApJ...722..566L,2010ApJ...715..743K,2013ApJ...764..191H} still limit standardization precision. Modern \acrshort{ml} approaches to standardization now focus on data-driven, differentiable, and multi-modal models rather than hand-engineered linear corrections. Already progress in these directions can be seen in modeling using probabilistic auto-encoders~\citep{2022ApJ...935....5S} and \acrlongpl{acr:vae} (\acrshortpl{acr:vae}; e.g., \href{https://github.com/LSSTDESC/parsnip}{\texttt{ParSNIP}}~\citealp{2021Boone_ParSNIP}), that predict time-evolving \acrshortpl{sed} from light curves, and use a differentiable forward model to compare in observation space. High quality, well-calibrated data has been instrumental in these endeavors, which can be augmented by the LSST/Rubin samples; however, strategies that account for the shifts induced by calibration errors must still be investigated. 

More recent efforts involve ML models as emulators \citep[e.g.][]{chen2020, kerzendorf2021, magee2024} for radiative transfer codes such as TARDIS~\citep{kerzendorf2014}, that can be used to infer \emph{physical parameters} such as the total luminosity, nickel mass, the composition and the asymmetry of the ejecta given multi-modal inputs such as light curve, time-series spectroscopy, and Rubin and high-resolution space-based imaging (e.g.\ from the \acrshort{esa} \textit{Euclid} mission and the \textit{Nancy Grace Roman Space Telescope}). These physically motivated emulators are also capable of several tasks that would have previously required individual specialized models. These include predicting the future evolution of \acrshort{sne} of all types, classifying spectra while being agnostic to the imbalance in extant training samples, and can help distinguish SN Ia from impostors that might otherwise contaminate the cosmological sample, as well as actively schedule follow-up spectroscopy.

\paragraph{Photometric Classification} The methodological evolution of photometric classifiers from feature-based approaches~\citep[e.g.][]{2018Narayan_ANTARES} to end-to-end learning has been driven by the challenge of processing irregular, heteroskedastic observations: \href{https://github.com/LSSTDESC/snmachine}{\tt SNmachine} \citep{SNmachine2016} leveraged a range of feature sets, from physics-based through to non-parametric approaches, coupled with a variety of traditional ML techniques to achieve high classification accuracy; \href{https://github.com/helenqu/scone}{\tt SCONE}'s Gaussian process interpolation \citep{SCONE:2021} creates regular 2D representations from sparse observations (see also \href{https://github.com/kboone/avocado}{\tt Avocado}; \citealp{2019Boone_avocado}); while transformer architectures \citep{2023Pimentel_Attention,2024Allam_Attention,ATAT2024} leverage self-attention mechanisms to handle missing data naturally. Hybrid, physics-informed approaches have also been explored to extract latent features from light curves using generative modeling, which are then used for classification \citep{2021Boone_ParSNIP}. Classification tools have already proven their utility in LSST survey optimization for supernova metrics, with realistic LSST survey cadences (e.g., \href{https://github.com/LSSTDESC/snmachine}{\tt SNMachine}, \citealt{Alves2022, Alves2023}).   

\acrshort{des} pioneered the use of neural networks for photometric classification of SNe~Ia for cosmological analysis \citep{Moller:2020,SCONE:2021,Moller:2022,Vincenzi:2023,DESSN:2024}. Propagating the prediction uncertainties from these models through to cosmological constraints remains an open problem. \href{https://github.com/supernnova/SuperNNova}{\tt SuperNNova} \citep{Moller:2020} addresses the former with a \acrfull{acr:rnn} providing calibrated probabilities essential for contamination modeling in dark energy constraints \citep{Vincenzi:2023}. In DES, \acrshortpl{bnn} and ensemble methods were also tested \citep{Moller:2024}; for DESC, we can advance fully Bayesian approaches to photometric classification for LSST. %DESC's proximity to survey operations makes it uniquely positioned to advance fully Bayesian approaches to photometric classification for LSST.

\paragraph{Forward Modeling of the Time-Domain Landscape}
Observational modeling led by DESC has catalyzed the development of neural approaches to photometric classification. \acrshort{plasticc} \citep{PLAsTiCC1810.00001, 2023Hlozek_PLasTicc} provided 3.49M test light curves across 18 transient classes using simulations from the \acrlong{snana} (\acrshort{snana}; \citealp{kessler2009}) with cadences and realistic observing conditions from the LSST \acrlong{opsim} \citep[\acrshort{opsim};][]{2014Delgado_OpSim}. The challenge established weighted logarithmic loss metrics prioritizing SNe Ia and \acrfull{kne} for DESC science goals \citep{2019Malz_Metric}, with winning solutions employing gradient boosting and ensemble neural networks requiring engineered features \citep{2019Boone_avocado,2023Hlozek_PLasTicc}. The dataset remains a foundational benchmark for time-series representation learning in astrophysics years after its development \citep{Fraga:2024,Masson:2024,CadizLeyton:2025, CadizLeyton:2025:MoE,ROMAE2025}. Building on PLAsTiCC, \acrshort{elasticc} \citep{2023AAS_ELAsTiCC, knop2023} stress-tested end-to-end broker infrastructure with $\sim$50M alerts streamed in real-time to seven community brokers from September 2022--January 2023. DESC remains the only Rubin Science Collaboration to have tested the alert infrastructure from end-to-end on this scale, as it is critical for deploying \acrshort{ai} models live, to support online learning and other tasks to optimize the scientific return from Rubin.

Similar to ELAsTiCC and PLAsTiCC, \acrfull{sassafras} is a novel dataset of simulated LSST spectroscopic follow-up with 2.1M spectra across 14 transient types and three telescopes -- Gemini, the \acrfull{soar}, and \acrshort{4most} -- using SNANA simulations with realistic noise from each of the three telescopes. These simulations contain an even distribution of spectra per class and have a wide range of redshift distribution between 0.023--1, minimizing the bias inherent from uneven distribution of spectra seen in real data. One group at \acrfull{skai} is currently utilizing SASSAFRAS to train a spectroscopic classifier and then transfer learn with real data to create a state of the art classifier.  Spectroscopic classifiers trained on SASSAFRAS will be essential for confirming transient labels for active learning algorithms as outlined below.

\paragraph{Online Learning for Spectroscopic Optimization}
While archival photometric classification will suffice for the bulk of LSST cosmology analyses, inference over partially obtained data remains crucial for prioritizing spectroscopic targeting before an event has ended. \Acrfull{gru}-based recurrent and convolutional neural networks have successfully classified partial-phase synthetic light curves \citep{2019Muthukrishna_RAPID,SCONE:2021,2023Gagliano_FirstImpressions, shah2025b_oracle}, but performance on observed data remains modest. Transformer-based methods are being increasingly used \citep{ATAT2024}, with synthetic pre-training playing a growing role in bridging the simulation gap \citep{2025Gupta_Sims}. Within DESC, host-galaxy correlations have been shown to improve early classification \citep{2021Gagliano_GHOST}; this has driven data-driven modeling of host-galaxy correlations for the ELAsTiCC challenge \citep{2023Lokken_SCOTCH}, although spurious host-galaxy associations and the small postage stamps of the field contained within the LSST alert packets may limit utility of real-time inference using these data.

Beyond real-time classification, active learning faces unique astronomical challenges: objects must be selected for spectroscopic follow-up before informative light curve data are obtained, the untargeted population is substantially dimmer than the spectroscopically confirmed sample used in training, and labeling costs vary dramatically with object brightness and sky position. The \acrfull{resspect}\footnote{\url{https://resspect.readthedocs.io/en/latest/}}, an initial approach to active learning for transient science, implements uncertainty sampling with random forest classifiers on Bazin \citep{2011Bazin_LCModel} parametric features, but requires a minimum of five observations per filter, limiting early-time selection \citep{2020Kennamer_RESSPECT}. More recent implementations have refined active learning for early-time SN Ia identification, demonstrating effective follow-up optimization with simulations \citep{Ishida:2019}, real-data a posteriori \citep{Leoni:2022} and real-time observational campaigns \citep{Moller:2025}, the latter revealing the need for training sets containing events beyond supernovae. \href{https://github.com/MichelleLochner/astronomaly}{\tt Astronomaly} \citep{2021Lochner_Astronomaly} introduces personalized anomaly detection by combining isolation forests with human relevance scoring, addressing the fundamental subjectivity of an anomaly label. The approach has been shown to double the rate of anomaly discovery in radio transients \citep{2025Andersson_astronomaly}. However, active learning remains fundamentally limited by the lack of an informative initial training set, such that early random sampling can produce biased or unrepresentative data that propagates through subsequent iterations of learning. This is a fundamental challenge for novelty detection in LSST data.

\paragraph{Prompt Processing with the Transient Alert Brokers}
The seven Rubin Community Brokers implement diverse classification pipelines. \acrfull{alerce} employs a \acrshort{acr:cnn} for top-level classification from alert postage stamps \citep{2021Carrasco_Stamp}, and a hierarchical random forest applied to photometric features for classification along a 15-class taxonomy \citep{2021Sanchez_AlerceLC}. \acrfull{ampel} uses a four-tier system with predominantly gradient-boosted random forests \citep{2025Nordin_AMPEL}, and Fink deploys multiple classifiers for early and late-time classification \citep{Fraga:2024,Leoni:2022, Moller:2020, Moller:2025, fink}. \acrfull{antares} employs multi-stage filtering with community-contributed Python classes for tagging sources \citep{2018Narayan_ANTARES}, while Lasair integrates a boosted decision tree classifier from host galaxy properties \citep{2020Smith_ATLAS} with \acrfull{moc}-based watchmaps for coordination with 4MOST's \acrfull{tides}, which will be providing 35,000 transient spectra for SN~Ia cosmology \citep{2024Williams_Lasair}. These brokers have demonstrated sub-second latency in processing millions of alerts during the ELAsTiCC campaign, with classification probabilities reported via standardized Avro schemas that enable systematic evaluation of heterogeneous ML architectures. DESC members are involved in all seven Rubin Community Brokers, offering substantial potential for shared software infrastructure for processing the LSST alert stream.

%The Rubin Community Brokers are \texttt{ALERCE} \citep{alerce}, \texttt{AMPEL} \citep{ampel}, \texttt{ANTARES} \citep{antares}, \texttt{Babamul}, \texttt{Fink} \citep{fink}, \texttt{LASAIR} \citep{lasair} and \texttt{Pitt-Google} and will be crucial for both endeavors, receiving, processing and filtering the 10 million nightly detections from LSST every night. They are crucial for both creation of samples for scientific analysis and the coordination of follow-up observations. Brokers have a strong AI/ML component with multiple classifiers targeting various supernovae types, AGNs, and other transients. Citations needed of ML algorithms developed in DESC in these brokers.

\paragraph{Cosmological Inference using Type Ia Supernovae}

Hierarchical Bayesian models have been applied to SN~Ia for various goals, including cosmological inference \citep[e.g.,][]{2011March_BAHAMAS, 2016Shariff_BAHAMAS, 2015Rubin_Unity, 2025Rubin_Unity, 2018Feeney_H0}, constructing empirical SED models \citep[\href{https://github.com/bayesn/bayesn}{\tt BayeSN};][]{2009Mandel_BayeSN, 2011Mandel_BayeSN, 2022Mandel_BayeSN, 2021Thorp_BayeSN, 2023Ward_BayeSN, 2024Grayling_BayeSN, Uzsoy_2024}, modeling intrinsic colors and dust extinction \citep{2017Mandel_SimpleBayeSN, 2022Thorp_BayeSN, 2024Thorp_BayeSN}, handling uncertain photometric classifications \citep{2007Kunz_BEAMS, 2012Hlozek_BEAMS} and redshifts \citep{2017Roberts_zBEAMS}, and modeling the spectrophotometric standards used in photometric calibration \citep{2025Boyd_DA, 2025Popovic_Dovekie}. However, in the LSST era such models will need to incorporate complex effects that cannot easily be treated analytically; e.g., selection effects, photometric classification and photometric redshifts. Work is ongoing to enhance our statistical models using \acrshort{acr:sbi}, to leverage the flexibility of neural networks to capture these complex effects \citep[e.g.,][]{2024_Boyd_Flows, 2024Karchev_SIDEreal, 2025Karchev_CIGARS}. SBI will enable scalable and principled statistical inference of cosmological parameters with LSST. 

\subsection{Theory and Modeling}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{emulator}, \meth{gaussian-process}, \meth{neural-surrogate}, \meth{differentiable-programming}, \meth{symbolic-regression}, \meth{sbi} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{scalability} \\
\themebullet \themekey{Opportunities.} Flexible emulation frameworks, model selection and hypothesis testing, efficient construction of realistic mock datasets, gradient-based sampling.
% I edited this, feel free to change - Dani Leonard
\end{ThemeBoxA}

The role of theory and modeling within \acrshort{desc} is to provide the essential bridge between cosmological parameters and the statistical observables derived from \acrshort{lsst} data. Accurate theoretical models are required to translate measured galaxy shapes, positions, and fluxes into constraints on dark energy, dark matter, and gravity. This entails constructing predictive models of large-scale structure, galaxy bias, baryonic physics, and lensing observables that can be robustly compared with data while marginalizing over astrophysical and observational systematics. As the scale and precision of LSST data demand modeling at unprecedented accuracy and speed, \acrshort{ml}-based emulators, differentiable theory libraries, and \acrshort{acr:sbi} approaches are increasingly central to this effort, enabling fast and robust connections between data and theory.


\paragraph{Fast Surrogates for Cosmological Likelihoods} 

Emulation and related methods for creating fast-surrogate models using ML and \acrshort{ai} will be of crucial importance in accelerating inference pipelines for cosmological analyses in DESC.  Emulation is an indispensable tool for integrating aspects of modeling which by nature require simulation too slow to ever consider incorporating directly in sampling (e.g., those requiring $N$-body or hydrodynamical simulations). At the same time, even for aspects of modeling which are more moderate in evaluation cost (seconds rather than many hours), emulation allows individual likelihood evaluations to be dramatically accelerated. This is one of the key ways we can make computationally feasible the sampling in high-dimensional parameter spaces which will be required for DESC analyses.

Work on these emulation techniques has included directly building emulation tools, particularly outside of \acrshort{wcdm} models \citep{ramachandra2021matter}, emulating intrinsic alignment correlations \citep{Pandya2025IAEmu}, as well as using emulators to efficiently evaluate modeling choices for LSST data \citep{boruah2024machine}. The DESC theoretical modeling package \href{https://github.com/LSSTDESC/CCL}{\tt pyCCL} \citep{chisari2019core} natively supports key matter-power-spectrum emulation tools \href{https://bitbucket.org/rangulo/baccoemu/src/master/}{\tt baccoemu} \citep{arico2021bacco} and \href{https://github.com/lanl/CosmicEmu}{\tt CosmicEmu} \citep{lawrence2017mira}. However, developing frameworks for DESC to analyze models outside $w$CDM in the nonlinear regime of LSST data remains a challenge that needs to be addressed \citep{Ishak:2019BwCDM}, for which AI can play a major role.  

% Lightweight, highly accurate neural network models can take on the heavy lifting associated with accelerating massively parallel computations that are inherently faster compared to the above examples (seconds per computation) but for which the use cases require millions or billions of evaluations, leading to major computational bottlenecks. Examples include the construction of mock galaxy catalogs with realistic photometry, and inference of \acrshort{sps} parameters via \acrshort{sed}-fitting to photometry for very large galaxy catalogs. Within DESC the \href{https://github.com/justinalsing/speculator/tree/master}{\texttt{speculator}} \citep{SPECULATOR} package, which is used within the \href{https://github.com/Cosmo-Pop/pop-cosmos}{\texttt{pop-cosmos}} generative model \citep{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}, accelerates SPS calculations in pop-cosmos by factors of 10,000$\times$ on \acrshort{gpu} hardware, relative to FSPS \citep{Conroy:2009, Conroy:2010, ConroyGunn:2010, Johnson:2021}  on \acrfullpl{cpu}.


Looking to the future, DESC would benefit from developing mechanisms to enable emulation that has more flexibility with respect to modeling components. Our current methods of building a single emulator from scratch per modeling set-up is high cost (computationally and in terms of person power). This does not scale well for enabling adaptation to new systematics modeling or inference in models outside of $w$CDM. Considering approaches which use meta-learning (e.g., \citealt{macmahon2025meta}) or which are philosophically aligned with foundation models would be of value. 

Another area of growth where fast emulators can make major impact is in model selection and hypothesis testing of beyond-$w$CDM models. Instead of being limited by the cost of theoretical model evaluations, future analyses will be constrained by how efficiently inference pipelines can navigate and compare competing cosmological models. Integrating these emulators within agentic AI systems (\autoref{sec:llm_agentic})---which can autonomously refine training data, adapt inference strategies, leverage tools for evidence computation and simulation-based inference, and even propose new model extensions---will further accelerate discovery. For DESC, this synergy will transform the capacity to test gravity, dark energy, and dark-sector interactions, turning high-quality data into a powerful engine for identifying new physics. 

\paragraph{Differentiable Programming for Accelerated Sampling}

Traditional cosmological inference pipelines often rely on computationally expensive numerical methods such as the \acrlong{camb} \citep[\acrshort{camb};][]{Lewis_2000} or HaloFit  \citep{Smith_2003, Takahashi_2012}, limiting the use of gradient-based sampling methods. Differentiable cosmological codes address this by enabling efficient computation of gradients with respect to model parameters, unlocking samplers such as \acrfull{hmc} and the \acrlong{nuts} \citep[\acrshort{nuts};][]{HoffmanGelman2014}. 

The \href{https://github.com/DifferentiableUniverseInitiative/jax_cosmo}{\texttt{jax-cosmo}} library \citep{JAX-COSMO} provides a differentiable and hardware-accelerated framework for cosmological computations. With a NumPy-compatible \acrshort{api} and close integration with tools such as NumPyro \citep{phan2019composable} and JAXopt \citep{jaxopt_implicit_diff}, \texttt{jax-cosmo} offers a practical foundation for building scalable, fully differentiable cosmological models. \cite{CosmoPower} combined \texttt{jax-cosmo} with neural-network emulators in \href{https://github.com/dpiras/cosmopower-jax}{\tt CosmoPower-JAX}, enabling high-dimensional Bayesian inference through automatic differentiation and GPU acceleration. The \href{https://github.com/fkeruzore/halox}{\texttt{halox}} package \citep{halox} uses \texttt{jax-cosmo} for cosmological calculations such as power spectra and distance measures in its modeling of dark-matter halo statistics (such as halo mass function and halo bias). \cite{2025arXiv250707833S} used \texttt{jax-cosmo} to test a differentiable Fisher-information approach based on score matching. Recently too, \cite{bartlett2025symbolic} have developed a symbolic emulator that leverages genetic programming-based symbolic regression to derive compact, analytic expressions for cosmological observables, including the radial comoving distance, linear growth factor, and nonlinear matter power spectrum. 

%The jax-cosmo library \citep{JAX-COSMO} enables differentiable and hardware-accelerated cosmological computations. Together with a NumPy-compatible API and close integration with tools such as NumPyro \citep{phan2019composable} and JAXopt \citep{jaxopt_implicit_diff}, this makes jax-cosmo a practical framework for building scalable and fully differentiable cosmological models. Since its first release, jax-cosmo has been used in several contexts. \cite{10.1093/mnras/stae2138} employed it to build a differentiable forward model for testing gradient-based samplers such as Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampler (NUTS) \citep{HoffmanGelman2014}, finding improved efficiency compared with standard Metropolis–Hastings methods. \cite{CosmoPower} combined jax-cosmo with neural-network emulators in CosmoPower-JAX, enabling high-dimensional Bayesian inference through automatic differentiation and GPU acceleration. The halox package \citep{halox} uses jax-cosmo for cosmological calculations such as power spectra and distance measures in its modeling of dark-matter halo statistics (such as halo mass function and halo bias). \cite{2025arXiv250707833S} used jax-cosmo to test a differentiable Fisher-information approach based on score matching. Recently too, \cite{bartlett2025symbolic} have developed a symbolic emulator that leverages genetic programming-based symbolic regression to derive compact, analytic expressions for cosmological observables, including the radial comoving distance, linear growth factor, and nonlinear matter power spectrum. When integrated into the jax-cosmo framework, these symbolic approximations replace computationally expensive numerical evaluations—such as CAMB \citep{Lewis_2000} or HALOFIT (e.g., \citealp{Takahashi_2012})—with differentiable, GPU-accelerated formulas, achieving sub-percent accuracy while reducing runtime by over an order of magnitude. This seamless integration enables efficient, gradient-based inference pipelines, such as those using Hamiltonian Monte Carlo, without sacrificing precision or interpretability.


\subsection{Cosmological and Survey Simulations}
\label{sec3:sims}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{emulator}, \meth{diffusion-model}, \meth{differentiable-programming}, \meth{sbi}, \meth{gnn}\\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{scalability}, \challenge{metrics} \\
\themebullet \themekey{Opportunities.} Joint modeling of galaxies and environments, inference at catalog and field level, modular components for DESC simulators, survey-scale generative models, survey design, systematics mitigation, pipeline stress tests.\end{ThemeBoxA}

Cosmological simulations are a fundamental tool not only for validating analysis pipelines but also, increasingly, for providing “theory” samples for \acrshort{acr:sbi} frameworks. Producing mock survey data at the scale and accuracy required for \acrshort{lsst} science remains a major challenge, which \acrshort{ml} can help address by emulating expensive numerical predictions and by providing data-driven models of otherwise poorly constrained aspects of the galaxy population. Neural emulators have long been used as fast surrogates for non-differentiable components of cosmological forward models, e.g., summary statistics of $N$-body simulations as in \href{https://github.com/lanl/CosmicEmu}{\tt CosmicEmu} \citep{CosmicEmu, moran2023mira}, \href{https://github.com/AemulusProject}{\tt Aemulus} \citep{2019ApJ...875...69D}, \href{https://github.com/dpiras/cosmopower-jax}{\tt CosmoPower-JAX} \citep{CosmoPower}, or \href{https://github.com/21cmfast/21cmEMU}{\tt 21cmEMU} \citep{21CMEMU}, and more recently for accelerating \acrshort{sps} calculations via models such as \href{https://github.com/justinalsing/speculator}{\tt speculator} and {\tt ProMage }\citep{SPECULATOR,ProMage}. Extending these approaches to additional components of the simulation pipeline holds the promise of greatly increasing the dynamical range, realism, and flexibility of LSST mock catalogs at manageable computational cost.

\paragraph{Population-Level Generative Models for Realistic Galaxy Catalogs}
Diffusion-based generative models operating in the space of physical galaxy parameters provide a powerful route to building realistic mock catalogs that remain anchored in deep-field observations. The \href{https://github.com/Cosmo-Pop/pop-cosmos}{\texttt{pop-cosmos}} framework \citep{Alsing:2024,Thorp:2025,Deger:2025} defines a score-based diffusion model over a high-dimensional SPS parameterization---\acrfull{sfh}, metallicity, dust, nebular emission, etc.---calibrated on $\sim$420,000 galaxies from COSMOS2020 \citep{Weaver:2022} spanning 26 bands from \acrshort{uv} to mid-\acrshort{ir}. Rather than directly emulating observed fluxes, \texttt{pop-cosmos} learns a data-driven prior $p(\boldsymbol{\theta}_{\mathrm{SPS}},z)$ over physical parameters and redshift that reproduces the joint distribution of observed photometry. This model encodes realistic priors on star-formation histories over cosmic time, and learns the evolution of the star-forming sequence. When coupled to survey-specific selection functions and noise models, \texttt{pop-cosmos} can therefore generate realistic mock galaxy catalogs that inherit both empirical constraints from deep multi-wavelength data and the flexibility of generative modeling.

\paragraph{Differentiable Empirical Galaxy–Halo Forward Modeling}
Complementary to purely catalog-level generative approaches, differentiable galaxy–halo forward models seek to describe galaxy populations as conditional generative processes on top of dark-matter structure. The
\href{https://github.com/ArgonneCPAC/diffsky/}{\tt Diffsky} framework \citep{OpenUniverse2024} rebuilds the traditional “halo $\rightarrow$ SFH $\rightarrow$ SED’’ chain using differentiable, physically interpretable blocks. \href{https://github.com/ArgonneCPAC/diffmah}{\tt Diffmah} \citep{Diffmah} provides a JAX-based, few-parameter model of halo mass assembly $M_{\rm halo}(t)$, replacing noisy merger trees with smooth, analytic, differentiable growth histories. On top of this, \href{https://github.com/ArgonneCPAC/diffstar/}{\tt Diffstar} \citep{Diffstar} models in situ star-formation histories with a small set of parameters (e.g., star-formation efficiency, gas-consumption timescale, quenching time), while \href{https://github.com/ArgonneCPAC/diffstarpop}{\tt DiffstarPop} \citep{Diffstarpop} lifts this to the population level by learning the statistical link between SFH parameters and halo assembly across suites of reference simulations. Finally, \acrlong{dsps} \citep[\href{https://github.com/ArgonneCPAC/dsps/}{\tt DSPS};][]{DSPS} maps these SFHs and associated metallicity/dust parameters to SEDs and photometry entirely within JAX. Together, {\tt Diffmah} + {\tt Diffstar}/{\tt DiffstarPop} + \texttt{DSPS} replace merger trees, non-differentiable semi-analytic recipes, and black-box SPS calls with a modular, probabilistic, fully differentiable stack whose low-dimensional, physically meaningful parameters can be calibrated and explored with gradient-based methods and SBI, while still generating large, realistic synthetic catalogs.
\acrshort{ai}-based models are in development to generate multiband galaxy images from these synthetic catalogs, conditioned on the parameters of the \texttt{Diffsky} suite.



\paragraph{Differentiable Cosmological N-body Solvers} Recent years have seen the emergence of particle–mesh $N$-body solvers implemented in modern, \acrshort{gpu}-accelerated, deep-learning frameworks that support automatic differentiation \citep[e.g.,][]{FlowPM, pmwd, DISCO-DJ, JAXPM}. Automatic differentiation enables hierarchical Bayesian inference directly over forward simulations of large-scale structure, opening a path toward full-field inference and near-optimal extraction of cosmological information. The main obstacles to deploying these methods at LSST scale are the computational and engineering demands of simulating survey-sized volumes in a differentiable way. Computing derivatives through the simulation implies non-trivial memory costs that are difficult to satisfy under the constraints of GPU accelerators. Several complementary strategies are being developed to address this challenge, including multi-node domain decomposition for distributed simulations \citep{Kabalan_jaxDecomp_2025}, techniques to reduce the memory cost of gradient evaluation \citep{Li__2024}, and improved time integrators that achieve a given accuracy with fewer time steps \citep{Rampf2025}. Beyond enabling full-field inference, differentiable simulations also enable the combination of physics-based solvers with learned components, yielding hybrid schemes that can improve the speed and accuracy of particle–mesh simulations \citep{Lanzieri_2023, Payot2023}.

\paragraph{Hydrodynamical-Simulation-Based Mappings of Galaxy Properties}
A complementary strategy is to treat state-of-the-art hydrodynamical simulations as high-fidelity “teachers’’ and use ML to distill their complex, small-scale physics into fast, effective models defined directly on dark-matter fields. Rather than specifying parametric galaxy–halo or SPS models, these approaches learn mappings from halo or large-scale-structure descriptors to galaxy properties as realized in the simulations. Recent work on \acrshort{ia} illustrates this paradigm. A traditional approach, followed by \citet{VanAlfen2024} is to develop an empirical IA model constrained by hydrodynamical simulations within a flexible \acrfull{hod}-like framework. A more ML-oriented approach is to learn an end-to-end emulator of galaxy properties. \citet{Jagvaral2025} introduce a geometric deep-learning approach in which galaxy shapes and orientations from IllustrisTNG \citep{IllustrisTNG} are modeled using E(3)-equivariant graph neural networks defined on the cosmic web, capturing the conditional distribution of shapes and orientations given halo mass, environment, and tidal field. This yields a fast, simulation-calibrated surrogate that reproduces intrinsic-alignment statistics at the percent level, enabling embedding of hydro-level realism into DESC mock catalogs without rerunning computationally expensive hydrodynamical simulations.


\subsection{Object Classification}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.}  \meth{ensembles}, \meth{gnn}, \meth{active-learning}, \meth{transformer}, \meth{self-supervised} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{data-sparsity}, \challenge{scalability}, \challenge{uq} \\
\themebullet \themekey{Opportunities.} Multi-survey training, Data-driven Priors, Generative Models 
\end{ThemeBoxA}
The classification of astronomical objects is an area that has seen a surge in \acrshort{ml} and \acrshort{ai} applications over the past decade.
In regards to \acrshort{desc} science, we call out three potentially relevant areas: the detection and removal of bogus/non-astrophysical sources, the classification of galaxy types, and star/galaxy separation.
While the first area is critical for minimizing the pollution of source catalogs, it falls mostly under the responsibility of the Rubin Project's data management team and is addressed at the instrument signature removal stage of the Rubin pipeline \citep{Bosch18hsc}.
Galaxy type classification has been a strong application of citizen science and \acrshort{ai}/\acrshort{ml} applications, with an increasing demand and opportunity expected from the depth that will be achieved by Rubin \citep[e.g.,][]{2024A&A...683A..42C}.
However, galaxy type classification is not currently identified as a main concern for DESC beyond what is necessary to ensure a clean sample of galaxies for cosmological analyses (e.g., identifying merging galaxies that could confuse some of cosmological measurements).

%Since the inception of the \href{https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/about/research}{Galaxy Zoo} citizen science experiment, galaxy typing through machine learning has become industrial, and deep learning is coming to the rescue to tackle both the faintness limit and the increasing demand for classification complexity and depth \citep[e.g.,][]{2024A&A...683A..42C}. 
%Insofar as the main issue for DESC is to ensure a clean sample of galaxies where, e.g., anomalous systems like merging galaxies could confuse some of its probes, galaxy type classification is not currently a main concern of DESC, but it is definitely of importance to the \acrshort{galsc}.

On the other hand, star/galaxy separation is expected to be crucial to DESC science, as it may influence catalog completeness, weak-lensing shear estimation, galaxy clustering estimates, calibration of photometric redshifts, and object selection for spectroscopy. At the imaging depth of \acrshort{lsst}, galaxies vastly outnumber stars, and a nontrivial fraction of those galaxies are compact and blue, making them morphologically and photometrically similar to point sources, especially under variable seeing and in crowded fields \citep{fadely12}. 

The nominal approach to star/galaxy separation implemented by the Rubin Science Pipeline uses a cut-based approach based on object “extendedness”, which is constructed from a comparison of the \acrshort{psf}- and model-based measurements \citep[see][and references therein]{slater_morphological_2020}. 
This classifier performs well at bright magnitudes but struggles at the faint end, leading to either very large contamination in the faint star sample (i.e., orders of magnitude more galaxies than stars) or significant (i.e., nearly total) incompleteness for stars. 
Prior work from DESC members, working on precursor surveys like \acrshort{des}, improved on simple cut-based analyses by using feature-based machine learning—decision trees, random forests, boosted ensembles, and shallow neural networks trained on catalog-level features such as colors, shape moments, and PSF--model magnitude differences \citep[e.g.,][]{2015MNRAS.450..666S, sevilla18, baqui21, bechtol2025}.
However, these methods are ultimately limited by deblending errors, incomplete PSF modeling, and the loss of informative spatial structure in catalog summaries.

Parallel investigations have examined models that ingest multi-band image cutouts together with catalog features, allowing networks to learn morphology directly while leveraging color-based information (e.g., proximity to the stellar locus, color-color degeneracies, and uncertainty-aware colors; \citealp{2017MNRAS.464.4463K}). 
Efforts increasingly incorporate PSF awareness \citep[e.g.][]{Patel2025NPE}, multi-epoch data (i.e., variability), and deblending context to improve robustness across seeing conditions and sky regions, and explore semi/self-supervised representation learning to exploit LSST’s vast unlabeled datasets. 
Looking ahead, promising methodologies include end-to-end probabilistic deep learning that propagates PSF and noise models into calibrated class probabilities \citep{2019MNRAS.490.3952B}, which can include prior probabilities based on spatial location and spectrum \citep{lopez19}; vision transformers and \acrfullpl{acr:gnn} that integrate multi-visit information; multi-modal architectures combining images, colors, variability, and proper motion; domain adaptation and label-shift correction to handle spatial and temporal heterogeneity; active learning using sparse spectroscopic labels; and simulation-based training on realistic survey mocks. Emphasis on \acrshort{acr:uq}, continual and federated learning across data releases, and physics-informed constraints should yield classifiers that are both scalable and scientifically reliable at the LSST depth.

\subsection{Deblending}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{vae}, \meth{npe}, \meth{instance-segmentation}, \meth{diffusion-model}, \meth{normalizing-flow}, \meth{cnn}, \meth{object-detection}, \meth{som} \\
\themebullet \themekey{Challenges.} \challenge{data-sparsity}, \challenge{metrics}, \challenge{uq} \\
\themebullet \themekey{Opportunities.} Multi-survey training, Data-driven Priors, Generative Models 
\end{ThemeBoxA}

Turning pixels into objects is a fundamental problem in astronomical survey pipelines. Object detection and deblending of \acrshort{lsst} data is a crucial step in producing catalogs useful for \acrshort{desc} science.  Given its unprecedented depth for a ground-based survey, LSST will face new challenges in its detection pipelines compared to previous legacy surveys \citep{Melchior21blending}.  Blending, or the overlapping of source light profiles, is an imaging systematic that affects all downstream analysis, as it becomes difficult (in reality intractable) to disentangle photons from a given source in a blend.  This problem is exacerbated with increased observing depth, as more light is collected from sources that are overlapping due to line-of-sight projections or physical interactions. Traditional object detection pipelines for wide-field surveys use a maximum likelihood estimator method \citep{Bosch18hsc} to identify peaks in intensity corresponding to sources in an image.  This method, while statistically justified, is still subject to failure modes, wherein \acrshort{ai} can provide alternative and complimentary methods.  Similarly, traditional deblending algorithms typically rely on models and assumptions about source light profiles that may not provide sufficient flexibility for the billions of sources LSST will observe \citep{Melchior18scarlet}.  DESC has been exploring and developing AI methods to aid in these challenging problems, which are crucial to understand and mitigate.

\paragraph{Catalog-level Blend Identification}  While a majority of sources observed by LSST are expected to have some level of blending, a particularly pernicious case is that of unrecognized blends.  These are sources in a blended scene that are indeed distinct (determined from high-resolution space-based observations), but are only recognized as a single source by cataloging and deblending pipelines.  Unrecognized blends impact measured properties such as galaxy shapes \citep{Dawson16ublends}, photometric redshifts \citep{Liang25catblend}, and more.  Estimates of the level of unrecognized blends in LSST  range from $\sim15$--$30\%$, with analysis of early LSST data compared to catalogs from the \acrshort{hst} \acrfull{candels} yielding an unrecognized blend rate of 18\% \citep{sitcom128}.  Blends that remain at the catalog level are definitionally unrecognized blends but may still be detectable as outliers via their multi-band photometry or their shapes. Random forests and \acrshortpl{acr:som} along with various anomaly detection algorithms were tested in \cite{Liang25catblend} who showed that unrecognized blends can be detected at a cost to the sample size. These algorithms were used to assign an unrecognized blend probability, however improvements can be made for specific science cases. For example, using the blend entropy (Ramel et al., in prep) can improve cluster cosmology by removing the most problematic unrecognized blends for cluster analysis. Designing better blending metrics like blend entropy and incorporating them into \acrshort{ml} algorithms like \acrshortpl{acr:gnn} is the main goal of the software package, \href{https://github.com/LSSTDESC/friendly}{friendly} . 

\paragraph{Image-level Deblending Using Deep Learning} Deep learning algorithms designed for object detection and deblending provide an alternative method to traditional pipelines that may help improve catalog completeness and source property measurements. For instance, the \acrfull{bliss} framework uses \acrshort{acr:npe} to infer probabilistic catalogs by training a \acrshort{acr:cnn} directly on multi-band images \citep{hansen2022}. BLISS produces well-calibrated posterior approximations for various source properties, and point estimates based on these posterior approximations outperform the standard LSST pipeline in source detection, flux measurement, star/galaxy classification, and galaxy shape estimation \citep{Duan25NPE}. The method is robust to spatially varying backgrounds and point-spread functions, provided these features are present in the simulated training images \citep{Patel2025NPE}. The \acrshort{deepdisc} instance segmentation \citep{Merz23DeepDISC} framework produces object catalogs and segmentation masks from image data, and is being tested with joint \textit{Roman}--Rubin data to incorporate multimodal information for downstream detection and deblending improvement. Both \href{https://github.com/astrodeepnet/debvader}{\tt DebVader} \citep{Arcelin21debvader} and \acrlong{madness} \citep[\acrshort{madness};][]{Biswas25mad} use \acrshortpl{acr:vae} to handle blending. They use self-supervised training to learn the structure of isolated galaxies. Through additional training of a deblending encoder they learn to isolate a galaxy from a blend. A specialized decoder can directly measure the characteristics of the galaxy (shape, \acrshort{photoz}) without reconstructing explicitly the image of the isolated galaxy. MADNESS adds a normalizing flow to the architecture to improve performance by modelling the latent-space distribution of galaxies, thereby providing an explicit likelihood for posterior optimization. Ongoing work uses a multimodal VAE to learn both from imaging and spectroscopy, adding more information in the latent space to improve the galaxy characteristics measurement, especially photo-$z$. The very principles of deblending VAEs alleviate the impact of unrecognized blends, and ongoing work on the use of probabilistic catalogs where the number of detected galaxies is itself non-deterministic will reduce it even more.  Even if sources are in such close proximity that LSST imaging will not be able to recognize the overlap and detect the blended group as one source, it is feasible to model the detected sources first, compute the residuals from the fit, and run detection again on the residuals. Because the residuals can have complicated structure, it is beneficial to perform the detection on multi-band residuals, where unrecognized sources appear as colored, localized over- or underdensities. Recognizing them, as well as their likely centers is possible and fairly effective with computer vision architectures like \acrshort{yolo} \citep{sowmya_kamath_2020_3721438}.

\paragraph{Data Driven Priors for Deblending with Explicit Likelihoods}

Generative models such as normalizing flows and diffusion models can be trained on unblended galaxies (potentially limited amounts of space-based data) and then serve as data-driven priors for galaxy morphologies \citep{Lanusse19gen}. Posterior optimization and sampling becomes possible for inverse problems with explicit likelihood functions (such as inpainting, deconvolution, and deblending). This is particularly effective in low \acrfull{snr} cases where the deblender \href{https://github.com/pmelchior/scarlet}{\tt scarlet}, \citep{Melchior18scarlet} which is the default deblending method in the Rubin Science Pipelines, is outperformed by a new, prior-augmented version from \href{https://github.com/pmelchior/scarlet2/}{\tt scarlet2} \citep{Sampson24scarlet2}. The same approach can also perform transient photometry in the presence of a host galaxy without the need for difference imaging \citep{Ward25scarlet2}. Additionally, posterior optimization in latent space by the MADNESS deblender \citep{Biswas25mad}, using data-driven priors, also outperformed scarlet.


% Skipping for now...
% \newpage
% \subsubsection{Instrumental Response Modeling}
% \begin{ThemeBoxA}[]
% \themebullet \themekey{Methodology.} TBD \\
% \themebullet \themekey{Challenges.} TBD \\
% \themebullet \themekey{Opportunities.} TBD
% \end{ThemeBoxA}

\subsection{Shape Measurement}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{differentiable-programming}, \meth{deep-network}, \meth{sbi} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{uq}, \challenge{data-sparsity}, \challenge{scalability} \\
\themebullet \themekey{Opportunities.} Joint optimization of detection, deblending, and shear, hybrid analytic–neural estimators,  realistic multi-instrument and multi-epoch scene modeling, active learning, unified shear–photo-z modeling
\end{ThemeBoxA}

Weak-lensing shape measurement is one of the most critical and challenging components of the \acrshort{desc} analysis pipeline: small percent-level biases in ensemble shear propagate directly into the cosmological parameters targeted by \acrshort{lsst}. Meeting DESC requirements therefore demands methods that are simultaneously accurate enough to control multiplicative and additive shear biases, computationally efficient enough to process billions of galaxies, and amenable to calibration. A unifying theme in recent work is the exploitation of differentiability and \acrshort{gpu} acceleration, both in explicit shear calibration schemes and in forward models.

\paragraph{Analytic Calibration and Differentiable Shear Estimators}
The \acrfull{anacal} framework  \citep{Li2023,Li2025_bias} demonstrates how differentiability can be used to obtain high-precision shear responses and noise-bias corrections without relying on large external simulation campaigns. By representing galaxy properties and pixelized images using differentiable basis functions, AnaCal yields analytic shear responses for detection, selection, and shape measurement, achieving LSST-grade accuracy with sub-millisecond inference per galaxy. More broadly, other calibration schemes such as metacalibration \citep{Huff2017,Sheldon2017} stand to benefit from differentiable image models and measurement operators: with gradients available throughout the pipeline, shear response and noise-bias corrections can be computed more quickly and robustly.

\paragraph{Deep Learning–Based Shape Estimators}
Modern deep-learning architectures provide a natural path to an end-to-end differentiable shear estimator that can simultaneously integrate detection, deblending, shear estimation, and robustness to image defects. Neural networks are inherently GPU-accelerated, highly parallel, and differentiable, making them well suited to high-throughput shear inference at LSST scale. Such an approach was originally demonstrated in \citep{Ribli2019} and is being explored in a DESC context using the \acrshort{deepdisc} architecture \citep{Merz23DeepDISC}, which was originally designed as a general purpose architecture for detection and segmentation and which can estimate gravitational shears within a single GPU-resident and differentiable model. Being automatically differentiable, this estimator can be calibrated using the schemes mentioned above.

\paragraph{Hierarchical Forward Modeling with Differentiable Image Simulators}
A complementary strategy frames shape measurement as a hierarchical forward-modeling problem, in which cosmological parameters, population-level distributions of galaxy properties, and individual galaxy shapes are inferred jointly from the pixel data \citep{Schneider2015}. In this view, a forward model generates simulated images given a set of hierarchical parameters, and inference proceeds by comparing these simulations to the observed images. Such approaches are made practical thanks to the \href{https://github.com/GalSim-developers/JAX-GalSim}{\tt JAX-GalSim} effort which re-implements key \href{https://github.com/GalSim-developers/GalSim}{\tt GalSim} \citep{ROWE2015121} functionalities in JAX, making this forward model fully differentiable and GPU-accelerated while supporting vectorized batch simulations of thousands of galaxies at once. In ongoing DESC efforts, \texttt{JAX-GalSim} is used to implement the hierarchical shear-inference framework of \citet{Schneider2015}. Instead of traditional \acrshort{mcmc}, gradient-based samplers (\acrshort{nuts}), GPU acceleration, and batching can be used to yield roughly an order-of-magnitude speedup while keeping multiplicative shear biases within LSST requirements. While the aforementioned approach relies on analytic surface brightness profiles to model galaxies, more realism can be achieved through projects like \href{https://github.com/pmelchior/scarlet2/}{\tt scarlet2} \citep{Sampson24scarlet2} which extends the modeling to non-parametric morphologies and blended scenes observed with multiple instruments, providing a JAX-based, differentiable scene-modeling framework in which gradients of the likelihood with respect to source parameters and hyperparameters are readily available.

\subsection{Synthesis and Recommendations}
\label{sec3:synthesis}

\acrshort{ml} has become a foundational component of the collaboration's scientific infrastructure. It appears at every stage of the analysis pipeline: from pixel-level data processing (deblending, shape measurement) through derived observables (\acrshort{photoz}, cluster masses) to cosmological inference itself (weak lensing, \acrshort{sne}, clusters). Adoption is not driven by bespoke, application-specific methods. Instead, a small set of core methodologies and a consistent set of fundamental challenges recur across the entire application landscape, as visualized in Figure~\ref{fig:chord-diagram}. These cross-cutting patterns have direct implications for how \acrshort{desc} should organize its \acrshort{ai}/\acrshort{ml} efforts, prioritize methodological investments, and structure collaboration-wide infrastructure.

Among shared approaches, a few key methodologies appear repeatedly: \textit{\acrshort{acr:sbi}}, enabling parameter estimation from lensing to supernovae, though constrained by the fidelity of forward-models; \textit{differentiable programming frameworks} such as \href{https://github.com/DifferentiableUniverseInitiative/jax_cosmo}{\texttt{jax-cosmo}}, \href{https://github.com/GalSim-developers/JAX-GalSim}{\tt JAX-GalSim}, \href{https://github.com/pmelchior/scarlet2/}{\tt scarlet2}, for unlocking gradient-based inference at scale;  \textit{\acrshort{nde} and generative models} such as \href{https://github.com/Cosmo-Pop/pop-cosmos}{\texttt{pop-cosmos}}, \href{https://github.com/astrodeepnet/debvader}{\tt DebVader}, and \href{https://github.com/LSSTDESC/madness}{\tt MADNESS}, which provide flexible probabilistic representations with demonstrated cross-application reusability; \textit{emulators} such as \href{https://github.com/dpiras/cosmopower-jax}{\tt CosmoPower-JAX} and \href{https://github.com/justinalsing/speculator}{\texttt{speculator}}, which trade training cost for orders-of-magnitude speedups; and \textit{active learning}, for maximizing scientific return from limited expert annotations. These techniques recur because the scientific challenges in DESC (extracting maximal information under stringent systematic control, scaling to billions of objects, marginalizing over complex nuisance parameters) demand the same classes of solutions regardless of the specific probe. The fundamental challenges in the use of these techniques are equally shared between domains: \textit{covariate shifts}, spectroscopic selection bias in photo-$z$, sim-to-real gaps in \acrshort{sne}, model misspecification limiting \acrshort{acr:sbi}, and domain adaptation across surveys; \textit{\acrshort{acr:uq}}, obtaining well-calibrated posteriors and propagating uncertainties to cosmological constraints; \textit{scalability}, from billions of galaxies to $10^7$ nightly alerts, demanding algorithmic and infrastructure innovations; \textit{data sparsity and rare events}, including limited labeled samples, rare transients, class imbalance, and challenging edge cases like blending; and \textit{metrics and evaluation}, defining task-relevant metrics, validation frameworks, and stress tests aligned with DESC science requirements. Addressing these methodologies and challenges in a general, reusable way (rather than independently within each working group) has multiplicative impact: improving SBI's robustness to covariate shifts benefits not only clusters but also photo-$z$, lensing, and SNe; developing robust UQ enhances reliability for deblending, shape measurement, and generative simulations simultaneously.\\
%This transversality represents a strategic opportunity to deliver more precise, robust, and comprehensive cosmological constraints from LSST in a reproducible and scientifically defensible manner.

The shared methodological approaches and barriers documented above demand a coordinated response. DESC should \textit{establish collaboration-wide AI/ML coordination mechanisms} (e.g., standing working group, cross-\acrshort{wg} task forces, interchange meetings) to ensure methodological innovations are rapidly evaluated for applicability across probes, common challenges are tackled collectively, and duplication is minimized. DESC should also \textit{invest in shared infrastructure and benchmarks} (reusable libraries, e.g., \href{https://github.com/DifferentiableUniverseInitiative/jax_cosmo}{\texttt{jax-cosmo}} and \href{https://github.com/GalSim-developers/JAX-GalSim}{\tt JAX-GalSim}, standardized interfaces, e.g., \acrshort{rail} for photo-$z$, and validation frameworks that stress-test methods under covariate shift and model misspecification) recognizing that these investments have multiplicative returns. DESC should \textit{develop collaboration-wide best practices and validation standards} for ML methods intended for cosmological inference, establishing requirements for UQ calibration such as coverage tests and \acrfull{pit} histograms, distribution-shift diagnostics, stress tests under deliberate misspecification. Finally, mechanisms to \textit{facilitate rapid dissemination of knowledge} (e.g., AI/ML workshops, shared tutorials, method spotlights in collaboration meetings) would accelerate the transfer of innovations across working groups.
