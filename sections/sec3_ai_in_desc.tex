\newpage
\section{Intersection of AI/ML Techniques with DESC Science}
\label{sec3:use_case_for_aiml}

The science goals of DESC place unusually stringent demands on statistical methodology. Extracting percent-level constraints on dark energy and tests of gravity from Rubin LSST data requires not only exquisite control of observational systematics, but also analysis pipelines that can efficiently exploit information distributed across billions of galaxies, multiple probes, and heterogeneous data modalities (images, catalogs, time series, simulations). AI and ML are already embedded in many of these workflows and their importance will only grow as analyses become more ambitious and data volumes increase.

In this section, we survey existing intersections of AI/ML methods with DESC science and highlight which aspects of the science case stand to benefit most from methodological advances. Across these applications, common methodological themes emerge: the need for trustworthy uncertainty quantification and robust calibration; scalability to LSST-sized datasets and high-dimensional parameter spaces; methods that can operate coherently across multiple surveys; and tight integration between learned models and physically motivated forward models. By organizing the landscape along both science goals and methodological challenges, our aim is to clarify where targeted investment in AI/ML will yield the greatest scientific returns for DESC, and to identify opportunities for cross-cutting solutions that can be shared across probes and working groups.

\subsection{Impact at the Analysis Level}

\subsubsection{Photometric redshifts} \label{sec3:photo-z}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{gaussian-process}, \meth{neural-density-estimation}, \meth{som}, \meth{transformer}, \meth{hierarchical-bayes}, \meth{neural-surrogate} \\
\themebullet \themekey{Challenges.}   \challenge{uq-calibration}, \challenge{scalability},
  \challenge{covariate-shift} \\
\themebullet \themekey{Opportunities.} Multi-survey training, simulation infrastructure, hierarchical inference
\end{ThemeBoxA}

The inference of photometric redshifts (photo-$z$) represents a foundational challenge for the Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST), where the vast majority of tens of billions of detected galaxies will lack spectroscopic redshift measurements due to both observational time constraints and the intrinsic faintness of the sample. Photometric redshifts are derived by establishing empirical or physically motivated mappings between broadband photometry (including colors, magnitudes), morphology, and redshift. This process is fundamentally limited by our incomplete knowledge of galaxy spectral energy distributions, stellar population synthesis models, and dust attenuation physics. However, the accuracy and reliability of photo-$z$ estimation is critical across virtually all extragalactic LSST science cases, including weak gravitational lensing, large-scale structure, galaxy cluster cosmology, and supernova surveys. To achieve the DESC goals for constraining the dark energy equation of state, calibration of photo-z estimates must reach the $0.002 \times (1+z)$ level for the first year of LSST data \citep{v1DESC-SRD}. Achieving these benchmarks necessitates not only accurate point predictions but also \textit{well-calibrated uncertainty quantification}, motivating an emphasis on probabilistic methods.


\paragraph{Supervised Photo-z Estimation} The empirical approach to  photo-$z$ inference is to learn a mapping from observed broadband photometry or imaging to redshift, leveraging spectroscopic training samples. From catalog-level photometry, empirical regressors such as random forests TPZ~\citep{carrasco2013tpz}, gradient boosting machines FlexZBoost~\citep{izbicki2017converting} and Gaussian processest GPz ~\citep{almosallam2016gpz} have demonstrated competitive performance by constructing mappings from color-magnitude space to redshift. Neural networks, including both fully connected and specialized architectures, have proven particularly adept at capturing complex relationships in high-dimensional photometric data. On the other hand, nearest-neighbor approaches such as kNN and CMNN~\citep{graham2017photometric} take the training as a reference sample and compute the average redshift of neighbors of the target galaxy. In a similar way, DNF~\citep{de2016dnf} performs a regression on the neighborhood, which allows the construction of a local linear model for each galaxy. \\ 
Contemporary approaches have evolved from point estimators to full probabilistic models capable of capturing the full conditional distribution $p(z \mid \text{photometry})$, with neural density estimation (NDE) techniques \citep[e.g. PZFlow][]{Crenshaw2024} enabling flexible, well-calibrated redshift PDFs via maximum likelihood training. Complementing catalog-based methods, image-based inference circumvents the information bottleneck imposed by aperture photometry by operating directly on multi-band pixel data, delegating feature extraction to deep neural networks that leverage morphology and spatial structure inaccessible to catalogs. The DeepDISC framework \citep{Merz23DeepDISC,Merz25DeepDISCpz} exemplifies this approach, integrating object detection, segmentation, and redshift estimation into a unified pipeline using vision transformers (MViTv2) as the backbone feature extractor coupled with Mixture Density Networks for probabilistic PDF estimation. \\
Despite their sophistication, \textit{all supervised ML methods remain fundamentally limited by the quality and representativeness of their spectroscopic training samples}: spectroscopic incompleteness, magnitude-limited surveys, and selection biases induce systematic offsets and distortions in the learned photo-$z$ mapping, particularly at faint magnitudes and high redshifts where spectroscopic follow-up is most incomplete \citep{newman2022}. This represents the main challenge for photo-$z$ today and has motivated dedicated calibration strategies.
A further problem is the ``implicit prior'' imposed by each photo-z method \citep{schmidt2020}.
These priors, which have a large impact on photo-z estimates, are opaque and difficult to quantify, making it difficult to compare and combine photo-z posteriors provided by different methods.


\paragraph{Calibration Strategies to Account for Covariate Shifts}  Self-Organizing Maps (SOMs) have emerged as the preeminent unsupervised learning technique for diagnosing and mitigating the biases caused by covariate shifts by performing non-linear dimensionality reduction of photometric feature vectors onto a discrete two-dimensional grid. SOM-based calibration approaches, such as those deployed in DES Y3 \citep{Myles2021} and KiDS analyses \citep{vanDenBusch2022}, directly assign photometric galaxies the empirical redshift distribution of spectroscopic galaxies in their SOM cell while down-weighting or even rejecting regions of color space poorly represented in the spectroscopic catalog. More sophisticated SOM-guided data augmentation strategies selectively populate under-represented SOM cells with simulated galaxies from mock catalogs improving ML model performance where spectroscopic coverage is deficient \citep{Zhang2025}. An alternative approach to SOM for covariate shift mitigation (and without using data augmentation) is represented by stratification by the propensity score (defined as the probability of a covariate vector to be admitted as part of the training set) of both training and target data. Within each propensity score group, supervised photo-$z$ can proceed with any method of choice. This \textit{StratLearn} approach is theoretically guaranteed (under some conditions) to cancel covariate shift~\citep{Autenrieth_2023}. It has demonstrated state-of-the-art performance in the PLAsTiCC SNIa classification challenge, a factor of $\sim 2$ improvement in photo-$z$ calibration from the cosmic shear KiDS+VIKING-450 dataset~\citep{Autenrieth_2024} and a reduced fraction of catastrophic errors and one order of magnitude improvement of the bias for simulated photo-$z$ reconstruction~\citep{Moretti_2025}.

\paragraph{Hybrid Template-Based Estimators}
In contrast to empirical photo-$z$ estimators, a broad class of ``template-based'' photo-$z$ estimators attempt to circumvent the problem of covariate shift using physical models of galaxy SEDs \citep{eazy,lephare}.
These estimators trade the problem of covariate shift for the problem of model misspecification.
Hybrid methods, however, attempt to combine the strengths of empirical and template-based estimators by deriving SED templates in a physics-informed, data-driven manner \citep{budavari2000,Csabai2000}.
These models have been shown to deliver higher-quality photo-$z$ estimates than traditional template-based estimators while suffering less from covariate shift than pure empirical methods \citep{crenshaw2020,li2025}.
They do not perform as well in-distribution as pure empirical methods, however, and still rely on spectroscopic calibration sets.
It may be possible to remedy these defects by implementing hybrid, physics-informed models in deep learning frameworks to enable self-supervised learning without reliance on spectroscopic data sets \citep{2021Boone_ParSNIP}. 



\paragraph{Population-Level Hierarchical Forward Modeling} Traditional photo-$z$ workflows estimate individual galaxy redshifts and aggregate these posteriors to derive population-level quantities such as ensemble redshift distributions $n(z)$ -- a computationally expensive bottom-up approach prone to biases when combining noisy individual posteriors \citep{Leistedt2016, Malz2021, Malz2022, Alsing2023}. Population-level inference inverts this paradigm by directly targeting the population distribution $P(\boldsymbol{\theta})$ over redshift and physical galaxy parameters (stellar mass, star formation rate, metallicity) as the primary inference objective, leveraging the collective constraining power of the entire photometric dataset while naturally incorporating physical priors on galaxy evolution. These methods rely on forward modeling: generating synthetic photometry from physical parameters via stellar population synthesis (SPS; for a review, see, e.g., \citealp{Conroy:2013, Iyer:2026}) models and comparing the distribution of model photometry to observed data. Classical SPS calculations (as implemented by, e.g., the FSPS and Prospector ecosystem; \citealp{Conroy:2009, Conroy:2010, ConroyGunn:2010, Leja:2017, Johnson:2021, Wang:2023}) are computationally prohibitive for large samples, motivating neural network emulators like \href{https://github.com/justinalsing/speculator/tree/master}{Speculator} \citep{SPECULATOR} that achieve $\sim10^{3}$--$10^{4}\times$ speedups with negligible accuracy loss. The \href{https://github.com/Cosmo-Pop/pop-cosmos}{pop-cosmos} framework \citep{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025} exemplifies this approach: it defines a probability distribution over a 16-dimensional SPS parameterization using a score-based diffusion model calibrated on $\sim$420,000 galaxies from COSMOS2020 \citep{Weaver:2022} with 26-band photometry spanning deep UV to mid-IR. 
%The \href{pop-cosmos}{https://github.com/Cosmo-Pop/pop-cosmos} 
This model enables direct estimation of tomographic redshift distributions, and, when used as a data-driven prior in SED fitting, highly accurate individual galaxy redshift inference.

\paragraph{Benchmarking and Evaluation Frameworks}
As AI methods become increasingly central to cosmological analyses, it is critical to develop robust frameworks for testing and validation that ensure reproducibility and enable systematic comparison of different approaches.
For this purpose, the DESC has developed RAIL (\textit{Redshift Assessment Infrastructure Layers}), an open-source, Python-based framework to support large-scale photometric-redshift workflows for the Vera C. Rubin Observatory's Legacy Survey of Space and Time.
Although RAIL is not itself an AI algorithm, it provides a comprehensive infrastructure that
(i) supplies a unified API and modular pipeline stages to train, apply, and compare a broad range of redshift estimators (catalog-based, image-based, probabilistic),
(ii) embeds evaluation modules and metrics for both individual-galaxy redshift and ensemble PDFs ~\citep{RAIL_2025}, and
(iii) enables data challenges to test the robustness of photo-z estimators to a wide array of systmatic errors.
This standardized framework facilitates reproducible results and fair benchmarking across different methods, which is essential for validating AI techniques in preparation for LSST data.


\subsubsection{Strong Lensing}
% \note{Text written by Stefan Schuldt, with help from Clecio de Bom, Sydney Erickson, Martin Millon and Padma Venkatraman. Comments are welcome!}
\label{sec:strong_lensing}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{cnn}, \meth{transformer}, \meth{sbi}, \meth{anomaly-detection}  \\
\themebullet \themekey{Challenges.} \challenge{data-sparsity}, \challenge{systematics-modeling} \\
\themebullet \themekey{Opportunities.} Multi-survey cross-matching (Roman+LSST+Euclid), population-level inference, automated discovery, subhalo constraints from anomalous flux ratios
\end{ThemeBoxA}

Strong gravitational lensing is a rare astrophysical phenomenon where the light of a distant object, the source, is deflected by the gravity of an intervening structure, the lens, forming multiple images of the background source. In galaxy-galaxy strong lensing, both the source and the lens are individual galaxies, while on larger scales, the lens could range from a group to an entire galaxy cluster. Despite their rarity, which is a result of the required near-perfect alignment of the source, lens, and observer, lensed systems are powerful cosmological probes. They offer unique opportunities to study the dark matter fraction and distribution on sub-galactic scales within the lens. Furthermore, the lensing magnification enables the study of high-redshift sources, offering insights into phenomena such as early galaxy evolution.

One particularly powerful application is time-delay cosmography, which uses strongly lensed transients to measure cosmological parameters such as the Hubble constant ($H_0$)~\citep[e.g.,][]{tdcosmo2025}. LSST is poised to revolutionize this field \citep{erickson_de_2025}. This approach yields a geometrical measurement and provides an independent method for resolving the Hubble tension, that is, the significant discrepancy between measurements of the Hubble constant from supernovae \citep[e.g., SH0ES;][]{riess22} and early-universe measurements from the Cosmic Microwave Background \citep[Planck;][]{planck20}. ML/AI models have been proposed for time-delay estimation from light curves~\cite{cuevas06,cuevas2010uncovering,otaibi16}, particularly kernel-based methods, which are the core of Support Vector Machines (SVMs). Moreover, a combination of time-delay lenses and large samples of static lenses from LSST has been demonstrated to enable a competitive dark energy measurement \citep{shajib_SL_2025}. LSST's impact on time-delay cosmography will be transformative, providing time-domain coverage for $\sim100$ more systems than current surveys \citep{abe_2025}. Beyond this, it will also serve as a unique discovery opportunity for new strong lenses, with forecasts of $170,000$ systems \citep{collett15}, two orders of magnitude more than are currently known. This large sample will not only be crucial for building the statistical power required by various science applications, but also for detecting a significant number of currently rare systems, such as double-source-plane lenses, lensed supernovae, and larger-mass-scale lenses containing multiple background sources. Finally, LSST will deliver high-quality ground-based images in six filters, enabling the analysis in multi-band imaging. 

\paragraph{Supervised Detection in the Low Data Regime} Given the rarity of strong lensing events, identifying them among billions of cutouts is inherently challenging for visual inspection in wide-field surveys. While early automated detection approaches relied on curvature-based features \citep{estrada2007systematic} and arc-characterizing descriptors \citep{de2012metodo, bom2017neural}, the advent of convolutional neural networks (CNNs) led to state-of-the-art performance in lens finding \citep{2018A&A...611A...2S, 2019MNRAS.482..807P, 2018MNRAS.473.3895L}. Building on this progress, \citet{metcalf19} launched a lens-finding challenge using Bologna Lens Factory simulations based on the Millennium data \citep{Lemson2006}, showing that LSST-like ground-based multi-band images are well suited for this task. A subsequent Euclid-like challenge produced a winning algorithm combining multi-resolution CNNs \citep{bom2022developing}, later validated on real data by \citet{melo25} using Legacy Survey and HST images to mimic LSST–Euclid synergy. Leveraging multiple networks classifications as ensemble lens classifiers showed improved results over a single network classifier \citep[e.g.][]{andika23, schuldt23b, gonzalez25}, while \cite{holloway_2024} incorporated citizen science annotations to enhance the performance. Because too few real lenses exist for supervised training, realistic mock datasets are essential. \citet{schuldt21} proposed simulating only the lensing effect on real galaxy images, now a standard practice also adopted in the ongoing LSST DESC and Strong Lensing Science Collaboration challenge (Bom et al., in prep.). In preparation for LSST, HSC data \citep{aihara18} -- with similar filters and pixel scale -- have been used to develop and compare models \citep[see e.g.,][]{shu22, andika23, canameras24, jaelani24, more24}. While early efforts focused on simple CNN or ResNet architectures, foundation models such as Zoobot \citep{ZoobotRelease2023} have recently achieved strong results on Euclid imaging \citep{walmsley25, lines25}, and will be adopted for LSST (see Sect.~\ref{sec:foundation_models}). Finally, ML methods are now expanding beyond galaxy-scale lenses to systems involving entire clusters \citep[][Bazzanini et al., in prep.]{schuldt25} and galaxy–galaxy lenses within clusters \citep{angora23}.

\paragraph{Simulation-Based Inference (SBI)} Beyond lens finding, \citet{hezaveh17} pioneered the use machine learning models to predict characteristics of strong lensing systems. Specifically, \citet{hezaveh17} showed that simple CNNs can be used to predict parameters of the lens (Einstein radius, position, ellipticity and angle) from images from the Hubble Space Telescope with a precision comparable to that of traditional methods. \citet{Perreault:2017} proposed using approximate Bayesian Neural Networks to obtain calibrated estimations of the marginal posterior of these lens parameters, an approach applied to ALMA observations in~\citet{Morningstar2018}. \citet{Legin2021, Legin2023} compared this approach to NLEs, demonstrating potential for better calibration in 2-stage SBI methods. The following years saw significant progress using HSC images to prepare for LSST \citep[e.g.,][]{pearson19, schuldt21, gentile23, schuldt23a, schuldt23b, gawade25}. Following earlier work by \citet{Park2021,Wagner-Carena2021}, \citet{erickson25} applied (sequential) Neural Posterior Estimation (NPE) within a hierarchical framework to model strongly lensed quasars, testing on real systems discovered by DES and followed-up with HST high-resolution imaging, and \citet{venkatraman_2025} applied hierarchical NPE modeling to simulated LSST i-band coadds.  Ongoing DESC work examines how modeling an uncertain sample of static galaxy-galaxy lenses with ML enables new cosmological constraints (Holloway et al. in prep.), leveraging the method demonstrated by \citep{li_gg_SL}.
%Furthermore, \citet{schuldt23b} made a first direct comparison between the model predicted quantities obtained in the classical way without machine learning using real HSC images. The classical procedure requires dedicated software that fits typically profiles to the observed image by minimizing the difference though Monte Carlo Markov Chain sampling, or similar sampling techniques. This makes it computationally very expensive and requires also significant human input time, such that it is not scalable to the full sample expected by LSST. 
Finally, \citet{filipp25} applied a sequential NPE approach to detect dark-matter clumps in strong-lensing systems, exploiting the sensitivity of lensing to total mass.
 
\paragraph{High Dimensional Inverse Problem for Lens Modeling}  The task of lens modelling, that is, predicting surface brightness of background sources and density maps of foreground lenses is, in its simplest form, a non-linear inverse problem involving a handful of parameters ($\sim 10-20$). However, as the quality and resolution of data increases, such parametric description of lensed object becomes too simplistic, and more complex parametrization become necessary to avoid biases. An example of such a parametrization that is particularly well-adapted to ML applications are pixelated images of sources surface brightness and projected densities of lenses. Traditionally, it has been difficult to characterize appropriate priors analytically on such high-dimensional spaces~\citep[see, e.g.,][]{Suyu2006, WarrenDye2005, Birrer2015, Delaunay2009, Nightingale2018}, however, recent advances in high-dimensional inference with deep learning has made progress on this front possible. \\
An initial attempt at solving the source reconstruction problem was presented in~\cite{Morningstar2019}, and extended in ~\cite{AlexAdam2023} to enable joint modeling of generalized pixellated lens densities and sources surface brightness. However, while these models provided high-fidelity MAP estimates, they lacked the crucial ability to quantify uncertainties. Approaches based on variation inference have also been proposed in~\citet{Chianese2020, Karchev2022GP, Mishra-Sharma2022}.\\
Advances of, e.g., \cite{Song_2019, Ho_2020, Song:2020, Song_2021, 2022Yang_Diffusion}, have shown that generative models used as expressive, data-driven priors are a promising alternative to address this problem. \cite{adam2022posterior, Karchev2022Diffusion} used score-based models (SBMs) as flexible priors in an explicit inference framework to produce posterior samples of background galaxy sources. In \cite{Barco2025blindinversion}, this method was extended to allow joint sampling the source and lens parameters for smooth, parametric lenses. Such methods have been shown to alleviate known biases in lens parameters induced by misspecified traditional priors, and methods have been proposed to empirically adapt initially biased SBM priors to correct for, e.g., population-level evolution of galaxy morphologies~\cite{Barco_2025}, and to empirically extend misspecified physical models~\cite{Payot2025}. More recently, \cite{RonanSLACLenses} has shown that SBM priors can be leveraged in a Gibbs sampling scheme to reanalyze HST data from SLACS lenses. Ongoing challenges include increasing the sampling efficiency of these methods to allow modelling a large fraction of the strong lenses expected with LSST.

\paragraph{Leveraging LSST's time-series data} LSST will generate an overwhelming number of transient alerts, making the discovery and characterization of strongly lensed short-lived transients (e.g., supernovae) both difficult and time-critical. Kernel-based methods and probabilistic machine learning models such as Gaussian processes will likely play a major role in time-delay inference for lensed quasars \citep[e.g.][]{cuevas06, cuevas2010uncovering, hojjati14, otaibi16, tak17} and supernovae \citep[e.g.][]{hayes24, hayes25} discovered by LSST. Temporal deep learning models will also be essential in this area. For instance, \citet{bag24} developed a model using unresolved light-curve data from difference imaging, while \citet{bag25} extended this to full multi-band time series with a 2D convolutional LSTM. \citet{huber24} instead used an LSTM to predict time delays between such transients directly from their light curves, whilst \citet{goncalves25} used an ensemble of CNNs to directly estimate $H_0$ from time series of lensed supernova images and \citet{Campeau2023} demonstrated the potential of NREs to infer $H_0$ from time delays and lens models. Beyond short-lived transients, \citet{jimenez25} modeled microlensing in lensed quasar light curves, and \citet{fagin_light_curves} introduced a Latent SDE framework to jointly model AGN variability and transfer functions, potentially extendable to lensed AGNs for joint inference of time delays and disk parameters.

\subsubsection{Weak Lensing}
\label{sec3:wlss}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{sbi},  \meth{neural-compression}, \meth{differentiable-programming} \\
\themebullet \themekey{Challenges.} \challenge{systematics-modeling}, \challenge{uq-calibration},
  \challenge{scalability}\\
\themebullet \themekey{Opportunities.} Multi-resolution joint processing (Roman+LSST), probabilistic deblending, DeepDISC instance segmentation, physics-informed priors for galaxy morphology
\end{ThemeBoxA}

As light from background galaxies travels through the Universe, its path is deflected by the gravitational potential of foreground matter, inducing subtle shape distortions of observed galaxies that can be statistically measured. This effect provides a direct probe of the total matter distribution in the Universe, making weak gravitational lensing a powerful tools for constraining cosmological parameters such as the matter density $\Omega_m$, the amplitude of matter fluctuations $\sigma_8$, and the dark energy equation of state.
With its unprecedented depth, image quality, and sky coverage, LSST will provide the most precise mapping of the large-scale structure of the Universe to date. This level of precision has two key implications:
(1) It  presents a major opportunity to refine cosmological constraints, motivating the development of advanced inference methods that can fully exploit this high-quality data; (2) It demands rigorous control of systematic uncertainties to ensure unbiased cosmological constraints.

\paragraph{Systematics modeling / mitigation}
Systematic errors such as imperfect shear calibration, photometric redshift uncertainties, spatially varying selection effects, and residuals in the point spread function need to be well characterized. To date, the precision of current cosmological surveys has permitted the use of state-of-the-art prescriptions that capture the dominant effects of these systematics (e.g.\ \citealp{Weaverdyck2021,RodriguezMonroy2022}). However, as forthcoming large-scale structure and weak-lensing data achieve substantially higher statistical precision, a more accurate and detailed characterization of these systematics will be required, at a level of complexity that renders purely analytical treatments intractable.  
ML methods offer a complementary pathway by learning complex non-linear mappings between observational features (for example, properties of individual galaxies, local image quality metrics, PSF residuals, depth maps, shape measurement parameters) and the resulting systematic bias or residual error \citep{Tewes:2018she,Rezaie2020,Pujol:2020wrk}. Neural-network or other machine-learning algorithms can be trained on simulated or calibration data for which the true systematic shifts are known, and can then be tuned to predict, flag or correct for the systematic effect when applied to real survey data \citep{Fluri:2022rvb}. In doing so these methods enable a more optimal separation of systematic contamination from the cosmological signal and thus a cleaner inferred signal with higher robustness.

\paragraph{Clean catalog construction} Systematics such as intrinsic alignments \citep[e.g.][]{Mandelbaum:2006, Mandelbaum:2011, Troxel:2015, Joachimi:2015} can be mitigated by constructing clean source and/or lens catalogs from galaxy populations that are known to be less susceptible to these effects. Scalable inference of galaxy properties that correlate with quiescent populations (such as specific star formation rate, sSFR) can enable the construction of intrinsic alignment-mitigated galaxy samples. For instance, machine-learned generative priors can be leveraged (e.g.\ \href{https://github.com/Cosmo-Pop/pop-cosmos}{pop-cosmos}; \citealp{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}) to estimate per-galaxy sSFR and construct clean catalogs of star-forming galaxies with conservative cuts based on this parameter. This approach, especially when combined with amortized neural posterior inference, is scalable to LSST-sized datasets and is expected to be superior to color-based selections, which are impacted by contamination. Moreover, generative models of the galaxy population can be applied in  a weak lensing context to directly infer the redshift distributions of source catalogs subject to tomographic binning and sample selection criteria, provided that the color--redshift relation is realistic and robust. This provides an alternative to approaches such as SOM-based calibration.

\paragraph{Neural Compression and Simulation-Based Inference}
Traditional weak-lensing analyses follow a two-step pipeline: compress high-dimensional shear or convergence fields into summary statistics, then perform Bayesian inference on these summaries to obtain posteriors over cosmological parameters. The matter power spectrum and shear–shear correlation functions remain workhorse statistics (KiDS-1000 cosmic shear: \citealt{Asgari2021}; DES Y3 cosmic shear: \citealt{Amon_2022,Secco_2022}), but in the Rubin era significant non-Gaussian information becomes available. This has motivated the use of higher-order moments such as the bispectrum and trispectrum \citep[e.g.][]{desy3_moments}, as well as peak counts \citep[e.g.][]{Marques_2024}, persistent homology \citep{prat2025}, and Minkowski functionals \citep[e.g.][]{PhysRevD.85.103513}. While powerful, these handcrafted summaries are not guaranteed to capture all cosmological information. An alternative, enabled by machine learning, is to learn approximately sufficient statistics by optimizing a neural network to compress each weak-lensing map into a low-dimensional representation. \cite{neural_summary_lanzieri_2025} benchmark different loss functions to identify those that yield near-sufficient summaries. Because the likelihood of these learned summaries is unknown, neural density estimators such as normalizing flows are then used to approximate the posterior within a SBI framework. This strategy was first demonstrated on survey data in \citep{Jeffrey:2021} and subsequently applied to recent surveys \citep[e.g.][]{jeffrey2024darkenergysurveyyear, kramsta2025}. However, although information-theoretically superior to two-point statistics, SBI methods have not yet delivered substantial gains in cosmological constraining power in practice, largely because it remains challenging to simulate cosmological fields with sufficient realism to avoid biased posteriors from model misspecification. In addition, neural summaries are notoriously difficult to interrogate: monitoring them for covariate shifts, unmodeled systematics, and failures in specific regions of data space is challenging, which in turn complicates the construction of robust null tests and diagnostic pipelines. 
 
\paragraph{Hierarchical Bayesian Field-Level Inference} With the advent of GPU-accelerated probabilistic programming, it has become feasible to model the full weak-lensing field in a hierarchical framework that links Gaussian initial conditions of the matter density to observed shear maps through an explicit forward simulation model. Proof-of-concept studies have demonstrated this approach in simplified weak-lensing settings \citep[e.g.][]{porqueres2023fieldlevelinferencecosmicshear}, showing substantial gains in constraining power relative to power-spectrum analyses. DESC members have contributed key building blocks for such end-to-end pipelines, including differentiable lensing lightcone constructions \citep{Lanzieri_2023} and accurate, differentiable ray-tracing schemes \citep{Zhou2024}. However, scaling these methods to a full LSST analysis remains extremely challenging: the survey volume and the resolution required for the forward model place stringent demands on memory, compute, and algorithmic efficiency. Ongoing work aims at lifting this bottleneck through distributed simulations over multiple GPUs \citep{Kabalan_jaxDecomp_2025}. In parallel with full forward modeling of the large-scale structure, DESC members have also explored map-based hierarchical inference using lognormal fields \citep{boruah2022mapbasedcosmologyinferencelognormal, Zhou2024Prd}, which is far less computationally demanding but whose ultimate accuracy is constrained by the limitations of the lognormal approximation.
As an alternative approach, DESC members have proposed using diffusion models to learn the forward model of the density field implicitly from simulations, combining this learned prior with an explicit likelihood to constrain against observed shear data \citep{remy2023}, enabling fast reconstruction of high-fidelity mass maps.


%\newpage
%\subsubsection{Dark Matter}
%\begin{ThemeBoxA}[]
%\themebullet \themekey{Methodology.} TODO\\
%\themebullet \themekey{Challenges.} TODO\\
%\themebullet \themekey{Opportunities.} TODO
%\end{ThemeBoxA}

%ADW: Brainstorming by ADW
%star/galaxy classification (old-school, but still important!), simulation-based inference and graph neural networks for stellar stream modeling and measurement, emulators for accelerating SSI


% \newpage
\subsubsection{Galaxy Clusters}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{sbi}, \meth{object-detection}\\
\themebullet \themekey{Challenges.} \challenge{systematics-modeling}, \challenge{covariate-shift}, \challenge{uq-calibration}\\
\themebullet \themekey{Opportunities.} Combination of imaging and catalog data, Hierarchical Modeling
\end{ThemeBoxA}

% Galaxy clusters probe the densest peaks of the universe, and formed relatively late in the history of the universe.  The evolution of their number count is therefore sensitive to dark energy.  Measurements of the galaxy cluster mass function have been the traditional approach to constrain cosmology. AI can facilitate cluster-cosmology in several stages.  
Galaxy clusters trace the most massive peaks of the matter density field and form relatively late in cosmic history, making their abundance and internal properties highly sensitive to the growth of structure and to dark energy. Cosmological constraints from clusters have traditionally relied on measurements of the cluster mass function and its redshift evolution, anchored by calibrated relations between mass and observable proxies. ML methods are now entering this pipeline at multiple stages (cluster finding, mass–observable calibration, and population-level inference) offering new ways to combine imaging, catalog, and multi-wavelength data while retaining control over systematics and uncertainties.


% The first step to performing a cluster cosmology analysis is `cluster-finding'. Several non-AI algorithms exist to find galaxy clusters, including RedMaPPer \citep{rykoff2014, rykoff2016} and WAZP \citep{aguena2021} for optically selected galaxy clusters. Several deep learning-based methods for cluster finding have emerged in the SZ \citep{bonjean2020, lin2021, hurier2021, Meshcheryakov2022}, X-ray [???], and optical \citep{chan2019, grishin2023, grishin2025, tian2025}.%[Lin 2022, Grishin+23; Grishin+25; Tian+25].  
% A benefit of deep learning-based methods is that cluster-finding can operate directly in image spaces, as opposed to only the information in processed catalogs.  These models can potentially learn features in the image that are not captured in the catalog data. In one example, the You Only Look Once (YOLO; \citealp{redmon2015, redmon2016, redmon2018}) architecture was trained to identify clusters with a training sample defined by the RedMaPPer cluster catalog, but also successfully detected false negative cases, verified as true clusters in external X-ray catalogs \citep{grishin2023}. Ongoing work within DESC \citep[e.g.][]{grishin2025} will enable the application of YOLO to LSST data and joint catalogs.
\paragraph{Cluster Finding from Images and Catalogs}
The first step in cluster cosmology is robust identification of cluster candidates. Non-ML algorithms such as RedMaPPer \citep{rykoff2014, rykoff2016} and WAZP \citep{aguena2021} have been widely used to find optically selected clusters from galaxy catalogs. In parallel, deep-learning–based cluster finders have emerged in the SZ \citep{bonjean2020, lin2021, hurier2021, Meshcheryakov2022} and optical domains \citep{chan2019, grishin2023, grishin2025, tian2025}. A key advantage of these approaches is that they can operate directly on images, rather than on pre-processed catalogs, and thus potentially exploit features (e.g.\ diffuse emission, subtle color–magnitude structure, environment) that are not captured in standard catalog-level summaries. For example, a You Only Look Once (YOLO; \citealp{redmon2015, redmon2016, redmon2018}) architecture trained on RedMaPPer clusters was shown to recover not only the training sample but also previously missed systems that were later confirmed in external X-ray catalogs \citep{grishin2023}. Ongoing DESC work \citep[e.g.][]{grishin2025} aims to deploy such models on LSST imaging and joint catalogs, with particular attention to domain adaptation and calibration across surveys.


% Another stage of cluster-cosmology relies on our ability to constrain galaxy cluster masses from observables, often referred to as the ``mass-observable'' relation.  This task has also seen an emergence of deep neural network based approaches, with methodology inferring masses from X-ray signatures \citep{ntampaka2019, krippendorf2024, iqbal2025}, dynamics of cluster member galaxies \citep{ho2019, ho2021, ho2022, wangthiele2025}, and the SZ signature \citep{deandres2022}.  
% A particular contribution of photometric galaxy data to cluster-based cosmology is in the weak lensing mass estimates, which is used to anchor the mass-observable relation.  The DESC tool, Cluster Mass Modeling \citep[CLMM;][]{aguena2021clmm}, currently uses traditional inference methods with MCMC to infer weak lensing masses from radial profiles that assume an underlying mass distribution model, such as the NFW profile.  Ongoing in DESC are the development and testing of likelihood implicit approaches to infer weak lensing masses, specifically an approach called Simulation Based Inference (SBI). 
\paragraph{Mass-Observable Relations and Weak-Lensing Mass Calibration}
Cosmological analyses require accurate and precise relations between cluster mass and observables (richness, SZ signal, X-ray luminosity/temperature, velocity dispersion). Deep neural networks are being explored as flexible mass estimators across multiple wavebands, including X-ray signatures \citep{ntampaka2019, krippendorf2024, iqbal2025}, the dynamics of member galaxies \citep{ho2019, ho2021, ho2022, wangthiele2025}, and SZ measurements \citep{deandres2022}. For Rubin and DESC, photometric galaxy data contribute primarily through weak-lensing mass estimates that anchor mass–observable relations. The DESC Cluster Mass Modeling tool (CLMM; \citealp{aguena2021clmm}) currently infers weak-lensing masses from radial shear profiles using traditional likelihoods and MCMC, assuming parametric mass models such as NFW. Ongoing work within DESC explores alternative SBI approaches at this level, which can in principle incorporate more realistic shear profiles, complex noise, and selection effects without requiring an explicit closed-form likelihood.

% Simulation-based inference enables a computationally efficient approach to derive posteriors for relevant parameters, such as the cluster mass.  Computational efficiency becomes particularly important for analyses where we might want to simultaneously account for models of individual galaxy clusters and the broader galaxy cluster population (e.g.\ Bayesian Hierarchical Modeling).  Recent numerical experiments within indicate consistency between MCMC and SBI posteriors, provided that the SBI does not suffer from model misspecification more than MCMC (Gill et al., in prep.).
% SBI has also been recently used for direct cosmology parameter estimation from observed data vectors, such as from the CMB \citep{cole2022, lemos2023cmb}, shear two-point statistics \citep{kramsta2025, modi2025}, void lensing analysis \citep{su2025}, topological summaries \citep{prat2025}, and galaxy cluster number counts \citep{reza2022, reza2024}.  The latter of these is now being applied to DESC simulations, with a goal of constructing alternative approaches to cluster-based cosmology that can be incorporated into the DESC Cluster Cosmology Pipeline.  The DESC cluster cosmology analysis will need to incorporate more astrophysical effects and probe a larger parameter space than stage-III experiments, which is increasingly inefficient to perform with a Likelihood-based method. The SBI approach will bring improved computation flexibilities and in some cases, efficiencies, for a DESC analysis.
\paragraph{Simulation-Based Inference for Cluster Cosmology}
SBI provides a computationally efficient route to deriving posteriors for cluster-level and population-level parameters directly from simulated data vectors. This is particularly attractive for analyses that must jointly model individual clusters and the cluster population via hierarchical frameworks, where traditional likelihood-based methods become increasingly costly and brittle as the parameter space and model complexity grow. Ongoing work within DESC indicate that SBI can recover cluster weak-lensing mass posteriors consistent with those from MCMC, provided that model misspecification is not worse than in the explicit-likelihood case (Gill et al., in prep.). In addition, SBI has also been demonstrated to produce relevant constraints directly from cluster counts \citep{reza2022, reza2024}, and this approach is now being developed on DESC simulations as alternative, simulation-native pathways to cluster cosmology that can be integrated into the DESC Cluster Cosmology Pipeline. Compared to Stage-III experiments, DESC cluster analyses will need to incorporate richer astrophysical modeling and explore larger parameter spaces; SBI offers the algorithmic flexibility and, in many regimes, the computational efficiency required to meet these demands.

% \newpage
\subsubsection{Supernova cosmology and transients}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{rnn}, \meth{transformer}, \meth{active-learning}, \meth{ensembles}\\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{model-misspecification}, \challenge{data-sparsity}, \challenge{scalability}\\
\themebullet \themekey{Opportunities.} PLAsTiCC/ELAsTiCC simulation infrastructure, DESC leadership in alert broker integration (ALERCE, Fink, ANTARES)
\end{ThemeBoxA}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - methodology
% - challenges
% - unique opportunities
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%% BROAD MOTIVATION/SCIENCE %%%% Photometric classification of SNe Ia necessary to do cosmological inference with Rubin data. LSST will drive discovery rates, population-level studies of rare transients. Methods for large-scale inference are sorely needed. Triaging is still necessary for follow-up classification/science.

%%%% TECHNIQUES %%%%
% - Partial-phase classification and active learning (e.g., RESSPECT) can help for real-time triaging.
% RNNs/transformers 
% ensemble methods for UQ
%   - Challenges for photometric SN~Ia cosmology with AI: distribution shifts/selection bias, use of simulations, uq

%%%% opportunities %%%%

% - DESC's leadership in PLAsTiCC/ELAsTiCC drove the infrastructure to model these observations, now we have good simulations we can take advantage of.

% - Many alert brokers in various stages of AI integration; they're very involved in DESC, and so DESC can uniquely connect data streams to algorithms. 

The Vera C. Rubin Observatory's Legacy Survey of Space and Time (LSST) is expected to detect $\sim$10 million transient and variable objects each night, a thousand-fold increase over current surveys. The sheer volume and cadence of detections renders traditional spectroscopic classification infeasible for most events. This presents a major bottleneck to the identification of pure Type Ia supernova (SN~Ia) samples for cosmological distance measurements, and to constraining the explosion physics of populations of rare and novel phenomena for the first time. To achieve reliable cosmological constraints and meet DESC goals of reducing the systematic uncertainties from light curve modeling below 3\% of existing models, analysis techniques demand \textit{well-calibrated uncertainty quantification}, \textit{adaptive and scalable performance}, and \textit{robustness to covariate shifts and data corruption}.


\paragraph{Spectrophotometric Modeling} Type Ia supernovae (SNe Ia) are broadly homogeneous and viable standard candles, but diversity in their spectro-temporal properties and persistent host-dependent effects \citep{2006ApJ...648..868S,2010MNRAS.406..782S,2010ApJ...722..566L,2010ApJ...715..743K,2013ApJ...764..191H} still limit standardization precision. Modern ML approaches to standardization now focus on data-driven, differentiable, and multi-modal models rather than hand-engineered linear corrections. Already, progress in these directions can be seen in modeling using Probabilistic Auto-encoders~\citep{2022ApJ...935....5S} or Variational Autoencoders (parSNIP)~\citep{2021Boone_ParSNIP}, which predicts time evolving spectra (SEDs) from light curves, and uses a differentiable forward model to compare in observation space. High quality, well-calibrated data has been instrumental in these endeavours, which can be augmented by the LSST/Rubin samples; however, strategies that account for the shifts generated by calibration errors must still be investigated. 

\paragraph{Photometric Classification} The methodological evolution of photometric classifiers from feature-based approaches to end-to-end learning has been driven by the challenge of processing irregular, heteroskedastic observations: \href{https://github.com/LSSTDESC/snmachine}{SNmachine} \citep{SNmachine2016} leveraged a range of feature sets, from physics-based through to non-parametric approaches, coupled with a variety of traditional machine learning techniques to achieve high classification accuracy; SCONE's Gaussian process interpolation \citep{SCONE:2021} creates regular 2D representations from sparse observations; while transformer architectures \citep{2023Pimentel_Attention,2024Allam_Attention,ATAT2024} leverage self-attention mechanisms to handle missing data naturally. Hybrid, physics-informed approaches have also been explored to extract latent features from light curves using generative modeling, which are then used for classification \citep{2021Boone_ParSNIP}. Classification tools have already proven their utility in LSST survey optimization for supernova metrics, with realistic LSST survey cadences (e.g., \href{https://github.com/LSSTDESC/snmachine}{SNMachine}, \citealt{Alves2022, Alves2023}).   

The Dark Energy Survey (DES) pioneered the use of recurrent neural networks for photometrically classifying SNe~Ia for cosmological analysis \citep{Vincenzi:2023,SCONE:2021,Moller:2020,Moller:2022,DESSN:2024}. Propagating the prediction uncertainties from these models through to cosmological constraints remains an open problem. SuperNNova \citep{Moller:2020} addresses the former with an RNN providing calibrated probabilities essential for contamination modeling in dark energy constraints \citep{Vincenzi:2023}. In DES, BNNs and ensemble methods were also tested; for DESC, we can advance fully Bayesian approaches to photometric classification for LSST. %DESC's proximity to survey operations makes it uniquely positioned to advance fully Bayesian approaches to photometric classification for LSST.

\paragraph{Forward Modeling of the Time-Domain Landscape}
Observational modeling led by DESC has catalyzed neural approaches to photometric classification. The Photometric LSST Astronomical Time-Series Classification Challenge \citep[PLAsTiCC][]{2023Hlozek_PLasTicc,knop2023} provided 3.49M test light curves across 18 transient classes using SNANA simulations with LSST OpSim \citep{2014Delgado_OpSim} cadences and realistic observing conditions. The challenge established weighted logarithmic loss metrics prioritizing Type Ia supernovae and kilonovae for DESC science goals \citep{2019Malz_Metric}, with winning solutions employing gradient boosting and ensemble neural networks requiring engineered features \citep{2019Boone_avocado,2023Hlozek_PLasTicc}. The dataset remains a foundational benchmark for time-series representation learning in astrophysics years after its development \citep{Fraga:2024,Masson:2024,CadizLeyton:2025, CadizLeyton:2025:MoE,ROMAE2025}. Building on PLAsTiCC, ELAsTiCC \citep{2023AAS_ELAsTiCC} stress-tested end-to-end broker infrastructure with $\sim$50M alerts streamed in real-time to seven community brokers from September 2022-January 2023. 

\paragraph{Online Learning for Spectroscopic Optimization}
While archival photometric classification will suffice for the bulk of LSST's cosmological needs, inference over partially-obtained data remains crucial for prioritizing spectroscopic targeting before an event has ended. GRU-based recurrent and convolutional neural networks have successfully classified partial-phase synthetic light curves \citep{2019Muthukrishna_RAPID,SCONE:2021,2023Gagliano_FirstImpressions, shah2025b_oracle}, but performance on observed data remains modest. Transformer-based methods are being increasingly used \citep{ATAT2024}, with synthetic pre-training playing a growing role in bridging the simulation gap \citep{2025Gupta_Sims}. Within the DESC, host-galaxy correlations have been shown to improve early classification \citep{2021Gagliano_GHOST}; this has driven data-driven modeling of host-galaxy correlations for the ELAsTiCC challenge \citep{2023Lokken_SCOTCH}, although spurious host-galaxy associations and the small postage stamps of the field contained within the LSST alert packets may limit DESC's capacity for real-time inference using these data.

Beyond real-time classification, active learning faces unique astronomical challenges: objects must be selected for spectroscopic follow-up before informative light curve data are obtained, the untargeted population is substantially dimmer than the spectroscopically-confirmed sample used in training, and labeling costs vary dramatically with object brightness and sky position. RESSPECT, an initial approach to active learning for transient science, implements uncertainty sampling with Random Forest classifiers on Bazin \citep{2011Bazin_LCModel} parametric features, but requires minimum 5 observations per filter, limiting early-time selection \citep{2020Kennamer_RESSPECT}. More recent implementations have refined active learning for early-time Type Ia supernova identification, demonstrating effective follow-up optimization with simulations \citep{Ishida:2019}, real-data a posteriori \citep{Leoni:2022} and real-time observational campaigns \citep{Moller:2025}, the latter revealing the need for training sets containing events beyond supernovae. \href{https://github.com/MichelleLochner/astronomaly}{Astronomaly} \citep{2021Lochner_Astronomaly} introduces personalized anomaly detection by combining isolation forests with human relevance scoring, addressing the fundamental subjectivity of an anomaly label. The approach has been shown to double the rate of anomaly discovery in radio transients \citep{2025Andersson_astronomaly}. However, active learning remains fundamentally limited by the lack of an informative initial training set, such that early random sampling can produce biased or unrepresentative data that propagates through subsequent iterations of learning. This is a fundamental challenge for novelty detection in LSST data.

\paragraph{Prompt Processing with the Transient Alert Brokers}
The seven Rubin Community Brokers implement diverse classification pipelines. ALERCE employs a convolutional neural network for top-level classification from alert postage stamps \citep{2021Carrasco_Stamp}, and a hierarchical Random Forest applied to photometric features for classification along a 15-class taxonomy \citep{2021Sanchez_AlerceLC}. AMPEL uses a four-tier system with predominantly gradient-boosted random forests \citep{2025Nordin_AMPEL}, and Fink deploys multiple classifiers for early and late-time classification \citep{Fraga:2024,Leoni:2022, Moller:2020, Moller:2025, fink}. ANTARES employs multi-stage filtering with community-contributed Python classes for tagging sources \citep{2018Narayan_ANTARES}, while LASAIR integrates a boosted decision tree classifier from host galaxy properties \citep{2020Smith_ATLAS} with MOC-based watchmaps for coordination with 4MOST/TiDES, which will providing 35,000 transient spectra for SN~Ia cosmology \citep{2024Williams_Lasair}. These brokers have successfully demonstrated minute-scale latencies in processing millions of alerts during the ELAsTiCC campaign, with classification probabilities reported through standardized Avro schemas that enable systematic evaluation of heterogeneous ML architectures. DESC members are involved in all seven Rubin Community Brokers, offering substantial potential for shared software infrastructure for processing the LSST alert stream.

%The Rubin Community Brokers are \texttt{ALERCE} \citep{alerce}, \texttt{AMPEL} \citep{ampel}, \texttt{ANTARES} \citep{antares}, \texttt{Babamul}, \texttt{Fink} \citep{fink}, \texttt{LASAIR} \citep{lasair} and \texttt{Pitt-Google} and will be crucial for both endeavors, receiving, processing and filtering the 10 million nightly detections from LSST every night. They are crucial for both creation of samples for scientific analysis and the coordination of follow-up observations. Brokers have a strong AI/ML component with multiple classifiers targeting various supernovae types, AGNs, and other transients. Citations needed of ML algorithms developed in DESC in these brokers.

\paragraph{Cosmological Inference using Type Ia Supernovae}

Hierarchical Bayesian models have been applied to type Ia supernovae for various goals, including cosmological inference \citep[e.g.][]{2011March_BAHAMAS, 2016Shariff_BAHAMAS, 2015Rubin_Unity, 2025Rubin_Unity, 2018Feeney_H0}, constructing empirical SED models \citep[\href{https://github.com/bayesn/bayesn}{BayeSN}; ][]{2009Mandel_BayeSN, 2011Mandel_BayeSN, 2022Mandel_BayeSN, 2021Thorp_BayeSN, 2023Ward_BayeSN, 2024Grayling_BayeSN, Uzsoy_2024}, modeling intrinsic colors and dust extinction \citep{2017Mandel_SimpleBayeSN, 2022Thorp_BayeSN, 2024Thorp_BayeSN}, handling uncertain photometric classifications \citep{2007Kunz_BEAMS, 2012Hlozek_BEAMS} and redshifts \citep{2017Roberts_zBEAMS}, and modeling the spectrophotometric standards used in photometric calibration \citep{2025Boyd_DA, 2025Popovic_Dovekie}. However, in the LSST era such models will have to incorporate complex effects that cannot easily be treated analytically; for example, selection effects, photometric classification and photometric redshifts. Work is ongoing to enhance our statistical models using simulation-based inference (SBI), leveraging the flexibility of neural networks to capture these complex effects \citep[e.g.][]{2024_Boyd_Flows, 2024Karchev_SIDEreal, 2025Karchev_CIGARS}. SBI will enable scalable and principled statistical inference of cosmological parameters with LSST. 

\subsubsection{Theory and Modeling}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{emulator}, \meth{gaussian-process}, \meth{neural-surrogate}, \meth{differentiable-programming}, \meth{sbi} \\
\themebullet \themekey{Challenges.} \challenge{scalability}, \challenge{model-misspecification} \\
\themebullet \themekey{Opportunities.} Flexible emulation frameworks, model selection and hypothesis testing, efficient construction of realistic mock datasets, gradient-based sampling.
% I edited this, feel free to change - Dani Leonard
\end{ThemeBoxA}

The role of theory and modeling within DESC is to provide the essential bridge between cosmological parameters and the statistical observables derived from LSST data. Accurate theoretical models are required to translate measured galaxy shapes, positions, and fluxes into constraints on dark energy, dark matter, and gravity. This entails constructing predictive models of large-scale structure, galaxy bias, baryonic physics, and lensing observables that can be robustly compared with data while marginalizing over astrophysical and observational systematics. As the scale and precision of LSST data demand modeling at unprecedented accuracy and speed, machine learning-based emulators, differentiable theory libraries, and simulation-based inference approaches are increasingly central to this effort, enabling fast and robust connections between data and theory.


\paragraph{Fast Surrogates for Cosmological Likelihoods} 

Emulation and related methods for creating fast-surrogate models using ML and AI will be of crucial importance in accelerating inference pipelines for DESC’s cosmological analyses.  Emulation is an indispensable tool for integrating aspects of modelling which by nature require simulation too slow to ever consider incorporating directly in sampling (e.g. those requiring N-body or hydrodynamical simulations). At the same time, even for aspects of modelling which are more moderate in evaluation cost (seconds rather than many-hours), emulation allows us to dramatically accelerate individual likelihood evaluations. This is one of the key ways we can make computationally feasible the sampling in high-dimensional parameter spaces which will be required for DESC analyses.

Work on these emulation techniques within DESC has included directly building emulation tools, particularly outside of wCDM models \citep{ramachandra2021matter}, as well as using emulators to efficiently evaluate modelling choices for LSST data \citep{boruah2024machine}. DESC's theoretical modelling package {\tt pyCCL} \citep{chisari2019core} natively supports key matter-power-spectrum emulation tools {\tt baccoemu} \citep{arico2021bacco} and {\tt cosmicemu} \citep{moran2023mira}. However, developing frameworks for DESC to analyze models outside wCDM in the nonlinear regime of LSST data remain a challenge that need to be addressed \citep{Ishak:2019BwCDM} and where AI can play a major role.  

Lightweight, highly accurate neural network models can take on the heavy lifting associated with speeding up massively-parallel computations that are inherently faster compared to the above examples (seconds per computation) but where the use cases require millions or billions of evaluations, leading to major computational bottlenecks. Examples include the construction of mock galaxy catalogues with realistic photometry, and inference of SPS parameters via SED-fitting to photometry for very large galaxy catalogues. An example use-case within DESC is \href{https://github.com/justinalsing/speculator/tree/master}{Speculator} \citep{SPECULATOR}, which is used within the \href{https://github.com/Cosmo-Pop/pop-cosmos}{pop-cosmos} generative model \citep{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}. Speculator accelerates SPS calculations in pop-cosmos by factors of 10,000x on GPU hardware, relative to Flexible Stellar Population Synthesis \citep{Conroy:2009, Conroy:2010, ConroyGunn:2010, Johnson:2021}  on CPUs.


Looking to the future, DESC would benefit from developing mechanisms to enable emulation which has more flexibility with respect to modeling components. Our current methods of building a single emulator from scratch per modeling-set-up is high-cost (computationally and in terms of person-power). This does not scale well for enabling adaptation to new systematics modeling or inference in models outside of wCDM. Considering approaches which use meta-learning (e.g. \citealt{macmahon2025meta}) or which are philosophically aligned with foundation models would be of value. 

Another area of growth where fast emulators can make major impact is in model selection and hypothesis testing of beyond wCDM models. Instead of being limited by the cost of theoretical model evaluations, future analyses will be constrained by how efficiently inference pipelines can navigate and compare competing cosmological models. Integrating these emulators within agentic AI systems (\autoref{sec:llm_agentic})—which can autonomously refine training data, adapt inference strategies, leverage tools for evidence computation and simulation-based inference, and even propose new model extensions—will further accelerate discovery. For LSST-DESC, this synergy will transform the capacity to test gravity, dark energy, and dark-sector interactions, turning high-quality data into a powerful engine for identifying new physics. 





\paragraph{Differentiable Programming for Accelerated Sampling}

The jax-cosmo library \citep{JAX-COSMO} enables differentiable and hardware-accelerated cosmological computations. Together with a NumPy-compatible API and close integration with tools such as NumPyro \citep{phan2019composable} and JAXopt \citep{jaxopt_implicit_diff}, this makes jax-cosmo a practical framework for building scalable and fully differentiable cosmological models. Since its first release, jax-cosmo has been used in several contexts. \cite{10.1093/mnras/stae2138} employed it to build a differentiable forward model for testing gradient-based samplers such as Hamiltonian Monte Carlo (HMC) and No-U-Turn Sampler (NUTS) \citep{HoffmanGelman2014}, finding improved efficiency compared with standard Metropolis–Hastings methods. \cite{CosmoPower} combined jax-cosmo with neural-network emulators in CosmoPower-JAX, enabling high-dimensional Bayesian inference through automatic differentiation and GPU acceleration. The halox package \citep{halox} uses jax-cosmo for cosmological calculations such as power spectra and distance measures in its modeling of dark-matter halo statistics. \cite{2025arXiv250707833S} used jax-cosmo as a benchmark for testing a differentiable Fisher-information approach based on score matching. Recently too, \cite{bartlett2025symbolic} have developed a symbolic emulator that leverages genetic programming-based symbolic regression to derive compact, analytic expressions for cosmological observables, including the radial comoving distance, linear growth factor, and nonlinear matter power spectrum. When integrated into the jax-cosmo framework, these symbolic approximations replace computationally expensive numerical evaluations—such as CAMB \citep{Lewis_2000} or HALOFIT (eg. \citealp{Takahashi_2012})—with differentiable, GPU-accelerated formulas, achieving sub-percent accuracy while reducing runtime by over an order of magnitude. This seamless integration enables efficient, gradient-based inference pipelines, such as those using Hamiltonian Monte Carlo, without sacrificing precision or interpretability.


\subsubsection{Cosmological and Survey Simulations}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{emulator}, \meth{diffusion-model}, \meth{differentiable-programming}, \meth{sbi}\\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{systematics-modeling} \\
\themebullet \themekey{Opportunities.} Joint modeling of galaxies and environments, inference at catalog and field level, Modular components for DESC simulators, survey-scale generative models, survey design, systematics mitigation, pipeline stress tests.\end{ThemeBoxA}

Cosmological simulations are a fundamental tool to not only validate analysis pipelines, but increasingly to provide “theory” samples for SBI frameworks. Producing mock survey data at the scale and accuracy required for LSST science remains a major challenge, which machine learning can help address by emulating expensive numerical predictions and by providing data-driven models of otherwise poorly constrained aspects of the galaxy population. Neural emulators have long been used as fast surrogates for non-differentiable components of cosmological forward models (e.g.\ summary statistics of N-body simulations as in CosmicEmu \citep{CosmicEmu}, Aemulus \citep{2019ApJ...875...69D}, CosmoPower \citep{CosmoPower}, or 21cmEMU \citep{21CMEMU}) and more recently for accelerating stellar population synthesis (SPS) calculations via models such as \textsc{Speculator} and \textsc{ProMage} \citep{SPECULATOR,ProMage}. Extending these approaches to additional components of the simulation pipeline holds the promise of greatly increasing the dynamical range, realism, and flexibility of LSST mock catalogs at manageable computational cost.

\paragraph{Population-Level Generative Models for Realistic Galaxy Catalogs}
Diffusion-based generative models operating in the space of physical galaxy parameters provide a powerful route to building realistic mock catalogs that remain anchored in deep-field observations. The \href{https://github.com/Cosmo-Pop/pop-cosmos}{\textsc{pop-cosmos}} framework \citep{Alsing:2024,Thorp:2025,Deger:2025} defines a score-based diffusion model over a high-dimensional SPS parameterization (star-formation histories, metallicities, dust, nebular emission, etc.), calibrated on $\sim$420,000 galaxies from COSMOS2020 \citep{Weaver:2022} spanning 26 bands from UV to mid-IR. Rather than directly emulating observed fluxes, \textsc{pop-cosmos} learns a data-driven prior $p(\boldsymbol{\theta}_{\mathrm{SPS}},z)$ over physical parameters and redshift that reproduces the joint distribution of observed photometry. This model encodes realistic priors on star-formation histories over cosmic time, and learn the evolution of the star-forming sequence. When coupled to survey-specific selection functions and noise models, \textsc{pop-cosmos} can therefore generate realistic mock galaxy catalogs that inherit both empirical constraints from deep multi-wavelength data and the flexibility of generative modeling. Within DESC, such population-level priors are particularly valuable as building blocks for photometric-redshift calibration, for testing SED-fitting pipelines, and for providing consistent galaxy populations to survey simulators such as Diffsky \citep{OpenUniverse2024}.

\paragraph{Differentiable Empirical Galaxy–Halo Forward Modeling}
Complementary to purely catalog-level generative approaches, differentiable galaxy–halo forward models seek to describe galaxy populations as conditional generative processes on top of dark-matter structure. The \textsc{Diffsky} framework \citep{OpenUniverse2024} rebuilds the traditional “halo $\rightarrow$ SFH $\rightarrow$ SED’’ chain using differentiable, physically interpretable blocks. \textsc{Diffmah} \citep{Diffmah} provides a JAX-based, few-parameter model of halo mass assembly $M_{\rm halo}(t)$, replacing noisy merger trees with smooth, analytic, differentiable growth histories. On top of this, \textsc{Diffstar \citep{Diffstar}} models in-situ star formation histories with a small set of parameters (e.g.\ star-formation efficiency, gas-consumption timescale, quenching time), while \textsc{DiffstarPop} \citep{Diffstar} lifts this to the population level by learning the statistical link between SFH parameters and halo assembly across suites of reference simulations. Finally, DSPS \citep{DSPS} maps these SFHs and associated metallicity/dust parameters to SEDs and photometry entirely within JAX. Together, \textsc{Diffmah} + \textsc{Diffstar}/\textsc{DiffstarPop} + DSPS replace merger trees, non-differentiable semi-analytic recipes, and black-box SPS calls with a modular, probabilistic, fully differentiable stack whose low-dimensional, physically meaningful parameters can be calibrated and explored with gradient-based methods and SBI, while still generating large, realistic synthetic catalogs.


\paragraph{Differentiable Cosmological N-body solvers} Recent years have seen the emergence of particle–mesh $N$-body solvers implemented in modern, GPU-accelerated, deep-learning frameworks that support automatic differentiation \citep[e.g.][]{FlowPM, pmwd, DISCO-DJ, JAXPM}. Automatic differentiation enables hierarchical Bayesian inference directly over forward simulations of large-scale structure, opening a path toward full-field inference and near-optimal extraction of cosmological information. The main obstacles to deploying these methods at LSST scale are the computational and engineering demands of simulating survey-sized volumes in a differentiable way. Computing derivatives through the simulation implies non-trivial memory costs that are difficult to satisfy under the constraints of GPU accelerators. Several complementary strategies are being developed to address this challenge, including multi-node domain decomposition for distributed simulations \citep{Kabalan_jaxDecomp_2025}, techniques to reduce the memory cost of gradient evaluation \citep{Li__2024}, and improved time integrators that achieve a given accuracy with fewer time steps \citep{Rampf2025}. Beyond enabling full-field inference, differentiable simulations also make it possible to combine physics-based solvers with learned components, yielding hybrid schemes that can improve the speed and accuracy of particle–mesh simulations \citep{Lanzieri_2023, Payot2023}.

\paragraph{Hydrodynamical-Simulation-Based Mappings of Galaxy Properties}
A complementary strategy is to treat state-of-the-art hydrodynamical simulations as high-fidelity “teachers’’ and use machine learning to distill their complex, small-scale physics into fast, effective models defined directly on dark-matter fields. Rather than specifying parametric galaxy–halo or SPS models, these approaches learn mappings from halo or large-scale-structure descriptors to galaxy properties as realized in the simulations. Recent work on intrinsic alignments illustrates this paradigm. A traditional approach, followed by \citep{VanAlfen2024} is to develop an empirical IA model constrained by hydrodynamical simulations within a flexible HOD-like framework. A more ML-oriented approach is to learn an end-to-end emulator of galaxy properties as demonstrated in \citep{Jagvaral2025} introduce a geometric deep-learning approach in which galaxy shapes and orientations from IllustrisTNG \citep{IllustrisTNG} are modeled using E(3)-equivariant graph neural networks defined on the cosmic web. In the latter, galaxy orientations are treated as elements of the Lie group $\mathrm{SO}(3)$, and a diffusion-based generative model on $\mathrm{SO}(3)\times\mathbb{R}^n$ learns the conditional distribution of shapes and orientations given halo mass, environment, and tidal field. This yields a fast, simulation-calibrated surrogate capable of reproducing intrinsic-alignment statistics at the percent level, providing a route to embedding hydro-level realism into DESC mock catalogs without rerunning expensive hydrodynamical simulations.


\subsection{Impact at the Technical Level}

\subsubsection{Deblending}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{vae}, \meth{npe}, \meth{instance-segmentation}, \meth{diffusion-model}, \meth{normalizing-flow} \\
\themebullet \themekey{Challenges.} \challenge{blending-crowding}, \challenge{metrics} \\
\themebullet \themekey{Opportunities.} Multi-survey training, Data-driven Priors, Generative Models 
\end{ThemeBoxA}

Turning pixels into objects is a fundamental problem in astronomical survey pipelines. Object detection and deblending of LSST data is a crucial step in producing catalogs useful for DESC science.  Given its unprecedented depth for a ground-based survey, LSST will face new challenges in its detection pipelines compared to previous legacy surveys \citep{Melchior21blending}.  Blending, or the overlapping of source light profiles, is an imaging systematic that affects all downstream analysis, as it becomes difficult (in reality intractable) to disentangle photons from a given source in a blend.  This problem is exacerbated with increased observing depth, as more light is collected from sources that are overlapping due to line-of-sight projections or physical interactions. Traditional object detection pipelines for wide-field surveys use a maximum likelihood estimator method \citep{Bosch18hsc} to identify peaks in intensity corresponding to sources in an image.  This method, while statistically justified, is still subject to failure modes, wherein AI can provide alternative and complimentary methods.  Similarly, traditional deblending algorithms typically rely on models and assumptions about source light profiles that may not provide sufficient flexibility for the billions of sources LSST will observe \citep{Melchior18scarlet}.  DESC has been exploring and developing AI methods to aid in these challenging problems, which are crucial to understand and mitigate.

\paragraph{Catalog-level Blend Identification}  While a majority of sources observed by LSST are expected to have some level of blending, a particularly pernicious case is that of unrecognized blends.  These are sources in a blended scene that are indeed distinct (determined from high-resolution space-based observations), but are only recognized as a single source by cataloging and deblending pipelines.  Unrecognized blends impact measured properties such as galaxy shapes \citep{Dawson16ublends}, photometric redshifts \citep{Liang25catblend}, and more.  Estimates of the level of unrecognized blends in LSST typically lie from $\sim15-30\%$, with analysis of early LSST data compared to catalogs from HST CANDELS yielding an unrecognized blend rate of 18\% \citep{sitcom128}.  Blends that remain at the catalog level are definitionally unrecognized blends but may still be detectable as outliers in the multi-band photometry and their shapes. The use of Random Forests and Self-Organizing Maps along with various anomaly detection algorithms were tested in \cite{Liang25catblend} who showed that it is possible to detect unrecognized blends at a cost to the sample size. These algorithms were used to assign an unrecognized blend probability, however improvements can be made for specific science cases. For example, using the blend entropy [Ramel et al. in prep, Project 284] can improve cluster cosmology by removing the most problematic unrecognized blends for cluster analysis. Designing better blending metrics like blend entropy and incorporating that into machine learning algorithms like gNNs is the main goal of friendly [Project 295]. 

\paragraph{Image-level Deblending Using Deep Learning} Deep learning algorithms designed for object detection and deblending provide an alternative method to traditional pipelines that may help improve catalog completeness and downstream source property measurements.  For instance, the Bayesian Light Source Separator (BLISS) framework utilizes neural posterior estimation alongside training a supervised convolutional neural network to produce probabilistic catalogs directly from simulated LSST images.  This method outperforms the standard LSST pipeline in source detection, as well as downstream flux measurement, star/galaxy classification, and galaxy shape measurement, \citep{Duan25NPE}.  The DeepDISC instance segmentation \citep{Merz23DeepDISC} framework produces object catalogs and segmentation masks from image data, and is being tested with joint Roman-Rubin data to incorporate multimodal information for downstream detection and deblending improvement. Both DebVader \citep{Arcelin21debvader} and MADNESS \citep{Biswas25mad} use Variational Autoencoders to handle blending. They use self-supervised training to learn the structure of isolated galaxies. Through additional training of a deblending encoder they learn to isolate a galaxy from a blend. A specialized decoder can directly measure the characteristics of the galaxy (shape, photo-z) without reconstructing explicitly the image of the isolated galaxy. MADNESS adds to the architecture a normalizing flow to improve the performances by modelling the latent space distribution of galaxies that provides an explicit likelihood for posterior optimization. Ongoing work uses a multimodal VAE to learn both from imaging and spectroscopy, adding more information in the latent space to improve the galaxy characteristics measurement, especially photo-z. The very principles of deblending VAEs alleviate the impact of unrecognized blends, and ongoing work on the use of probabilistic catalogs where the number of detected galaxies is itself non-deterministic will reduce it even more.  Even if blended sources appear in such close proximity that LSST imaging will not be able to recognize the overlap and detect the blended group as one source, it is feasible to model the detected sources first, compute the residuals from the fit, and run detection again on the residuals. Because the residuals can have complicated structure, it is beneficial to perform the detection on multi-band residuals, where unrecognized sources appear as colored, localized over- or underdensities. Recognizing them, as well as their likely centers is possible and fairly effective with computer vision architectures like YOLO \citep{sowmya_kamath_2020_3721438}.

\paragraph{Data Driven Priors for Deblending with Explicit Likelihoods}

Generative models like normalizing flows and diffusion models can be trained on unblended galaxies (potentially with limited amounts for space-based data) and then serve as data-driven priors for galaxy morphologies \citep{Lanusse19gen}. Posterior optimization and sampling becomes possible for inverse problems with explicit likelihood functions (such as inpainting, deconvolution, and deblending). This is particularly effective in low SNR cases, where the deblender scarlet \citep{Melchior18scarlet} which is the default deblending method in the Rubin pipeline, is outperformed by a new, prior-augmented version \citep{Sampson24scarlet2}. The same approach can also perform transient photometry in the presence of a host galaxy without the need for difference imaging \citep{Ward25scarlet2}. Additionally, posterior optimization in latent space by the MADNESS deblender \citep{Biswas25mad}, using data-driven priors, also outperformed scarlet.


% Skipping for now...
% \newpage
% \subsubsection{Instrumental Response Modeling}
% \begin{ThemeBoxA}[]
% \themebullet \themekey{Methodology.} TBD \\
% \themebullet \themekey{Challenges.} TBD \\
% \themebullet \themekey{Opportunities.} TBD
% \end{ThemeBoxA}

\subsubsection{Shape Measurement}
\begin{ThemeBoxA}[]
\themebullet \themekey{Methodology.} \meth{differentiable-programming}, \meth{deep-network}, \meth{sbi} \\
\themebullet \themekey{Challenges.} \challenge{covariate-shift}, \challenge{blending-crowding}, \challenge{uq-calibration}, \challenge{systematics-modeling} \\
\themebullet \themekey{Opportunities.} Joint optimization of detection, deblending, and shear, hybrid analytic–neural estimators,  realistic multi-instrument and multi-epoch scene modeling, active learning, unified shear–photo-z modeling
\end{ThemeBoxA}

Weak-lensing shape measurement is one of the most sensitive components of the DESC analysis pipeline: small percent-level biases in ensemble shear propagate directly into the cosmological parameters targeted by LSST. Meeting DESC requirements therefore demands methods that are simultaneously accurate enough to control multiplicative and additive shear biases, computationally efficient enough to process billions of galaxies, and amenable to calibration. A unifying theme of recent work is to exploit differentiability and GPU acceleration, both in explicit shear-calibration schemes and in forward models.

\paragraph{Analytic Calibration and Differentiable Shear Estimators}
The Analytic Calibration (AnaCal) framework  \citep{Li2023,Li2025_bias} demonstrates how differentiability can be used to obtain high-precision shear responses and noise-bias corrections without relying on large external simulation campaigns. By expressing galaxy properties and pixelized images in terms of differentiable basis functions, AnaCal delivers analytic shear responses for detection, selection, and shape measurement, achieving LSST-grade accuracy with sub-millisecond CPU inference per galaxy. At present, however, AnaCal is implemented as a CPU-based shape-measurement engine that assumes pre-detected sources. Extending this paradigm to GPU-based implementations and integrating it more tightly with detection and scene modeling would unlock orders-of-magnitude throughput gains. More broadly, metacalibration-style schemes also stand to benefit from differentiable image models and measurement operators: with gradients available throughout the pipeline, shear response and noise-bias corrections can be computed faster and more robustly.

\paragraph{Deep Learning–Based Shape Estimators}
Modern deep-learning architectures provide a natural path to end-to-end differentiable shear estimator that can simultaneously integrate detection, deblending, shear estimation, and robustness to image defects. Neural networks are inherently GPU-accelerated, highly parallel, and differentiable, making them well suited to high-throughput shear inference at LSST scale. Such an approach was originally demonstrated in \citep{Ribli2019} and can are being explored in a DESC context using the DeepDISC architecture \citep{Merz23DeepDISC} which was originally designed as a general purpose architecture for detection and segmentation and which can estimate gravitational shears within a single GPU-resident and differentiable model. Being automatically differentiable, this estimator can be calibrated using the schemes mentioned above.

\paragraph{Hierarchical Forward Modeling with Differentiable Image Simulators}
A complementary strategy frames shape measurement as a hierarchical forward-modeling problem, in which cosmological parameters, population-level distributions of galaxy properties, and individual galaxy shapes are inferred jointly from the pixel data \citep{Schneider2015}. In this view, a forward model generates simulated images given a set of hierarchical parameters, and inference proceeds by comparing these simulations to the observed images. Such approaches are made practical thanks to the JAX-GalSim effort \citep{jaxgalsim} which re-implements key GalSim \citep{ROWE2015121} functionalities in JAX, making this forward model fully differentiable and GPU-accelerated while supporting vectorized batch simulations of thousands of galaxies at once. In ongoing DESC efforts, JAX-GalSim is used to implement the hierarchical shear-inference framework of \citet{Schneider2015}, but instead of traditional MCMC, gradient-based samplers (NUTS), GPU-acceleration, and batching can be used to yield roughly an order-of-magnitude speedup while keeping multiplicative shear biases within LSST requirements. While the aforementioned approach relies on analytic surface brightness profiles to model galaxies, more realism can be achieved through projects like Scarlet2 \citep{Sampson24scarlet2} which extends the modeling to non-parametric morphologies and blended scenes observed with multiple instruments, providing a JAX-based, differentiable scene-modeling framework in which gradients of the likelihood with respect to source parameters and hyperparameters are readily available.
