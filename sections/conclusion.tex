\newpage
\section{Summary and Conclusion}
The Vera C.\ Rubin Observatory LSST will generate heterogeneous data at a scale and complexity that strain traditional analysis pipelines. DESC’s mission is to convert these data into robust constraints on the dark sector, which demands methods that are statistically powerful, scalable, and operationally reliable. AI/ML, from neural density estimators for photometric redshifts to simulation-based inference and generative models for field-level cosmology, have \textit{already} demonstrated that they can address key bottlenecks in this program. At the same time, their utility for precision cosmology hinges on trustworthy uncertainty quantification, explicit treatment of model misspecification and covariate shift, and fully reproducible integration into DESC workflows.

Sections 3 and 4 demonstrates that DESC is at the forefront of developing cutting-edge machine learning applications in astronomy. Research into machine learning is now integral to the primary LSST cosmological probes—including strong and weak lensing, supernovae, galaxy clusters, and large-scale structure—as well as cross-cutting topics such as theory, photometric redshifts, simulations, and deblending. Across DESC working groups and the broader cosmology community, several critical themes and methodologies have crystallized:

\begin{itemize}
 \item \textbf{Simulation-Based Inference (SBI):} SBI has emerged as a powerful methodology, enabling analyses of a complexity that typically exceeds the capabilities of traditional forward modeling. This domain offers fertile ground for machine learning research, particularly in the development of emulators to accelerate pipeline components and in extending analyses beyond traditional point statistics. However, SBI remains sensitive to model misspecification—a challenging problem to solve in a machine learning paradigm.
 \item \textbf{Bayesian Methodology and Uncertainty Quantification:} While Bayesian frameworks are ubiquitous in cosmology, machine learning is increasingly being explored to accelerate inference on LSST-scale datasets that would otherwise be computationally intractable. Furthermore, the high precision required by cosmology requires accurate uncertainty estimates which is not common practice in machine learning. Building on the application of Bayesian neural networks and related methods, this an area where DESC is well-positioned to drive fundamental developments in machine learning.
 \item \textbf{Validation and Benchmarking:} For cosmology, rigorous validation is paramount. Algorithms must not only be accurate and unbiased but also capable of correctly propagating uncertainty. Given that covariate shift is inevitable in many supervised learning contexts, it must be mitigated through accurate simulations and domain-adaptation techniques. Benchmarking and validation is particularly important for algorithms used in products intended for broad usage, such as foundation models and simulations. The RAIL project, developed by DESC specifically to benchmark photometric redshift algorithms, serves as an excellent model for such validation frameworks.
 \item \textbf{Active Learning and Discovery:} Active learning has become an essential part of machine learning and will be crucial in managing LSST data. The RESSPECT project, a collaborative initiative developing an active learning pipeline for transient classification, is an example of the comprehensive infrastructure required for effective active learning. Furthermore, human-in-the-loop workflows will be vital for anomaly detection and the identification of rare phenomena within the vast LSST dataset, facilitating novel discoveries that purely automated systems might overlook.
\end{itemize}
%This white paper surveys the intersection of AI/ML with DESC science analyses, highlighting concrete roles for ML in photometric redshifts, strong lensing, weak lensing and large-scale structure, clusters, supernova cosmology and transients, theory and modeling, and survey simulations (\autoref{sec3:use_case_for_aiml}). It then identifies methodological directions where DESC science directly motivates advances in the broader AI/ML ecosystem: Bayesian and simulation-based inference, physics-informed generative models, and novelty-detection frameworks that can operate at LSST scale (\autoref{sec4:aiml_research}). Finally, we emphasize the emerging importance of data foundation models and agentic AI systems (LLMs and multi-agent systems) as cross-cutting technologies, provided their deployment is coupled to rigorous evaluation, governance, and provenance tracking (See \autoref{sec5:emerging_tech}).

Realizing this potential requires DESC to treat AI/ML not as ad hoc accelerators, but as primary components of the measurement pipeline. Sections~6--8 of this paper outline the software, computing, and data infrastructure required to support AI/ML at scale, including a shared AI software stack, containerized and RSP-compatible workflows, a DESC Data Registry for model and data products, and benchmark suites that tie model performance directly to cosmological and systematic-error budgets (see \autoref{sec6:infra_requirements}). We also discuss opportunities for broader coordination with Rubin operations, community brokers, external AI/ML institutes, and industry, as well as risks ranging from model miscalibration and covariate shift to data rights compliance, environmental cost, and the erosion of human oversight (see \autoref{sec7:broader_coordination}).

On the basis of these insights, we identify a small number of high-level actions for the DESC community and its partners:

\begin{itemize}
  \item \textbf{DESC leadership and governance:} Establish a sustained AI/ML governance structure (e.g., an AI/ML Oversight and Policy Board) with responsibility for vetting high-impact AI uses, maintaining standards for uncertainty calibration, benchmarking, and reproducibility, and coordinating cross-working-group pathfinder analyses that demonstrate cosmology-grade AI/ML pipelines end-to-end.

  \item \textbf{Infrastructure providers and external partners:} Work with Rubin operations, LINCC, national laboratories, and computing centers to secure schedulable GPU and storage allocations sized for DESC’s foundation models and simulation-based inference campaigns, integrated with broker infrastructures and survey simulators. Coordinate with full-stream alert brokers and other Science Collaborations to define shared interfaces, provenance standards, and validation datasets that connect AI/ML performance directly to DESC’s science requirements.

  \item \textbf{DESC members and working groups:} Invest in training and documentation so that AI/ML methods, including foundation models and agentic systems, are deployed by teams that understand their assumptions, limitations, and failure modes. Treat human-in-the-loop review, explicit risk registries, and transparent reporting of AI-driven selection and inference as core elements of DESC scientific practice rather than afterthoughts.
\end{itemize}

Taken together, these steps will position DESC to use AI/ML in a way that is both ambitious and disciplined. LSST-era cosmology will not be limited by a lack of algorithms, but by our ability to connect those algorithms to physical modeling, survey simulations, and governance structures that make their behavior legible and trustworthy. By adopting a coherent AI/ML strategy grounded in DESC’s science priorities and supported by robust infrastructure, the collaboration can help shape best practices for AI in fundamental physics, while ensuring that the coming wave of AI tools advances precision dark-energy science without adding complexity to an already challenging problem.