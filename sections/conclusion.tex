\newpage
\section{Summary and Conclusion}
\label{sec:conclusion}

The Vera C.\ Rubin Observatory \acrshort{lsst} will generate heterogeneous data at a scale and complexity that strain traditional analysis pipelines. \acrshort{desc}’s mission is to convert these data into robust constraints on the dark sector, which demands methods that are statistically powerful, scalable, and operationally reliable. \acrshort{ai}/\acrshort{ml}, from \acrshort{nde} for \acrshortpl{photoz} to \acrshort{acr:sbi} and generative models for field-level cosmology, have \textit{already} demonstrated that they can address key bottlenecks in this program. At the same time, their utility for precision cosmology hinges on trustworthy \acrshort{acr:uq}, explicit treatment of model misspecification and covariate shift, and fully reproducible integration into DESC workflows.

\autoref{sec3:use_case_for_aiml} and \autoref{sec4:aiml_research} demonstrate that DESC is at the forefront of cutting-edge machine learning applications in astronomy. Research into machine learning is now integral to the primary LSST cosmological probes—including strong and weak lensing, supernovae, galaxy clusters, and large-scale structure—as well as cross-cutting topics such as theory, photometric redshifts, simulations, and deblending. Across DESC working groups and the broader cosmology community, several critical themes and methodologies have crystallized:

\begin{itemize}
 \item \textbf{Simulation-Based Inference (SBI):} SBI has emerged as a powerful methodology, enabling analyses of a complexity that typically exceeds the capabilities of traditional forward modeling. This domain offers fertile ground for machine learning research, particularly in the development of emulators to accelerate pipeline components and in extending analyses beyond traditional point statistics. However, SBI remains sensitive to model misspecification, lossy summaries and inaccurate posterior approximations, problems which are particularly challenging to solve in a machine learning paradigm.
 \item \textbf{Bayesian Methodology and Uncertainty Quantification (UQ):} While Bayesian frameworks are ubiquitous in cosmology, machine learning is increasingly being explored to accelerate Bayesian inference on LSST-scale datasets that would otherwise be computationally intractable. Furthermore, the high precision required by cosmology requires accurate uncertainty estimates that go beyond common practice in machine learning. Building on the application of Bayesian neural networks and related methods, DESC is well positioned to drive fundamental developments in this area.
 \item \textbf{Validation and Benchmarking:} For cosmology, rigorous validation is paramount. Algorithms must not only be accurate and unbiased but also capable of correctly propagating uncertainty. Covariate shift, inevitable in many supervised learning contexts, must be mitigated through accurate simulations and techniques for domain adaptation. Benchmarking and validation are particularly important for algorithms used in products intended for broad usage, such as \acrshortpl{fm} and simulations. The \acrshort{rail} project (see \autoref{sec3:photo-z}), developed by DESC specifically to benchmark photometric redshift algorithms, serves as an excellent model for such validation frameworks.
 \item \textbf{Active Learning and Discovery:} Active learning has become an essential part of machine learning and will be crucial in managing LSST data. The \acrshort{resspect} project (see \autoref{sec3:td}), a collaborative initiative developing an active learning pipeline for transient classification, is an example of the comprehensive infrastructure required for effective active learning. Furthermore, human-in-the-loop workflows will be vital for anomaly detection and the identification of rare phenomena within the vast LSST dataset, facilitating novel discoveries that purely automated systems might overlook.
\end{itemize}
%This white paper surveys the intersection of AI/ML with DESC science analyses, highlighting concrete roles for ML in photometric redshifts, strong lensing, weak lensing and large-scale structure, clusters, supernova cosmology and transients, theory and modeling, and survey simulations (\autoref{sec3:use_case_for_aiml}). It then identifies methodological directions where DESC science directly motivates advances in the broader AI/ML ecosystem: Bayesian and simulation-based inference, physics-informed generative models, and novelty-detection frameworks that can operate at LSST scale (\autoref{sec4:aiml_research}). Finally, we emphasize the emerging importance of data foundation models and agentic AI systems (LLMs and multi-agent systems) as cross-cutting technologies, provided their deployment is coupled to rigorous evaluation, governance, and provenance tracking (See \autoref{sec5:emerging_tech}).

Realizing this potential requires DESC to treat AI/ML as primary components of the measurement pipeline. \autoref{sec6:infra_requirements}--\ref{sec:aiml_risks} of this paper outline the software, computing, and data infrastructure required to support AI/ML at scale. Sustainable integration of emerging tools requires a shared AI software stack, containerized and \acrshort{rsp}-compatible workflows, a DESC Data Registry for model and data products, and benchmark suites that tie model performance directly to cosmological and systematic-error budgets (see \autoref{sec6:infra_requirements}). These methods also present opportunities for broader coordination with Rubin operations, community brokers, external AI/ML institutes, and industry, which we outline in Section~\ref{sec7:broader_coordination}, but present risks ranging from model miscalibration and covariate shift to data rights compliance, environmental cost, and the erosion of human oversight (see \autoref{sec7:broader_coordination}).

On the basis of these insights, we have defined a series of recommendations (R1--R15) and opportunities (O1--O5) in the Executive Summary (\autoref{sec:exec_summary}), spanning methodological research, foundation models, LLMs and agentic AI, infrastructure and software, organizational coordination, human capital, and external partnerships. Implementing these recommendations would position DESC to use AI/ML for ambitious and disciplined science. LSST-era cosmology will not be limited by a lack of algorithms, but by our ability to connect those algorithms to physical modeling, survey simulations, and governance structures that make their behavior interpretable and trustworthy. By adopting a coherent AI/ML strategy grounded in DESC’s science priorities and supported by robust infrastructure and shared purpose, the collaboration can help shape best practices in the use of AI across fundamental physics and ensure that the coming wave of AI tools advances precision dark energy science while upholding the highest scientific standards. 