\newpage 
\section{Risks, Challenges, and Mitigation Strategies for AI/ML in DESC}
\label{sec:aiml_risks}

The increasing reliance on \acrshort{ai}/\acrshort{ml} within \acrshort{desc} brings not only opportunities but also a set of technical, organizational, and societal risks that must be managed deliberately. The aim is not to discourage the use of AI, but to ensure that methods are deployed in ways that are scientifically robust, sustainable over the \acrshort{lsst} decade of operations, compatible with DESC’s standards for transparency and reproducibility and with the broader scientific and educational aims of DESC. Below we highlight key challenge areas together with concrete mitigation strategies.

\paragraph{Methodological Robustness and Interpretability}
AI/ML models trained directly on data are vulnerable to familiar statistical pitfalls: biased or incomplete training data, overfitting \citep{2023Huppenkothen,2023Smith}, and domain mismatch between simulations and real survey data \citep{2023Ciprijanovic,Pandya2025}. For methods that sit close to top-level cosmological inferences, there is a justified reluctance to adopt ``black-box'' results as reference constraints unless they can be thoroughly validated and stress-tested. This is amplified for neural summary statistics and learned emulators \citep{2022VillaescusaNavarro, 2023Huppenkothen, 2023Smith}, where it can be difficult to diagnose failure modes or to construct transparent null tests. There is also a subtle “ML-only” risk: some future analyses will be so data- and compute-intensive that no independent non-ML cross-check is feasible, making it even more important that AI-based pipelines be internally well understood and stress-tested.\\
From a DESC perspective, mitigation rests on \emph{explicit validation and interpretability practices}: (i) defining standardized simulation and challenge suites where AI and traditional methods are compared head-to-head in the regime where both are applicable; (ii) publishing diagnostic plots and ablation studies that isolate which data features drive the constraints; (iii) requiring explicit documentation of model training domains, assumptions, and known failure modes, and discouraging use outside those regimes; (iv) developing approximate surrogate models or interpretable summaries (e.g., response functions, feature attributions) that can be inspected by domain experts; and (v) encouraging redundancy where it matters most—for example, using different architectures, loss functions, or simulation pipelines to cross-check key inferences, even when all are “AI-based”. Any AI-based result used as a reference cosmological constraint should be accompanied by a documented validation program and, where possible, benchmarked against simpler baselines. We recognize that imposing these higher standards carries a non-negligible human effort cost, which in turn creates a natural opportunity for agentic AI systems to automate parts of the validation workflow and thereby reduce that burden.

\paragraph{Provenance, Reproducibility, and Integration into Pipelines}
As analyses become more complex, certifying full provenance (from raw data through simulations, training runs, and model selection) becomes harder but more important. If DESC cosmology results depend on opaque training pipelines, unversioned models, or undocumented hyperparameters, small bugs or biases can consume a non-negligible fraction of the “tension budget” in precision tests of \acrshort{lcdm}. While these issues are common to most long-term software infrastructures, the rapid pace of methodological advancement in AI/ML hinders provenance tracking, and the stochastic nature of most neural network training can prevent complete reproducibility. There is also the practical challenge of integrating AI components into mature pipelines that already rely on well-tested codes.\\
Mitigation here is largely infrastructural and procedural: (i) treat trained models as first-class data products, with versioning, metadata, and model cards describing training data, objectives, and known limitations; (ii) require that AI components be runnable from containerized environments and integrated into \acrshort{ci} pipelines with regression tests; (iii) maintain “shadow” implementations (simpler, slower, or more traditional pipelines) for cross-checks where feasible; and (iv) define clear deprecation and maintenance policies so that AI dependencies do not become unmaintainable over the survey lifetime.

\paragraph{Safe Usage, Data Rights, and External Services}
Widespread availability of commercial \acrshortpl{llm} and AI services lowers the barrier to experimentation but introduces new questions about data governance and safe usage. Uploading proprietary Rubin/DESC data or unpublished results to third-party services may raise data-rights concerns, and using off-the-shelf models without understanding their limitations can encourage application of techniques outside their domain of validity.\\
DESC can mitigate these issues by (i) establishing clear guidelines on what kinds of data and metadata may be used with external services, in coordination with Rubin, \acrfull{lsst-da}, and agency policies; (ii) prioritizing self-hosted or collaboration-controlled deployments (e.g., for LLMs and inference services) for sensitive workloads; and (iii) prioritizing services committed to long term availability of their AI models and transparent versioning of these models, for long term reproducibility.

\paragraph{Human Capital, Training, and Sustainability}
AI/ML tools (and, increasingly, agentic assistants) can accelerate research by automating repetitive tasks and lowering the barrier to entry for complex workflows. However, there is a real risk that early-career researchers learn to operate pipelines as “black boxes” or solely by prompt engineering, without acquiring a deep understanding of the underlying statistics and physics, and thus weakening long-term scientific literacy and even cognitive abilities~\citep{kosmnya25, Trotta2025AIThreat}. \\
DESC can turn this into an opportunity by (i) framing AI/ML training as an integral part of graduate and postdoctoral education, combining hands-on use of tools with explicit coverage of underlying concepts; (ii) pairing students with mentors who can help them “open the box” at least once (e.g., by reproducing a result from scratch or implementing a simplified version of a method); (iii) encouraging contributions to shared, well-documented libraries rather than bespoke one-off scripts, spreading maintenance across the collaboration and ensuring that successful methods become communal assets; and (iv) explicitly furthering a scientific culture that does not unduly favor efficiency and speed over in-depth engagement with modeling and creative thinking.  

\paragraph{Environmental Impact}
Large-scale adoption of AI/ML methods comes with significant environmental impact: data centers and computing farms are energy intensive, water hungry, and have generally a negative environmental impact in terms of land usage, noise and e-waste production. The first step toward an environmentally sustainable practice for DESC is to quantify the computing resources used in AI/ML development. This should cover not just the training, validation and deployment costs for methods presented in a research paper (as is common practice), but also track the computational expenditure for hyperparameter search, failed tests, aborted training runs and architectural dead-ends -- which often dwarf the compute needed for the ultimate implementation. \\
To this end, DESC needs to develop easy-to-use, adaptable and constantly reviewed guidelines regarding how to log and report transparently such computational costs, so as to monitor compute usage in DESC over the lifetime of LSST. Taking advantage of more energy-friendly implementations (both software and hardware) should also be encouraged, while training on the matter should be available to all DESC members. \\
Detailed evaluations of astronomy-specific activities in terms of their carbon footprints are scarce, and usually limited to research infrastructure \citep[see, e.g.,][]{knödlseder2025}. \cite{2020NatAs...4..843S} find that even before the rise of AI/ML, supercomputer usage already accounted for the majority of Australian astronomers' carbon footprint. However, compute-related carbon emissions are but one aspect of the multi-faceted environmental impact of AI/ML, which remains understudied. It would be desirable to address this gap by developing in-depth evaluations of the wider environmental cost of LSST-related AI/ML usage, as a way to support sustainability alongside scientific gains \citep{Bashir2024Climate}. 

\paragraph{Computing Resources and Infrastructure}
Realizing the full potential of AI/ML within DESC will place non-trivial demands on computing resources. Training large foundation models for images, catalogs, or time series, and running large-scale simulation-based inference, require sustained access to \acrshort{gpu} clusters at a scale beyond traditional analysis workloads. Similarly, if DESC wishes to host its own LLMs or other generative models for work involving sensitive or proprietary data, these services will need reliable, secure GPU backends and operational support over many years. Without careful planning, AI workloads risk competing destructively with other science uses for scarce accelerators, or fragmenting into ad hoc deployments that are hard to maintain.\\
Mitigation here is primarily strategic: (i) aligning major AI training campaigns with DESC’s existing resource-allocation processes and external partners (e.g., LSST-DA, national and international \acrshort{hpc} centers); (ii) prioritizing shared, reusable models and services over one-off experiments; (iii) investing in efficient training and inference schemes (e.g., parameter-efficient finetuning, mixed precision, model distillation); and (iv) treating any self-hosted LLM or foundation-model service as collaboration infrastructure, with clear policies on access, data governance, and long-term support.


Overall, the main risks associated with AI/ML in DESC are not existential but \emph{operational}: biases that are hard to diagnose, results that are difficult to reproduce, methods that are fragile under domain shift, and human capital that is either over- or under-reliant on automation. By treating AI components with the same methodological rigor as any other part of the cosmology pipeline requiring validation, documentation, governance, and training, DESC can reap the benefits of these tools while keeping these risks manageable.
