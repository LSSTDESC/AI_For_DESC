\newpage 
\section{Risks, Challenges, and Mitigation for AI/ML in DESC}
\label{sec:aiml_risks}

The increasing reliance on AI/ML within DESC brings not only opportunities but also a set of technical, organizational, and cultural risks that must be managed deliberately. The aim is not to discourage the use of AI, but to ensure that methods are deployed in ways that are scientifically robust, sustainable over the LSST decade, and compatible with DESC’s standards for transparency and reproducibility. Below we highlight key challenge areas together with concrete mitigation strategies.

\paragraph{Methodological Robustness and Interpretability}
AI/ML models are vulnerable to familiar statistical pitfalls: biased or incomplete training data, overfitting, and domain mismatch between simulations and real survey data. For methods that sit close to top-level cosmological inferences, there is a justified reluctance to adopt “black-box” results as reference constraints unless they can be thoroughly validated and stress-tested. This is amplified for neural summary statistics and learned emulators, where it can be difficult to diagnose failure modes or to construct transparent null tests. There is also a subtle “ML-only” risk: some future analyses may be so data- and compute-intensive that no independent non-ML cross-check is feasible, making it even more important that AI-based pipelines be internally well understood and stress-tested.\\
From a DESC perspective, mitigation rests on \emph{explicit validation and interpretability practices}: (i) defining standardized simulation and challenge suites where AI and traditional methods are compared head-to-head; (ii) publishing diagnostic plots and ablation studies that isolate which data features drive the constraints; (iii) requiring explicit documentation of model training domains, assumptions, and known failure modes, and discouraging use outside those regimes; (iv) developing approximate surrogate models or interpretable summaries (e.g.\ response functions, feature attributions) that can be inspected by domain experts; and (v) encouraging redundancy where it matters most—for example, using different architectures, loss functions, or simulation pipelines to cross-check key inferences, even when all are “AI-based”. Any AI-based result used as a reference cosmological constraint should be accompanied by a documented validation program and, where possible, benchmarked against simpler baselines.

\paragraph{Provenance, Reproducibility, and Integration into Pipelines}
As analyses become more complex, guaranteeing full provenance (from raw data through simulations, training runs, and model selection) becomes harder but more important. If DESC cosmology results depend on opaque training pipelines, unversioned models, or undocumented hyperparameters, small bugs or biases can consume a non-negligible fraction of the “tension budget” in precision tests of $\Lambda$CDM. There is also the practical challenge of integrating AI components into mature pipelines that already rely on well-tested, algorithmic codes.\\
Mitigation here is largely infrastructural and procedural: (i) treat trained models as first-class data products, with versioning, metadata, and model cards describing training data, objectives, and known limitations; (ii) require that AI components be runnable from containerized environments and integrated into CI pipelines with regression tests; (iii) maintain “shadow” implementations (simpler, slower, or more traditional pipelines) for cross-checks where feasible; and (iv) define clear deprecation and maintenance policies so that AI dependencies do not become unmaintainable over the survey lifetime.

\paragraph{Safe Usage, Data Rights, and External Services}
Widespread availability of commercial LLMs and AI services lowers the barrier to experimentation but introduces new questions about data governance and safe usage. Uploading proprietary Rubin/DESC data or unpublished results to third-party services may raise data-rights concerns, and using off-the-shelf models without understanding their limitations can encourage application of techniques outside their domain of validity.\\
DESC can mitigate these issues by (i) establishing clear guidelines on what kinds of data and metadata may be used with external services, in coordination with Rubin, LSST-DA, and agency policies; (ii) prioritizing self-hosted or collaboration-controlled deployments (e.g.\ for LLMs and inference services) for sensitive workloads; and (iii) prioritizing services committed to long term availability of their AI models and transparent versioning of these models, for long term reproducibility.

\paragraph{Human Capital, Training, and Sustainability}
AI/ML tools (and, increasingly, agentic assistants) can accelerate research by automating repetitive tasks and lowering the barrier to entry for complex workflows. However, there is a real risk that early-career researchers learn to operate pipelines as “black boxes” or as prompt engineers, without acquiring a deep understanding of the underlying statistics, physics, and numerics. At the same time, increased use of AI for infrastructure development can create technical debt and a maintenance burden for sophisticated codebases.\\
DESC can turn this into an opportunity by (i) framing AI/ML training as an integral part of graduate and postdoctoral education, combining hands-on use of tools with explicit coverage of underlying concepts; (ii) pairing students with mentors who can help them “open the box” at least once (e.g.\ by reproducing a result from scratch or implementing a simplified version of a method); and (iii) encouraging contributions to shared, well-documented libraries rather than bespoke one-off scripts, spreading maintenance across the collaboration and ensuring that successful methods become communal assets.

\paragraph{Computing Resources and Infrastructure}
Realizing the full potential of AI/ML within DESC will place non-trivial demands on computing resources. Training large foundation models for images, catalogs, or time series, and running large-scale simulation-based inference, requires sustained access to GPU clusters at a scale that goes beyond traditional analysis workloads. Similarly, if DESC wishes to host its own LLMs or other generative models for work involving sensitive or proprietary data, these services will need reliable, secure GPU backends and operational support over many years. Without careful planning, AI workloads risk competing destructively with other science uses for scarce accelerators, or fragmenting into ad hoc deployments that are hard to maintain.\\
Mitigation here is primarily strategic: (i) aligning major AI training campaigns with DESC’s existing resource-allocation processes and external partners (e.g.\ LSST-DA, national and international HPC centers); (ii) prioritizing shared, reusable models and services over one-off experiments; (iii) investing in efficient training and inference schemes (e.g.\ parameter-efficient finetuning, mixed precision, model distillation); and (iv) treating any self-hosted LLM or foundation-model service as collaboration infrastructure, with clear policies on access, data governance, and long-term support.


Overall, the main risks associated with AI/ML in DESC are not existential but \emph{operational}: biases that are hard to diagnose, results that are difficult to reproduce, methods that are fragile under domain shift, and human capital that is either over- or under-reliant on automation. By treating AI components with the same methodological rigor as any other part of the cosmology pipeline—through validation, documentation, governance, and training—DESC can reap their benefits while keeping these risks well under control.
