\newpage
\section{AI/ML Research Motivated by DESC Science Needs}
\label{sec4:aiml_research}

Extracting robust cosmological constraints from LSST requires not only advanced algorithms but also a coherent methodological foundation that bridges simulation, data processing, and inference. Each of these pillars must meet unprecedented demands in scale, accuracy, and interpretability, demands that challenge the limits of both physical modeling and machine learning. Beyond simply applying existing AI techniques, DESC must develop methods tailored to the structure of astronomical data, the physics of observables, and the statistical rigor required for precision cosmology.

The scientific ambitions of LSST thus motivate AI/ML research in several key areas.
First, Bayesian inference and uncertainty quantification (UQ) must evolve to handle the high-dimensional, hierarchical models that describe cosmic fields and galaxy populations, while maintaining interpretability and calibration across vast data volumes.
Second, simulation-based inference (SBI) and related implicit-likelihood methods must confront the challenge of model misspecification and covariate shifts, ensuring that learned posteriors remain valid when simulations imperfectly represent real observations.
Third, physics-informed modeling, through differentiable programming and hybrid generative–physical architectures, offers a path toward interpretable and physically consistent deep learning, capable of representing both known and unknown components of the Universe.
Fourth, discovery and anomaly detection are essential to LSST’s potential for unexpected science, requiring representation learning and active human–AI collaboration to identify rare and previously unmodeled phenomena.
Finally, LSST’s diverse data modalities (images, spectra, and time-domain signals) demand specialized neural architectures that can process astronomical data optimally while preserving physical and statistical meaning.

This section examines these research directions in detail, outlining both recent progress and outstanding challenges. We emphasize not only algorithmic innovation but also the principles of validation, calibration, and interpretability required to integrate AI into cosmological analysis pipelines.

The fundamental question is: \textit{What would convince us of a cosmological result obtained with AI?}
Answering this question defines the research agenda for AI/ML in DESC and ensures that machine learning becomes not merely a computational shortcut, but a scientifically trustworthy component of cosmological inference.


\subsection{Bayesian Inference and Uncertainty Quantification}
% 
% Main points of this section:
%  - Cover the challenges in uncertainty quantification at the frontier of current methodologies 

Uncertainty quantification (UQ) represents perhaps the most critical challenge for deploying deep learning in precision cosmology. It is an area where AI in the sciences demands solutions that are different from AI in many commercial settings. Robust UQ must distinguish between aleatoric uncertainties (irreducible measurement noise) and epistemic uncertainties (model limitations and incomplete knowledge), as these have fundamentally different implications for cosmological inference and systematic error budgets.
ML is poised to be revolutionary for inference, but only if present-day challenges can be addressed satisfactorily. 

% Make the distinction here between the different configuration cases, implicit and explicit inference and how they differ from each other.

\subsubsection{Explicit Likelihood-Based Bayesian Inference}
\label{sec4:Bayes}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{hierarchical-bayes}, \meth{variational-inference}, \meth{gaussian-process} \\
\themebullet \themekey{Addresses.} \challenge{uq-calibration}, \challenge{scalability}
\end{ThemeBoxA}
% Here we cover cases where the likelihood is explicity known
% so, no neural density estimation for learning the likelihood or priors.
% We still should comment on the difficulty 
% We are going to talk about the challenges
%   - Research and strategies for accelerated sampling leveraging differentiable likelihoods
%   - Challenges of scaling inference to large simulation models 

    The inferential paradigm in astrophysical and cosmological data analysis has been for the past two decades primarily Bayesian, as this offers conceptual, methodological, and computational benefits~\citep{Trotta_2008}. The massive increase in data size and complexity afforded by LSST will require a new step forward in inferential methodology, as LSST data will challenge the computational feasibility of current inferential engines. We cover in this section the case of \textit{explicit inference}, where we have the ability to directly evaluate the log-likelihood of our probabilistic models, and potentially its gradients. 


\paragraph{Accelerating Posterior Inference} Markov Chain Monte Carlo (MCMC) methods have been the workhorses of likelihood-based Bayesian parameter inference to date. Notable examples are Gibbs sampling \citep{Casella1992} and parallelized versions of Metropolis-Hastings (e.g. the affine-invariant sampler by ~\citealp{goodman2010ensemble, Foreman_Mackey_2013}). For cases where multimodality and/or strong parameter degeneracies are important, nested sampling~\citep{RN599,Ashton_2022} in its many variants, (e.g.\ MultiNest~\citealp{Feroz:2007kg, Feroz:2009}; Polychord~\citealp{Handley:2015,Handley:2015fda}; dynesty~\citealp{RN1654}; DNEst4~\citealp{Brewer:2016scw}), recently improved with gradients~\citep{lemosGGNS}, accelerated with neural emulators~\citep{Lovick:2025wdj}, or normalizing flows~\citep{RN1653} have been key to ensuring reliable inference.  

However, as analyses become increasingly complex, involving large number of nuisance parameters and expensive likelihood evaluations, the cost of running cosmological inference with conventional techniques becomes prohibitive. 

One avenue to speed up inference is to leverage access to the gradients of the log-posterior. When such gradients are available, a number of inference methods can benefit from them, including the well-established Hamiltonian Monte Carlo (HMC; \citealp{neal2011hmc}), as well as more modern generalizations such as No-U-Turn Sampler (NUTS; \citealp{HoffmanGelman2014}). Remarkably, physical ideas continue to lead the development of gradient-based inference techniques. Riemann Hamiltonian Monte Carlo (RHMC) \citep{RHMC} simulates trajectories in arbitrary geometries by following the geodesics of the likelihood manifold. This makes RHMC schemes extremely robust sampling algorithms for high-dimensional inference in the face of severely non-gaussian posteriors. However, RHMC hasn't seen a wide spread adoption due to instability of second-order auto-differentiation needed to compute the curvature of the likelihood. In a similar vein, relativity-inspired HMC schemes \citep{RelativtyHMC} have recently been proposed that particularly target Minkowski geometries. This effectively introduces a maximum speed for the simulated particle, slowing it down in the areas of most challenging geometry, achieving most of the goals of RMHC without many of its hurdles. Alternatively, schemes that depart from Hamiltonian dynamics have been proposed. Langevin Monte Carlo (LMC) \cite{LangevineMC} reformulates the traditional inference problem as a stochastic differential equation (SDE) whose solution is Monte Carlo chain that dissipates from the prior into the posterior distribution. In this frame work, the metropolis adjustment that ensures the target distribution is sampled is replaced with the bias requirement on the solution of the SDE, leasing to significant speed ups \citep{GrumittLangevin}. A final avenue of improvement is to revise the assumed partition function of the particles simulated by the inference algorithm. Traditional HMC schemes assume that the distribution of particles being simulated follows a canonical partition function. However, more efficient sampling schemes can be constructed by exploring other partition functions. Microcanonical or energy-conserving HMC (MCHMC) \citep{Robnik2022MicrocanonicalHM}) explores the posterior distribution using a single energy shell. Similarly to in relativistic schemes, this achieved by modifying the momentum of the particle to slow down at the regions of high-posterior density \citep{SteegHMC}, leading to a far more efficient sampling. Micro-Canonical Langevin Monte Carlo (MCLMC) \cite{MCLMC} is a sampling algorithm that combines all the ideas described above to great success. MCLMC has been already deployed to perform inference on physics problems such as lattice field theory simulations \citep{MCLMC} and even inside cosmology where it has been shown to speed up field level inference by an order of magnitude \citep{2023arXiv230709504B}. This makes MCLMC the cutting edge of gradient-based inference schemes and promising tool to speed up analyses within DESC.   %\todo{Add more content to this, around additional research on gradient-based samplers or tradeoffs of MCHMC/MCLMC}. 
%\cite{2025arXiv251025824B} uses the analogy of light refraction to design a mechanism for moving samples through the parameter space, guided by the likelihood $L(x)$, i.e  the refractive index is $n(x)=L(x)^{1/(D-1)}$, with $D$ being the dimension of the parameter space. This causes sampling paths to bend naturally toward high-likelihood regions, similar to how light refracts toward denser media. Sampling trajectories automatically move toward regions of higher probability. This ray-tracing method offers the possibility for interpreting existing samplers as (MC)HMC/(MC)LMC, etc. The experiments conducted by the author show scalability and robustness to stochastic gradient errors and can pass through the arbitrary likelihood and parameter space barrier. These key features would be worth testing in the context of DESC analysis.

Alternatively, one can replace a complicated posterior distribution with a more tractable one. Variational Inference (VI) aims to find an approximation to the posterior distribution $p$ by a "surrogate" parametrized distribution $q_\phi$, whose parameters $\phi$ are trained to minimize the Kullback-Leibler divergence between $q$ and $p$ (see, for instance, \citealp{Uzsoy_2024, JAX-COSMO}, using the JAX powered NumPyro framework, \citealp{phan2019composable}). Here gradients are only needed for $q$, not for $p$.
%\todo{add more citations.}

Another area of research focuses on \textit{neural sampling methods}, which leverage in various ways neural networks to accelerate sampling while attempting to preserve asymptotic correctness guarantees. For instance, Normalizing Flows have been used to re-parameterize the sampling space and cure complex geometries \citep{2022PNAS..11909420G}. Another recent line of research also leverages ideas from Diffusion Models and use a neural score model to accelerate sampling \citep{2025arXiv250411713H}. 

As shown above, such inference strategies depend on differentiable components and will strongly benefit when likelihood codes are rewritten in frameworks allowing for automatic differentiation. An additional advantage of making probabilistic models compatible with such frameworks is that they usually support GPU acceleration and vectorization, which opens up yet another avenue for acceleration (e.g. the Numpyro \citep{phan2019composable, bingham2019pyro} and BlackJax \citep{cabezas2024blackjax} libraries written in JAX). Affine-invariant samplers are particularly suited to vectorization on GPU hardware, as has been demonstrated in astronomy contexts (e.g. \citealp{Thorp:2024, Thorp:2025}; using the \href{https://github.com/justinalsing/affine}{affine} sampling code). 

%\note{RT added the following to collect Bayesian evidence methods under one heading; possibly move this further down?} 
\paragraph{Bayesian model comparison} Estimation of the Bayesian evidence, the central quantity for model comparison, remains challenging when the model being compared are very high dimensional. Nested sampling has been established as one of the main methods for Bayesian evidence computation, but in its original formulation it suffers from the curse of dimensionality: the efficiency of the constrained sampling step decreases rapidly as the dimensionality of parameter space increases. This has been somewhat mitigated by recent developments such as PolyChord~\citep{Handley:2015}, which can be used in a few hundreds dimensions; dynesty~\citep{RN1654}, which uses dynamical allocation of live points; and GGNS~\citep{lemosGGNS}, exploiting gradients, generative flows and differentiable programming to achieve better efficiency and accuracy in up to $\sim 200$ dimensions.
A suite of other methods for the evaluation of the high-dimensional average of the likelihood over the prior have are also being explored, sometimes combining density estimation with neural techniques  \citep[e.g.][]{RN590,Srinivasan_2024,mcewen2023}. However, they remain confined to moderately low-dimensional parameter spaces, of order a few tens of dimensions. 

The frontier represented by evidence estimation in very large dimensional (of order $10^3$ or more) parameter spaces from real data remains largely untouched outside of synthetic demonstrative examples where the ground truth is known. Neural ratio estimation (NRE) shows promise in this respect, in that evidence estimation can be obtained from an NRE architecture by adding a suitable inferential head that is trained only on model labels, thus implicitly marginalizing over all parameters in the model. Such an approach naturally also generalizes to perform Bayesian model averaging. An example of this method is~\cite{SimSIMS}, where six models for empirical corrections for supernova type Ia data are compared from CSP observations within a Bayesian hierarchical model setting with $\sim 4,000$ latent variables. 

\paragraph{Hierarchical Bayesian Models in Extremely High Dimensions} The manyfold increase in data size requires in many cases a more sophisticated model to capture effects that were previously unimportant; this in turn increases the dimensionality of the parameter space (especially in hierarchical models, where the latent space dimensionality scales with the number of objects within the model); the likelihood might become intractable, or previously used approximations, such as approximate Gaussianity or linear propagation of errors~\citep{Karchev2022}, neglecting of Eddington bias~\citep{Karchev_STARNRE}, might break down. 

%\sout{As mention in \autoref{sec3:wlss}, there is growing interest in conducting so-called explicit full-field inference for cosmological surveys, which implies performing inference over an entire approximated N-body simulation model, with a number of parameters in the hundreds of millions to billion parameters.} \todo{complete on the challenges here, for sure we need differentiability and extremely good samplers, cite comparison done in DESC of the cost of these models compared to SBI, and also mention more software needs for inference frameworks that would scale to distributed GPU computing systems.}

 \autoref{sec3:wlss} introduced so-called full-field inference for cosmological surveys, in which not only cosmological parameters are inferred but also the initial conditions that seed the evolution of the large-scale structure of the Universe~\citep{porqueres2023fieldlevelinferencecosmicshear}. The fidelity of the forward simulation directly controls the accuracy of posterior constraints on cosmological parameters; consequently, this approach requires exploring extremely high-dimensional parameter spaces (millions to billions of parameters). Sampling such spaces is intractable for traditional MCMC and instead calls for gradient-based methods, as noted above. This, in turn, demands a forward simulation that is both fast and differentiable.

A practical choice is particle--mesh (PM) simulations, which are fast and can be made differentiable relatively easily~\citep{Feng_2016}. Several JAX-based~\citep{jax2018github} GPU-accelerated, differentiable PM codes are now available, such as \texttt{PMWD}~\citep{Li__2024}; however, differentiating such high-dimensional simulations remains memory-intensive, making scaling to distributed GPU systems essential. Addressing this challenge, \texttt{jaxDecomp}~\citep{Kabalan_jaxDecomp_2025} provides differentiable and distributed PM components—including distributed 3D FFTs and halo-exchange routines—and integrates with \texttt{JAXPM}~\citep{JAXPM}, delivering a JAX-based, fast, differentiable, and distributed PM code that serves as an explicit forward model for large-scale cosmological inference.

Compared to implicit inference methods (see next section~\ref{sec4:sbi}), explicit inference is substantially more computationally expensive. Given the same simulation, inferring from a well-constructed summary statistic can deliver cosmological constraints comparable to field-level inference, but at a fraction of the computational cost~\citep{Zeghal_2025}. Nevertheless, explicit inference offers several advantages. First, analyzing statistical errors directly in data space is more interpretable than working with compressed summaries; even with optimal compression, signals can mix and model misspecification becomes difficult to detect. Explicit inference can treat systematic contaminations as additional parameters to be sampled \citealp{2019porqueres}, becoming a machine-aided report of contaminations that have a characteristic pattern on the sky. Second, explicit inference is designed for extensible, modular models in which new physics can be added—e.g., augmenting the simulation with a baryonification model—whereas implicit inference would require retraining the neural density estimator from scratch. Taken together, these properties make explicit inference well suited to joint inference of cosmology, systematics, and redshift-distribution uncertainties—capabilities that are considerably more difficult with implicit approaches. Additionally, explicit inference provides a digital twin of the Universe, which has multiple scientific applications but also provide a unique way of testing the results by cross-validating with independent data \citep{stopyra24}.

\subsubsection{Implicit Likelihood Bayesian Posterior Inference}
\label{sec4:sbi}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{sbi}, \meth{npe}, \meth{normalizing-flow} \\
\themebullet \themekey{Addresses.} \challenge{uq-calibration}, \challenge{scalability}
\end{ThemeBoxA}

The other paradigm is implicit inference, in which we do not assume direct access to the likelihood function, but only have access to samples from the joint distribution $p(x, \theta)$ of data samples $x$ and parameters of interest $\theta$. It should be noted that this situation covers both the case of \textit{simulation}-based inference, or the case where $x$ and $\theta$ are available from a training sample of real observations (the canonical example being photo-z estimation from a set of spectroscopic observations). 

In particular, simulation-based inference is rapidly emerging as a powerful alternative to traditional fitting techniques for Bayesian models. The key idea is to replace an explicit likelihood function by forward simulating (under the model) parameters-data pairs, which are then used to train a neural network to perform inference \citep[e.g.][]{Alsing2018, AlsingWandelt2018, AlsingWandelt2019,savchenko2024,lyu2025}. The advantages are that the (potentially intractable) likelihood can, in principle, incorporate physical effects of arbitrary complexity, which would otherwise be difficult to model (including e.g. selection effects and complex parameter dependencies). In some variants~\citep{Miller2021} the 1-- or 2--dimensional {\em marginal} distribution for the parameters of interest is targeted directly, thus circumventing the need to evaluate the high-dimensional joint posterior over all parameters in the model; such approaches are naturally suited to Bayesian evidence estimation~\citep{SimSIMS}. 

Additionally, inference can be {\em amortized} within a certain prior range, meaning that once trained the network can deliver almost instantaneous posteriors for a wide range of parameter values, a critical benefit when dealing with billions of galaxies \citep{2022ApJ...938...11H}.  This speed-up also permits posterior calibration methods (e.g., guaranteed coverage), which are computationally unfeasible with traditional posterior evaluation methods.

\paragraph{Neural Density Estimation (NDE) methods} The fundamental building blocks of these methods is \textit{Neural Density Estimation}, where a neural network is used to estimate a distribution, or a ratio of distributions. Various kinds of methods exist: neural likelihood (NLE; e.g.\ \citealp{Papamakarios2019, Lueckmann2019, Alsing:2019}), neural posterior (NPE; e.g.\ \citealp{PapamakariosMurray2016, Lueckmann2017}) and neural ratio (NRE) estimation are among the most popular (for an overview see~\citealp{Alsing:2019, Cranmer2020, Lueckmann2021}). Implementations of NLE and NPE both learn a density based on simulated parameter--data pairs (see e.g.\ \citealp{Alsing:2019}), with a variety of different approaches used for learning the multivariate joint or conditional density. Approaches to this include Gaussian mixtures \citep[e.g.][]{Alsing2018}, mixture density networks \citep[e.g.][]{PapamakariosMurray2016}, and normalizing flows (e.g.\ \citealp{Papamakarios2017, Papamakarios2019, Alsing:2019, Jeffrey:2021, 2022ApJ...938...11H}; for a review see \citealp{Kobyzev2021, Papamakarios2021}). More sophisticated density estimators used in generative modeling -- such as continuous normalizing flows \citep{Grathwohl2018, Chen2018}, score-based diffusion models \citep{Song:2020}, flow-matching models \citep{Lipman:2022}, and transformers \citep{Transformers2017} -- are also well suited to NLE and NPE tasks \citep[e.g.][]{DiazRiveroDvorkin2020, Geffner:2022, Wildberger:2023, Gloeckler:2024} alongside the generative modelling tasks they are commonly used for \citep[e.g.][]{Alsing:2024, Cuesta:2024, Thorp:2025}. 
%\todo{talk more about the different flavors of density estimations, including score based ones, and recent work from MackeLab on transformer-based models}


\paragraph{Optimal Neural Summarization} %\todo{write this section to talk about IMNN, VMIM, and others, also comment on the debate between discriminative and generative (should we build summaries, or a generative model of the high dimensional data/problem)}.
To ensure the robustness of implicit inference, the process is usually divided into two main steps enabling each neural network to focus on a specific task: (1) compression of high-dimensional data into informative summary statistics, and (2) performing Bayesian inference using neural density estimation methods on this low dimensional but highly informative statistic. To maximize information extraction and improve constraints on cosmological parameters, the community has increasingly adopted neural network–based summarization techniques. While any neural network can be trained on the regression task of inferring parameters given data \citep[e.g.][]{Gupta2018MAE, kacprzak2022deeplss, lu2023cosmological}, it is unclear how much of the information contained in the data is extracted. In particular, \cite{neural_summary_lanzieri_2025} demonstrate that standard regression loss functions do not guarantee the systematic construction of sufficient statistics. Information maximising neural networks (IMNNs, \citealp{2018PhRvD..97h3004C}) directly address this problem by learning summary functions that maximize the Fisher information. They can produce nearly exact posteriors and are thus approximately sufficient statistics of the data. Another approach is to derive a loss function directly from the definition of sufficiency, that is, by maximizing the mutual information between the summary statistics and parameters of interest \citep{Jeffrey:2021, chen2021neural}.


\paragraph{Controlling Epistemic Errors in Inference Results} One fundamental limitation of NDE methods is that their reliance on a neural network to model at some level the likelihood of the data is inherently imperfect. In the asymptotic regime of infinite data and flexible neural network, the approximation to the target posterior will converge, but in practice we are never guaranteed to find ourselves in this regime, and must therefore take into account and mitigate \textit{epistemic errors}. Several strategies have been developed over the years to quantify and mitigate this epistemic uncertainty on inference results.  Markov Chain Monte Carlo sampling over network parameters provides gold-standard uncertainty estimates but at usually prohibitive computational cost. In addition, detecting convergence of the chain remains difficult, usually necessitating drawing more samples than ultimately needed. Because such approach is extremely expensive, other approaches have been developed. \textit{Bayesian Neural Networks} approximate the posterior distribution over network parameters through Variational Inference, providing principled uncertainty estimates at reduced computational cost. However, the tradeoff between approximation quality and speed remains concerning, especially in the highly multi-modal loss landscapes of deep neural networks. In such settings, scalable variational methods often collapse to a single mode of the posterior rather than exploring the full diversity of solutions \citep{fort2020losslandscape}, which limits the quality of their uncertainty estimates. One of the most used VI methods in astronomy is Monte Carlo dropout which utilizes the dropout layer commonly introduced in deep neural networks to prevent correlated activation as one of the computationally cheapest approximations to Bayesian inference, however, its theoretical justification and empirical accuracy are questionable \citep{LeFolg:2021}. Nonetheless, comparisons with other methods have shown promise for astronomical applications: strong lensing \citep{Perreault:2017}, supernova time-series classification \citep{Moller:2020,Moller:2022ICML}, and star time-series classification \citep{Astromer2023, CadizLeyton:2025, CadizLeyton:2025:MoE,CadizLeyton:2025:UQ}. Other VI methods such as Bayes by Backprop and SWAG have been sparsely used for time-series classification and regression with mixed results \citep{Moller:2020,Cranmer:2021}. Another strategy is Deep
Ensembles in which multiple networks are trained from different random initializations to provide uncertainty estimates via the variance of their predictions \citep{Makinen:2021,Moller:2022,Moller:2024}. Unlike variational methods, ensembles capture uncertainty by effectively sampling from different modes of the loss landscape, resulting in more robust and better-calibrated uncertainty estimates. However, they are more computationally demanding than MC dropout, and do not mitigate errors arising from model misspecification. Comparative studies exploring the trade-offs between these UQ methodologies for achieving superior uncertainty evaluation are a growing focus in the field \citep{CadizLeyton:2025:UQ}.


\subsubsection{Model Mispecification and Covariate Shifts}
\label{sec4:model-misspec}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{sbi} \\
\themebullet \themekey{Addresses.} \challenge{model-misspecification}, \challenge{covariate-shift}
\end{ThemeBoxA}
%\note{This section should be reworked a little bit further, be very clear that model mispecifiation is currently the main thing that holds us back in SBI for cosmo in wlss for instance, and when doing implicit inference it is very hard to know when we are working with a mispecified model. it is easier to know with explicit models.}

From a technical standpoint, Simulation-Based Inference (SBI) has achieved impressive results: Neural Posterior Estimation (NPE) with normalizing flows performs well in low-dimensional regimes~\citep[e.g.][]{srinivasan2025}, Neural Likelihood Estimation (NLE) scales satisfactorily to higher dimensions, and marginal Neural Ratio Estimation (NRE) has shown success across diverse applications~\citep[e.g.][]{alvey2024,list2023,Franco_Abell_n_2024,Saxena_2024}. Yet, these demonstrations rely primarily on simulated data and therefore represent best-case scenarios; direct validations of SBI on real data remain scarce~\citep{2024Karchev_SIDEreal,Lueber2025}.

The robustness of SBI depends critically on the realism and completeness of the simulations that underpin it. Simulations must reproduce all relevant aspects of the observations, including astrophysical, instrumental, and observational effects. Any unmodeled process leads to domain shift or model misspecification, which can severely bias inference. This is particularly problematic for NRE, which relies on accurate joint modeling of data and parameters~\citep{filipp25}, while NLE is comparatively more interpretable since it operates directly in data space. Even when the theoretical model is sound, the observational and noise models must be equally faithful—a condition often unmet given the traditional divide between theoretical and observational cosmology. Bridging this gap is essential for SBI to succeed. Efforts are underway to diagnose and quantify model misspecification through simulation-based calibration and related approaches~\citep{2018arXiv180406788T,2023PMLR..20219256L,CoLT,montel2025}. While model misspecification is a key vulnerability of SBI, it is not fundamentally different (if more difficult to diagnose and cure) than the similar risk incurred when using explicit, likelihood-based models: missing components of the model w.r.t. the true data-generating process will lead to potentially severe bias in the resulting inference. SBI is a relatively new techniques, and therefore appropriate diagnostic tools are still being developed to ensure its robustness and reliability. 

\paragraph{Training Set Representativity}
A fundamental challenge across deep learning applications in astronomy is the representativity of training data. Models trained on simulations may fail to generalize to real observations, while those trained on current surveys may struggle with the deeper, higher-resolution LSST data. Techniques such as fake source injection—embedding simulated objects into real images—can mitigate these gaps~\citep{2016MNRAS.457..786S,2018PASJ...70S...6H}, though their success depends on how realistic the injected sources are. The problem is particularly acute for rare phenomena, where training examples are intrinsically limited. To improve generalization, domain adaptation, transfer learning, and hierarchical Bayesian methods are being explored. Principled approach such as stratified learning (discussed in section~\ref{sec3:photo-z}) can mitigate covariate shift with little modification in the learning procedure, but other methods often require substantial experimentation and modifications to training procedures~\citep{2025arXiv251019168K}. Such corrections must themselves be treated as part of the inference pipeline and undergo rigorous calibration and uncertainty quantification. 


\paragraph{Physics Hardening}
When available datasets are incomplete or non-representative, physics-informed augmentation can enhance robustness. For example, the DESC ELAsTiCC challenge~\citep{knop2023} injected simulated transients (e.g., supernovae, AGNs, variable stars) to make classifiers more resilient to unmodeled classes. Similarly, latent representations derived from stellar population synthesis (SPS) models can generate synthetic photometry for missing or incomplete observations~\citep[e.g.\ \href{https://github.com/Cosmo-Pop/pop-cosmos}{pop-cosmos}][]{Alsing:2024,Thorp:2025,Deger:2025}, and facilitate comparisons with hydrodynamical simulations without observational systematics. However, these methods inherit the assumptions and uncertainties of the underlying theoretical models—such as uncertain nebular emission strengths in SPS~\citep{Byler2017_Nebular,Li2025_cue,Morisset2025_Nebular,2025arXiv250103133N}—which can themselves introduce model misspecification~\citep{Leistedt2023,Jespersen2025_opticalIR}. Addressing these limitations requires deeper astrophysical modeling of galaxy formation and evolution, as well as diagnostic tools for identifying misspecification in high-dimensional generative models~\citep[e.g.][]{Thorp:2025:QQ}. Because physics-informed generative models (e.g., those capturing information within SPS parameterizations) can be used to synthesize observables that the model has not been trained on, such models can be validated not only against unseen data from a test set but also on new types of observations and other surveys \citep[e.g.][]{Alsing2023, Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}. 


\subsubsection{Validating Inference Results}
\label{sec4:validation}

\begin{ThemeBoxA}
\themebullet \themekey{Addresses.} \challenge{uq-calibration}
\end{ThemeBoxA}

%\note{This section needs to be fleshed out and talk about all the tests for calibration, detecting mispecication and such. Mention PQ-mass/TARP, and more generally good practices to validate the potentially high dimensional approximated posteriors.}

While the quality of neural posteriors can be tested \citep{2018arXiv180406788T, 2023PMLR..20219256L}, and while statistical tests can be performed to determine the probability that the distribution learnt by a generative model is identical to that of the training data \citep{PQMass}, an open issue is the determination and procurement of a sufficient volume of training data for those test to be sufficiently sensitive and for the learned distribution to be accurate.

Models trained with the same data but different algorithms exhibit different probability calibration characteristics that must be evaluated and corrected. Similarly, identical algorithms trained on different training sets require independent calibration assessment. Common diagnostics include reliability diagrams used in time-series classification \citep{Moller:2020} or non-conformity scores from conformal inference techniques \citep{Xie:2025}, both of which compare predicted probabilities against observed frequencies. Detection of anomalies, i.e. the classification whether a signal is anomalous enough to be reported, is particularly vulnerable because it probes the tails of a learned distribution.  For regression tasks, calibration ensures that predicted uncertainties accurately capture the true error distribution. Poorly calibrated uncertainties can introduce systematic biases in downstream cosmological analyses, leading to incorrect parameter constraints. Examples of recalibration methods for lens modeling is \citep{Perreault:2017,Karchev2022GP,gentile23}. Concerning generative models, \cite{Campagne_2025} proposed a “two-models” framework to evaluate their statistical consistencies trained on independent subsets of galaxy images. The results emphasize the need of large enough datasets for enable calibration and validation strategies specific to each generative architecture (e.g. GAN, Normalizing Flows and score-diffusion based), since apparent visual quality and morphological variable distributions alone do not guarantee statistical reliability.


\subsection{Physics-Informed Approaches}
\label{sec4:physics-informed}

% \begin{ThemeBoxA}
% \themebullet \themekey{Related Methodologies.} \meth{physics-informed}, \meth{differentiable-programming} \\
% \themebullet \themekey{Addresses.} \challenge{model-misspecification}, \challenge{systematics-modeling}
% \end{ThemeBoxA}

From a high level point of view, neural networks are never perfectly trustworthy and are often hardly interpretable. This motivates a general desire to build \textit{physics-informed} models, which can leverage as many explicit physical constraints as possible, thus limiting the potential failure modes of AI components. 

\subsubsection{Hybridization of Generative Modeling and Physical Models}
\label{sec4:hybrid-gen-phys}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{normalizing-flow}, \meth{diffusion-model}, \meth{neural-surrogate}, \meth{differentiable-programming} \\
\themebullet \themekey{Addresses.} \challenge{model-misspecification}
\end{ThemeBoxA}

A promising direction for scientific inference is the hybridization of explicit, physics-based models with generative components that flexibly represent unknown or intractable distributions. In this framework, flow-based or diffusion models serve as probabilistic priors over complex latent variables, such as galaxy morphology or small-scale baryonic processes, while the rest of the model remains physically interpretable and simulation-driven. These generative priors have already proven powerful in astronomical inference, for instance in the estimation of galaxy properties and photometric redshifts at scale (e.g.\ \href{https://github.com/Cosmo-Pop/pop-cosmos}{pop-cosmos}; \citealp{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}), and in the generation of high-fidelity, field-level HI maps from dark matter simulations~\citep{Mishra:2025}. More broadly, generative models naturally support amortized inference frameworks, where neural posterior estimators are trained on samples from the generative process, enabling accurate Bayesian inference without MCMC sampling but at the cost of higher upfront training effort.

\paragraph{Differentiable Programming} To fully integrate such probabilistic components with physical simulations, modern astrophysical codes are increasingly being reimplemented in automatic-differentiation libraries. Differentiable simulators eliminate the approximation errors of emulators and provide exact gradient information for optimization and uncertainty quantification. Examples include differentiable particle-mesh cosmological solvers~\citep{FlowPM,DISCO-DJ}, theoretical cosmology computations in \texttt{jax-cosmo}~\citep{JAX-COSMO}, galaxy–halo connection models in \texttt{diffsky}/\texttt{diffstar}~\citep{Diffstar}, stellar population synthesis in \texttt{dsps}~\citep{DSPS}, halo-model calculations in \texttt{halox}~\citep{halox}, and differentiable image simulations in \texttt{jax-galsim}~\citep{jaxgalsim}. 

While the widely used \texttt{GalSim} library~\citep{ROWE2015121} produces realistic galaxy images, its non-differentiable design limits efficient gradient-based inference. In contrast, the emerging \texttt{jax-galsim} library reimplements core \texttt{GalSim} functionalities in JAX, yielding GPU-accelerated, fully differentiable forward models that enable direct gradient computation for both population-level and individual-level parameters. Similarly, \href{https://github.com/pmelchior/scarlet2}{Scarlet2} offers a JAX-based, differentiable framework for non-parametric source morphologies and blended scenes observed by multiple instruments. Both libraries support vectorized batch simulations, crucial for large-scale hierarchical inference, and allow gradients of differentiable likelihoods to be computed automatically for maximum-likelihood estimation, variational inference, or gradient-based MCMC.


\paragraph{High-Dimensional Inverse Problems with Explicit Likelihood and Data-Driven Priors} 
% \todo{Populate this paragraph to talk about deblending, work by Alex Adam, and Benjamin Remy. Concentrate on what are the outstanding issues for these methods: e.g. sampling the posterior is not easy }
High-dimensional inverse problems are central to cosmology, from galaxy deblending and strong-lensing source reconstruction to recovering the dark matter field from noisy data. In these settings, the forward process, such as instrumental response, noise model, or lensing distortion, is well understood and can be encoded in an explicit likelihood. The underlying components, however, such as galaxy morphologies or the non-Gaussian dark matter structure, lack closed-form descriptions and require expressive statistical models. Generative models, such as diffusion models, can learn realistic priors from high-dimensional observations or simulations. Combining such data-driven priors with explicit likelihoods yields a principled framework: the prior enforces realistic structure, while the likelihood anchors inference to the data, even under low signal-to-noise conditions where fully amortized approaches may drift. Recent works have demonstrated this hybrid approach for galaxy source reconstruction and strong lens modeling \citep{adam2022posterior,Barco2025blindinversion} and dark matter field inference \citep{remy2023}. Moreover, the presence of an explicit likelihood enables learning data-driven priors directly from observations, through iterative refinement using posterior samples \citep{rozet2024learning, Barco_2025}. A remaining challenge is efficient posterior sampling, as inference with diffusion priors entails solving stochastic differential equations, which is computationally demanding.



\subsubsection{Imposing Consistency with Physical Equations and Symmetries}
\label{sec4:physics-constraints}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{physics-informed} \\
\themebullet \themekey{Addresses.} \challenge{systematics-modeling}
\end{ThemeBoxA}
%\note{This section can be fleshed out further, it's a bit thin right now.}

For trustworthy results, we additionally demand that the ML outputs at various stages of the pipeline satisfy null tests (e.g. B-modes in gravitational lensing or rho-statistics for PSF modeling, \citealp{10.1111/j.1365-2966.2010.16277.x}) or obey the laws of physics of the corresponding analysis component rather than merely reporting final results with high fidelity. The modularity of the pipelines and multi-scale nature of the phenomena asks for validations at every analysis stage. Crucially, our knowledge of physical relations (in the universe, in the atmosphere, in the instrument) permits a form of validation that is typically omitted or impossible in industry applications of AI/ML. This creates significant research opportunities in areas such as invariant/equivariant representation learning and geometric learning, with possible interdisciplinary implications beyond the scope of DESC \citep{2025arXiv250902661F}. For instance, with Physics Informed Neural Networks (PINNs) one can, in principle, find solutions for explicitly specified differential equations if their optimization could be made more robust \citep{2024arXiv240201868R}. 

\subsection{Novelty Detection and Discovery}
\label{sec:discovery}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{anomaly-detection}, \meth{self-supervised}, \meth{active-learning} \\
\themebullet \themekey{Addresses.} \challenge{out-of-distribution}, \challenge{covariate-shift}
\end{ThemeBoxA}
A central scientific promise of LSST lies in its potential for unexpected discovery. Many now-fundamental astrophysical phenomena, such as strong lenses, fast radio bursts, and pulsars, as well as singular systems like the Bullet Cluster, were first identified as anomalies. With an anticipated catalog exceeding 20 billion galaxies and roughly 10 million alerts per night, however, detecting novel phenomena in LSST data represents an unprecedented challenge.

Generative models offer a powerful framework for unsupervised discovery by learning the ensemble properties of galaxy images, spectra, photometry, and time-domain behavior, and by enabling the detection of statistically anomalous signals without labeled training data~\citep{2023AJ....166...75L}. In time-series and high-energy astronomy, representation learning has already proven effective for discovery-oriented analyses~\citep{Dillmann2025,Song2025}. However, a common failure mode of generative models, where atypical signals appear highly typical \citep{2018arXiv181009136N}, will mean that true outliers may not be recognized, a loss if we seek to find them (e.g. rare supernovae such as PISN) and a problem if they contaminate carefully selected samples used in high-precision cosmology (e.g. unrecognized blends in shape catalogs \citep{Dawson16ublends} or catastrophic outliers in photo-z estimates). 
More specifically, standard unsupervised methods often struggle in the dense and homogeneous latent spaces produced by deep representations~\citep[e.g.][]{Baron2025,StarEmbed2025}. For instance, while \citet{AstronomalyDecals2024} successfully combined Zoobot (a foundation model discussed in \autoref{sec:foundation_models}) features with the \href{https://github.com/MichelleLochner/astronomaly}{Astronomaly} framework~\citep{2021Lochner_Astronomaly} to identify new sources, \citet{ZoobotApplications2022} found that tailored anomaly-detection techniques were necessary even within the well-studied Galaxy Zoo dataset. This line of work culminated in Astronomaly: Protege~\citep{Protege2025}, a general-purpose, active anomaly detection system optimized for exploration in deep latent spaces.

The emergence of deep learning and foundation models (\autoref{sec:foundation_models}) further elevates the importance of active learning, the tight integration of human expertise and machine-driven pattern recognition~\citep{Protege2025}. Self-supervised methods and foundation models promise the ability to generate rich, general-purpose representations, but expert oversight remains essential to interpret their outputs and assess scientific relevance. Automated systems may flag outliers or cluster data effectively, yet human judgment is required to determine which patterns constitute genuine discovery. As AI systems evolve toward agentic operation (\autoref{sec:llm_agentic}), the collaboration between human and machine will become increasingly intertwined. Embedding active-learning capabilities directly within AI infrastructures will therefore be critical to enable rapid, scalable, and participatory scientific discovery—bridging expert analysis and citizen science within the LSST era.







%\note{we can also have a deep dive into methods for anomaly detection, and talk about anomaly detection for stellar streams with methods like CWoLa(https://arxiv.org/abs/2305.03761)}

% \subsection{Executive Summary of Research Priorities and Challenges}
% \note{This is experimental, doesn't render well}
% \paragraph{Bayesian Inference and Uncertainty Quantification}
% \textbf{Key Challenges:} Scaling inference to high-dimensional hierarchical models; disentangling aleatoric and epistemic uncertainties; maintaining calibration under domain shift.\\  
% \textbf{Research Priorities:}
% \begin{itemize}
%     \item Develop gradient-based and neural samplers (e.g., flow-MCMC, diffusion HMC) for large cosmological models.
%     \item Establish standardized benchmarks for posterior calibration and robustness.
%     \item Create automated uncertainty and calibration diagnostics integrated into DESC pipelines.
%     \item Design frameworks for propagating uncertainty through multi-step cosmological analyses.
%     \item Adapt inference software stacks to GPU-accelerated differentiable computing.
% \end{itemize}

% \paragraph{Model Misspecification and Covariate Shifts}
% \textbf{Key Challenges:} Imperfect simulations; unreliable domain-shift detection; limited representativity of training sets; difficulty validating inference under model error.  \\
% \textbf{Research Priorities:}
% \begin{itemize}
%     \item Develop simulation-based calibration and domain-shift diagnostics.
%     \item Advance domain adaptation and transfer learning for improved robustness.
%     \item Use hybrid real/simulated training with fake-source injection to increase realism.
%     \item Integrate physics-hardening strategies combining SPS priors and observational data.
%     \item Benchmark the robustness of SBI methods to model misspecification.
% \end{itemize}

% \paragraph{Physics-Informed Approaches}
% \textbf{Key Challenges:} Neural networks are opaque and often unconstrained by physics; integrating differentiable simulators with data-driven models is complex; physics-informed networks are difficult to train stably.  
% \textbf{Research Priorities:}
% \begin{itemize}
%     \item Develop hybrid generative–physical models with diffusion or flow priors for complex astrophysical processes.
%     \item Expand differentiable simulation ecosystems (\texttt{FlowPM}, \texttt{jax-cosmo}, \texttt{diffsky}, \texttt{jax-galsim}).
%     \item Implement architectures that encode conservation laws and symmetries directly.
%     \item Benchmark physics-informed neural networks (PINNs) and geometric learning for LSST.
%     \item Co-design analysis pipelines and differentiable simulators for end-to-end inference.
% \end{itemize}

% \paragraph{Novelty Discovery}
% \textbf{Key Challenges:} Scaling anomaly detection to petabyte data volumes; false positives in dense latent spaces; limited integration of human feedback.  
% \textbf{Research Priorities:}
% \begin{itemize}
%     \item Develop foundation-model representations for unsupervised discovery.
%     \item Integrate active learning frameworks (e.g., Astronomaly:~Protege) for human–AI collaboration.
%     \item Design representation learning strategies for sparse, temporal, or blended data.
%     \item Create LSST-scale benchmark datasets for anomaly detection.
%     \item Build interactive discovery pipelines that combine human expertise with machine efficiency.
% \end{itemize}

% \paragraph{Neural Architectures for Astronomical Data}
% \textbf{Key Challenges:} Multi-modal data (images, spectra, time-series) require specialized architectures; handling sparsity, irregular sampling, and PSF effects; enforcing physical consistency.  
% \textbf{Research Priorities:}
% \begin{itemize}
%     \item Develop architectures tailored to astronomical data modalities.
%     \item Advance graph and geometric neural networks for cosmic structures and environments.
%     \item Enable multi-modal fusion across imaging, spectroscopy, and time-domain data.
%     \item Integrate physical priors and instrument models into architecture design.
%     \item Benchmark architectures under realistic LSST-like observing conditions.
% \end{itemize}

% \paragraph{Cross-Cutting Recommendations}
% \begin{itemize}
%     \item Integrate validation, calibration, and interpretability checks at every analysis stage.
%     \item Establish DESC-wide benchmarks and reproducibility standards.
%     \item Maintain human-in-the-loop oversight for quality control and physical plausibility.
%     \item Promote physics-aware ML that respects symmetries and differential constraints.
%     \item Invest in differentiable, GPU-accelerated infrastructure for unified simulation–inference workflows.
% \end{itemize}


