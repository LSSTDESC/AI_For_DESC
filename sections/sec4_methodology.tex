\newpage
\section{Methodological Research Priorities to Advance ML for Precision Cosmology}
\label{sec4:aiml_research}

Extracting robust cosmological constraints from \acrshort{lsst} requires not only advanced algorithms but also a coherent methodological foundation that bridges simulation, data processing, and inference. Each of these pillars must meet unprecedented demands in scale, accuracy, and interpretability, demands that challenge the limits of both physical modeling and \acrshort{ml}. Beyond simply applying existing \acrshort{ai} techniques, \acrshort{desc} must develop methods tailored to the structure of astronomical data, the physics of observables, and the statistical rigor required for precision cosmology.

The scientific ambitions of LSST thus motivate AI/ML research in several key areas.
First, Bayesian inference and \acrshort{acr:uq} must evolve to handle the high-dimensional, hierarchical models that describe cosmic fields and galaxy populations, while maintaining interpretability and calibration across vast data volumes.
Second, \acrshort{acr:sbi} and related implicit-likelihood methods must confront the challenge of model misspecification and covariate shifts, ensuring that learned posteriors remain valid when simulations imperfectly represent real observations.
Third, physics-informed modeling, through differentiable programming and hybrid generative–physical architectures, offers a path toward interpretable and physically consistent deep learning, capable of representing both known and unknown components of the Universe.
Fourth, discovery and anomaly detection are essential to LSST’s potential for unexpected science, requiring representation learning and active human-AI collaboration to identify rare and previously unmodeled phenomena.

This section examines these research directions in detail, outlining both recent progress and outstanding challenges. We emphasize not only algorithmic innovation but also the validation, calibration, and interpretability principles required to integrate AI into cosmological analysis pipelines.

The fundamental question is: \textit{What would convince us of a cosmological result obtained with AI?}
Answering this question defines the research agenda for AI/ML in DESC and ensures that ML becomes not merely a computational shortcut, but a scientifically trustworthy component of cosmological inference.


\subsection{Bayesian Inference and Uncertainty Quantification}

\acrshort{acr:uq} represents perhaps the most critical challenge for deploying deep learning in precision cosmology. It is an area where \acrshort{ai} in the sciences demands solutions that differ from those in many commercial settings. Robust UQ must distinguish between aleatoric uncertainties (irreducible measurement noise) and epistemic uncertainties (model limitations and incomplete knowledge), as these have fundamentally different implications for cosmological inference and systematic error budgets.
\acrshort{ml} is poised to be revolutionary for inference, but only if current challenges are satisfactorily addressed. 

\subsubsection{Explicit Likelihood-Based Bayesian Inference}
\label{sec4:Bayes}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{hierarchical-bayes}, \meth{variational-inference}, \meth{gaussian-process} \\
\themebullet \themekey{Addresses.} \challenge{uq}, \challenge{scalability}
\end{ThemeBoxA}
    The inferential paradigm in astrophysical and cosmological data analysis has been for the past two decades primarily Bayesian, as this offers conceptual, methodological, and computational benefits~\citep{Trotta_2008}. The massive increase in data size and complexity afforded by \acrshort{lsst} will require a new step forward in inferential methodology, as LSST data will challenge the computational feasibility of current inferential engines. We cover in this section the case of \textit{explicit inference}, where we have the ability to directly evaluate the log-likelihood of our probabilistic models, and potentially its gradients. 

\paragraph{Accelerating Posterior Inference} \acrshort{mcmc} methods have been the workhorses of likelihood-based Bayesian parameter inference to date. Notable examples are Gibbs sampling \citep{Casella1992} and parallelized versions of Metropolis--Hastings (e.g., the affine-invariant sampler by ~\citealp{goodman2010ensemble, Foreman_Mackey_2013}). For cases where multimodality and/or strong parameter degeneracies are important, nested sampling~\citep{RN599,Ashton_2022} in its many variants (e.g., \texttt{MultiNest}~\citealp{Feroz:2007kg, Feroz:2009}; \texttt{PolyChord}~\citealp{Handley:2015,Handley:2015fda}; \texttt{dynesty}~\citealp{RN1654}; \texttt{DNEst4}~\citealp{Brewer:2016scw}), recently improved with gradients~\citep{lemosGGNS}, accelerated with neural emulators~\citep{Lovick:2025wdj}, or normalizing flows~\citep{RN1653} have been key to ensuring reliable inference.  

However, as analyses become increasingly complex, involving large numbers of nuisance parameters and expensive likelihood evaluations, the cost of running cosmological inference with conventional techniques becomes prohibitive. 

One avenue to speed up inference is to leverage access to the gradients of the log-posterior. When such gradients are available, a number of inference methods can benefit from them, including the well-established \acrshort{hmc} \citep{neal2011hmc}, as well as more modern generalizations such as \acrshort{nuts} \citep{HoffmanGelman2014}. Remarkably, physical ideas continue to lead the development of gradient-based inference techniques. \acrlong{rhmc} \citep[\acrshort{rhmc}][]{RHMC} simulates trajectories in arbitrary geometries by following the geodesics of the likelihood manifold. This makes RHMC schemes extremely robust sampling algorithms for high-dimensional inference in the face of severely non-gaussian posteriors. However, RHMC has not seen a wide spread adoption due to instability of second-order auto-differentiation needed to compute the curvature of the likelihood. In a similar vein, relativity-inspired HMC schemes \citep{RelativtyHMC} have recently been proposed that particularly target Minkowski geometries. This effectively introduces a maximum speed for the simulated particle, slowing it down in the areas of most challenging geometry, achieving most of the goals of RMHC without many of its hurdles. The recent Ray-tracing sampler \citep{2025arXiv251025824B} take a related approach, using light refraction as the guiding analogy to steer samples toward high-likelihood regions while providing a unifying framework in which HMC, LMC, and related methods emerge as special cases. As an alternative to Hamiltonian dynamics, \acrlong{lmc} (\acrshort{lmc}; \citealp{LangevineMC}) is based on a Langevin diffusion (an \acrshort{sde} whose invariant distribution is the target posterior) and in practice simulates a discretization of this process to generate approximate posterior samples. In this framework, the Metropolis adjustment that ensures the target distribution is sampled can be replaced with a bias requirement on the solution of the SDE, leading to significant speed ups \citep{GrumittLangevin}. A final avenue of improvement is to revise the assumed partition function of the particles simulated by the inference algorithm. Traditional HMC schemes assume that the distribution of particles being simulated follows a canonical partition function. However, more efficient sampling schemes can be constructed by exploring other partition functions. Microcanonical or energy-conserving HMC \citep[\acrshort{mchmc},][]{Robnik2022MicrocanonicalHM} explores the posterior distribution using a single energy shell. In a way similar to relativistic schemes, this is achieved by modifying the momentum of the particle to slow down at the regions of high-posterior density \citep{SteegHMC}, leading to a far more efficient sampling. \Acrlong{mclmc} \citep[\acrshort{mclmc};][]{MCLMC} is a sampling algorithm that combines all the ideas described above to great success. MCLMC already has been deployed to perform inference on physics problems such as lattice field theory simulations \citep{MCLMC} and even for cosmology where it has been shown to speed up field-level inference by an order of magnitude \citep{2023arXiv230709504B,simononfroy2025benchmark}. This makes MCLMC the cutting edge of gradient-based inference schemes and a promising tool to speed up analyses within \acrshort{desc}.   

Alternatively, one can replace a complicated posterior distribution with a more tractable one. \acrshort{vi} aims to find an approximation to the posterior distribution $p$ by a ``surrogate" parametrized distribution $q_\phi$, whose parameters $\phi$ are trained to minimize the Kullback-Leibler divergence between $q$ and $p$ (see, e.g., \citealp{Uzsoy_2024, JAX-COSMO}, using the JAX-powered NumPyro framework, \citealp{phan2019composable}). Here gradients are only needed for $q$, not for $p$.

Another area of research focuses on \textit{neural sampling methods}, which leverage in various ways neural networks to accelerate sampling while attempting to preserve asymptotic correctness guarantees. For example, normalizing flows have been used to re-parameterize the sampling space and cure complex geometries \citep{2022PNAS..11909420G}. Another recent line of research also leverages ideas from diffusion models and uses a neural score model to accelerate sampling \citep{2025arXiv250411713H}. 

As shown above, such inference strategies depend on differentiable components and will benefit greatly when likelihood codes are rewritten in frameworks that support automatic differentiation. An additional advantage of making probabilistic models compatible with such frameworks is that they usually support \acrshort{gpu} acceleration and vectorization, which opens up yet another avenue for acceleration---e.g.,the Numpyro \citep{phan2019composable, bingham2019pyro} and BlackJax \citep{cabezas2024blackjax} libraries written in JAX. Affine-invariant samplers \citep[see][]{Foreman_Mackey_2013} are particularly suited to vectorization on GPU hardware, as has been demonstrated in astronomy contexts (e.g.,\citealp{Thorp:2024, Thorp:2025}, using the \href{https://github.com/justinalsing/affine}{\tt affine} sampling code). 

\paragraph{Bayesian model comparison} Estimation of the Bayesian evidence, the central quantity for model comparison, remains challenging when the models being compared are very high dimensional. Nested sampling has been established as one of the main methods for Bayesian evidence computation, but in its original formulation it suffers from the curse of dimensionality: the efficiency of the constrained sampling step decreases rapidly as the dimensionality of parameter space increases. This has been somewhat mitigated by recent developments such as \texttt{PolyChord}~\citep{Handley:2015}, which can be used in a few hundreds of dimensions; \texttt{dynesty}~\citep{RN1654}, which uses dynamical allocation of live points \citep[see also][]{higson19}; and \acrlong{ggns}~\citep[\acrshort{ggns};][]{lemosGGNS}, which exploits gradients, generative flows and differentiable programming to achieve better efficiency and accuracy in up to $\sim 200$ dimensions.
A suite of other methods for the evaluation of the high-dimensional average of the likelihood over the prior are also being explored, sometimes combining density estimation with neural techniques  \citep[e.g.,][]{RN590,mcewen2023,Srinivasan_2024}. However, they remain confined to moderately low-dimensional parameter spaces, of order a few tens of dimensions. 

The frontier represented by evidence estimation in very large dimensional (of order $10^3$ or more) parameter spaces from real data remains largely untouched outside of synthetic demonstration examples where the ground truth is known. \acrshort{nre} shows promise in this respect, in that evidence estimation can be obtained from an NRE architecture by adding a suitable inferential head that is trained only on model labels, thus implicitly marginalizing over all parameters in the model. Such an approach naturally also generalizes to performing Bayesian model averaging. An example of this method is~\cite{SimSIMS}, where six models for empirical corrections for \acrshort{snia} data are compared from \acrfull{csp} observations \citep{krisciunas17} within a Bayesian hierarchical model setting with $\sim 4,000$ latent variables. 

\paragraph{Hierarchical Bayesian Models in Extremely High Dimensions} The manyfold increase in data size requires in many cases a more sophisticated model to capture previously unimportant effects; this in turn increases the dimensionality of the parameter space (especially in hierarchical models, where the latent space dimensionality scales with the number of objects within the model); the likelihood might become intractable, or previously used approximations, such as approximate Gaussianity or linear propagation of errors~\citep{Karchev2022}, neglecting of Eddington bias~\citep{Karchev_STARNRE}, might break down. 

 \autoref{sec3:wlss} introduced so-called full-field inference for cosmological surveys, in which not only cosmological parameters are inferred but also the initial conditions that seed the evolution of the large-scale structure of the Universe~\citep{porqueres2023fieldlevelinferencecosmicshear}. The fidelity of the forward simulation directly controls the accuracy of posterior constraints on cosmological parameters; consequently, this approach requires exploring extremely high-dimensional parameter spaces (millions to billions of parameters). Sampling such spaces is intractable for traditional MCMC and instead calls for gradient-based methods, as noted above. This, in turn, demands a forward simulation that is both fast and differentiable (see \autoref{sec3:sims}) to make full-field inference at LSST scale attainable.

It is worth noting that such hierarchical full-field inference models are substantially more computationally expensive than alternative \acrshort{acr:sbi} methods (see next section~\ref{sec4:sbi}), but offer several advantages. First, analyzing statistical errors directly in data space is more interpretable than working with the compressed summary statistics typical of SBI workflows; even with optimal compression, signals can mix and model misspecification becomes difficult to detect. Here, systematic contamination can be treated as additional parameters to be sampled \citep{2019porqueres}, becoming a machine-aided report of contaminations that have a characteristic pattern on the sky. Second, hierarchical Bayesian inference is designed for extensible, modular models in which new physics can be added—e.g., augmenting the simulation with a baryonification model—whereas SBI would require retraining neural density estimators from scratch. Taken together, these properties make hierarchical Bayesian inference well suited to joint inference of cosmology, systematics, and redshift-distribution uncertainties—capabilities that are considerably more difficult with implicit approaches. Additionally, hierarchical inference provides a digital twin of the Universe, which has multiple scientific applications but also provides a unique way of testing the results by cross-validating with independent data \citep{stopyra24}.

\subsubsection{Implicit Likelihood Bayesian Posterior Inference}
\label{sec4:sbi}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{sbi}, \meth{npe}, \meth{normalizing-flow} \\
\themebullet \themekey{Addresses.} \challenge{uq}, \challenge{scalability}
\end{ThemeBoxA}

The other paradigm is implicit inference, in which we do not assume direct access to the likelihood function, but only have access to samples from the joint distribution $p(x, \theta)$ of data samples $x$ and parameters of interest $\theta$. It should be noted that this situation covers both the case of \acrshort{acr:sbi}, and the case where $x$ and $\theta$ are available from a training sample of real observations (the canonical example being \acrshort{photoz} estimation from a set of spectroscopic observations). 

In particular, SBI is rapidly emerging as a powerful alternative to traditional fitting techniques for Bayesian models. The key idea is to replace an explicit likelihood function by forward simulating (under the model) parameter-data pairs, which are then used to train a neural network to perform inference \citep[e.g.,][]{Alsing2018, AlsingWandelt2018, AlsingWandelt2019,savchenko2024,lyu2025}. The advantages are that the (potentially intractable) likelihood can, in principle, incorporate physical effects of arbitrary complexity, which would otherwise be difficult to model (including, e.g., selection effects and complex parameter dependencies). In some variants~\citep{Miller2021} the 1- or 2-dimensional {\em marginal} distribution for the parameters of interest is targeted directly, thus circumventing the need to evaluate the high-dimensional joint posterior over all parameters in the model; such approaches are naturally suited to Bayesian evidence estimation~\citep{SimSIMS}. 

Additionally, inference can be {\em amortized} within a certain prior range, meaning that once trained the network can deliver almost instantaneous posteriors for a wide range of parameter values, a critical benefit when dealing with billions of galaxies \citep{2022ApJ...938...11H}.  This speed-up also permits posterior calibration methods (e.g., guaranteed coverage), which are computationally unfeasible with traditional posterior evaluation methods.

\paragraph{Neural Density Estimation (NDE) methods} The fundamental building blocks of these methods is \acrshort{nde}, where a neural network is used to estimate a distribution, or a ratio of distributions. Various kinds of methods exist: \acrshort{nle} (e.g., \citealp{Papamakarios2019, Lueckmann2019, Alsing:2019}), \acrshort{acr:npe} (e.g., \citealp{PapamakariosMurray2016, Lueckmann2017}) and \acrshort{nre} are among the most popular (for an overview see~\citealp{Alsing:2019, Cranmer2020, Lueckmann2021}). Implementations of NLE and NPE both learn a density based on simulated parameter-data pairs (see, e.g., \citealp{Alsing:2019}), with a variety of different approaches used for learning the multivariate joint or conditional density. Approaches to this include Gaussian mixtures \citep[e.g.,][]{Alsing2018}, mixture density networks \citep[e.g.,][]{PapamakariosMurray2016}, and normalizing flows (e.g., \citealp{Papamakarios2017, Papamakarios2019, Alsing:2019, Jeffrey:2021, 2022ApJ...938...11H}; for a review see \citealp{Kobyzev2021, Papamakarios2021}). More sophisticated density estimators used in generative modeling -- such as continuous normalizing flows \citep{Grathwohl2018, Chen2018}, score-based diffusion models \citep{Song:2020}, flow-matching models \citep{Lipman:2022}, and transformers \citep{Transformers2017} -- are also well suited to NLE and NPE tasks \citep[e.g.,][]{DiazRiveroDvorkin2020, Geffner:2022, Wildberger:2023, Gloeckler:2024} alongside the generative modelling tasks they are commonly used for \citep[e.g.,][]{Alsing:2024, Cuesta:2024, Thorp:2025}. 

\paragraph{Optimal Neural Summarization} 
To ensure the robustness of implicit inference, the process is usually divided into two main steps enabling each neural network to focus on a specific task: (1) compression of high-dimensional data into informative summary statistics, and (2) performing Bayesian inference using neural density estimation methods on this low dimensional but highly informative statistic. To maximize information extraction and improve constraints on cosmological parameters, the community has increasingly adopted neural network–based summarization techniques. While any neural network can be trained on the regression task of inferring parameters given data \citep[e.g.,][]{Gupta2018MAE, kacprzak2022deeplss, lu2023cosmological}, it is unclear how much of the information contained in the data is extracted. In particular, \cite{neural_summary_lanzieri_2025} demonstrate that standard regression loss functions do not guarantee the systematic construction of sufficient statistics. \Acrlongpl{imnn} (\acrshortpl{imnn}; \citealp{2018PhRvD..97h3004C}) directly address this problem by learning summary functions that maximize the Fisher information. They can produce nearly exact posteriors and are thus approximately sufficient statistics of the data. Another approach is to derive a loss function directly from the definition of sufficiency, i.e., by maximizing the mutual information between the summary statistics and parameters of interest \citep{Jeffrey:2021, chen2021neural}.


\paragraph{Controlling Epistemic Errors in Inference Results} One fundamental limitation of NDE methods is that their reliance on a neural network to model at some level the likelihood of the data is inherently imperfect. In the asymptotic regime of infinite data and flexible neural network, the approximation to the target posterior will converge, but in practice we are never guaranteed to find ourselves in this regime, and must therefore take into account and mitigate \textit{epistemic errors}. Several strategies have been developed over the years to quantify and mitigate this epistemic uncertainty on inference results.  \acrshort{mcmc} sampling over network parameters provides gold-standard uncertainty estimates but at usually prohibitive computational cost \citep[e.g.][]{2025arXiv251025824B}. In addition, detecting convergence of the chain remains difficult, usually necessitating drawing more samples than ultimately needed. Because such approach is extremely expensive, other approaches have been developed. \textit{\acrshortpl{bnn}} approximate the posterior distribution over network parameters through \acrshort{vi}, providing principled uncertainty estimates at reduced computational cost. However, the tradeoff between approximation quality and speed remains concerning, especially in the highly multi-modal loss landscapes of deep neural networks. In such settings, scalable variational methods often collapse to a single mode of the posterior rather than exploring the full diversity of solutions \citep{fort2020losslandscape}, which limits the quality of their uncertainty estimates. One of the most used VI methods in astronomy is Monte Carlo dropout which utilizes the dropout layer commonly introduced in deep neural networks to prevent correlated activation as one of the computationally cheapest approximations to Bayesian inference; however, its theoretical justification and empirical accuracy are questionable \citep{LeFolg:2021}. Nonetheless, comparisons with other methods have shown promise for astronomical applications: strong lensing \citep{Perreault:2017}, supernova time-series classification \citep{Moller:2020,Moller:2022ICML}, and star time-series classification \citep{Astromer2023, CadizLeyton:2025, CadizLeyton:2025:MoE}. Other VI methods such as Bayes by Backprop and SWAG have been sparsely used for time-series classification and regression with mixed results \citep{Moller:2020,Cranmer:2021}. Another strategy is Deep
Ensembles in which multiple networks are trained from different random initializations to provide uncertainty estimates via the variance of their predictions \citep{Makinen:2021,Moller:2022,Moller:2024}. Unlike variational methods, ensembles capture uncertainty by effectively sampling from different modes of the loss landscape, resulting in more robust and better-calibrated uncertainty estimates. However, they are more computationally demanding than MC dropout, and do not mitigate errors arising from model misspecification. Comparative studies exploring the trade-offs between these \acrshort{acr:uq} methodologies for achieving superior uncertainty evaluation are a growing focus in the field \citep{CadizLeyton:2025}.


\subsubsection{Model Mispecification and Covariate Shifts}
\label{sec4:model-misspec}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{sbi} \\
\themebullet \themekey{Addresses.} \challenge{covariate-shift}
\end{ThemeBoxA}

From a technical standpoint, \acrshort{acr:sbi} has achieved impressive results: \acrshort{acr:npe} with normalizing flows performs well in low-dimensional regimes~\citep[e.g.,][]{srinivasan2025}, \acrshort{nle} scales satisfactorily to higher dimensions, and marginal \acrshort{nre} has shown success across diverse applications~\citep[e.g.,][]{alvey2024,list2023,Franco_Abell_n_2024,Saxena_2024}. Yet, these demonstrations rely primarily on simulated data and therefore represent best-case scenarios; direct validations of SBI on real data remain scarce~\citep{2024Karchev_SIDEreal,Lueber2025}.

The robustness of SBI depends critically on the realism and completeness of the simulations that underpin it. Simulations must reproduce all relevant aspects of the observations, including astrophysical, instrumental, and observational effects. Any unmodeled process leads to domain shift or model misspecification, which can severely bias inference. This is particularly problematic for NRE, which relies on accurate joint modeling of data and parameters~\citep{filipp25}, while NLE is comparatively more interpretable since it operates directly in data space. Even when the theoretical model is sound, the observational and noise models must be equally faithful—a condition often unmet given the traditional divide between theoretical and observational cosmology. Bridging this gap is essential for SBI to succeed. Efforts are underway to diagnose and quantify model misspecification through simulation-based calibration and related approaches~\citep{2018arXiv180406788T,2023PMLR..20219256L,CoLT,montel2025,kelly2025simulation}. While model misspecification is a key vulnerability of SBI, it is not fundamentally different (if more difficult to diagnose and cure) than the similar risk incurred when using explicit, likelihood-based models: missing components of the model w.r.t. the true data-generating process will lead to potentially severe bias in the resulting inference. SBI is a relatively new technique, and therefore appropriate diagnostic tools are still being developed to ensure its robustness and reliability. 

\paragraph{Training Set Representativity}
A fundamental challenge across deep learning applications in astronomy is the representativity of training data. Models trained on simulations may fail to generalize to real observations, while those trained on current surveys may struggle with the deeper, higher-resolution \acrshort{lsst} data. Techniques such as fake source injection—embedding simulated objects into real images—can mitigate these gaps~\citep{2016MNRAS.457..786S,2018PASJ...70S...6H}, though their success depends on how realistic the injected sources are. The problem is particularly acute for rare phenomena, where training examples are intrinsically limited. To improve generalization, domain adaptation, transfer learning, and hierarchical Bayesian methods are being explored. E.g., \cite{Swierz2024} use domain adaptation to obtain more robust data summaries that can generalize well between simulated data and mock observations, enabling more accurate neural density estimation. Principled approaches such as stratified learning (discussed in Section~\ref{sec3:photo-z}) can mitigate covariate shift with little modification in the learning procedure, but other methods often require substantial experimentation and modifications of training procedures~\citep{2025arXiv251019168K}. Such corrections must themselves be treated as part of the inference pipeline and undergo rigorous calibration and uncertainty quantification. 


\paragraph{Physics Hardening}
When available datasets are incomplete or non-representative, physics-informed augmentation can enhance robustness. For example, the \acrshort{desc} \acrshort{elasticc} challenge~\citep{knop2023} injected transients simulated using semi-analytic models (e.g., \acrshort{sne}, \acrshort{kne}) to make classifiers more resilient to underrepresented classes, and \cite{Moskowitz2024} augmented spectroscopically-incomplete training samples with simulated photometry to improve photometric redshift estimation. Latent representations derived from \acrshort{sps} models can also be used to generate synthetic photometry for missing or incomplete observations~\citep[e.g., \href{https://github.com/Cosmo-Pop/pop-cosmos}{\tt pop-cosmos}][]{Alsing:2024,Thorp:2025,Deger:2025}, and facilitate comparisons with hydrodynamical simulations without observational systematics. However, these methods inherit the assumptions and uncertainties of the underlying theoretical models—such as uncertain nebular emission strengths in SPS~\citep{Byler2017_Nebular,Li2025_cue,Morisset2025_Nebular,2025arXiv250103133N}—which can themselves introduce model misspecification~\citep{Leistedt2023,Jespersen2025_opticalIR}. Addressing these limitations requires deeper astrophysical modeling of galaxy formation and evolution, as well as diagnostic tools for identifying misspecification in high-dimensional generative models~\citep[e.g.,][]{Thorp:2025:QQ}. Because physics-informed generative models (e.g., those that capture information within SPS parameterizations) can be used to synthesize observables that the model has not been trained on, such models can be validated not only against unseen data from a test set but also on new types of observations and other surveys \citep[e.g.,][]{Alsing2023, Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}. 


\subsubsection{Validating Inference Results}
\label{sec4:validation}

\begin{ThemeBoxA}
\themebullet \themekey{Addresses.} \challenge{uq}, \challenge{metrics}
\end{ThemeBoxA}

While the quality of neural posteriors can be tested \citep{2018arXiv180406788T, 2023PMLR..20219256L}, and while statistical tests can be performed to determine the probability that the distribution learned by a generative model is identical to that of the training data \citep{PQMass}, an open issue is the determination and procurement of a sufficient volume of training data for those tests to be sufficiently sensitive and for the learned distribution to be accurate.

Models trained on the same data but with different algorithms exhibit distinct probability calibration characteristics that must be evaluated and corrected. Similarly, identical algorithms trained on different training sets require independent calibration assessment. Common diagnostics include reliability diagrams used in time-series classification \citep{Moller:2020} or non-conformity scores from conformal inference techniques \citep{Xie:2025}, both of which compare predicted probabilities against observed frequencies. Detection of anomalies, i.e., the classification whether a signal is anomalous enough to be reported, is particularly vulnerable because it probes the tails of a learned distribution.  For regression tasks, calibration ensures that predicted uncertainties accurately capture the true error distribution. Poorly calibrated uncertainties can introduce systematic biases in downstream cosmological analyses, leading to incorrect parameter constraints. Recalibration methods for lens modeling are presented by, e.g.,  \citet{Perreault:2017}, \citet{Karchev2022GP}, and \citet{gentile23}. Regarding generative models, \cite{Campagne_2025} propose a “two-models” framework to evaluate their statistical consistencies trained on independent subsets of galaxy images. The results emphasize the need for large-enough datasets to enable calibration and validation strategies specific to each generative architecture (e.g., generative adversarial networks, normalizing flows and score-based diffusion), since apparent visual quality and morphological variable distributions alone do not guarantee statistical reliability.


\subsection{Physics-Informed Approaches}
\label{sec4:physics-informed}

From a high-level point of view, neural networks are never perfectly trustworthy and are often hardly interpretable. This motivates a general desire to build \textit{physics-informed} models, which can leverage as many explicit physical constraints as possible, thus limiting the potential failure modes of \acrshort{ai} components. 

\subsubsection{Hybridization of Generative Modeling and Physical Models}
\label{sec4:hybrid-gen-phys}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{normalizing-flow}, \meth{diffusion-model}, \meth{neural-surrogate}, \meth{differentiable-programming} \\
\themebullet \themekey{Addresses.} \challenge{covariate-shift}, \challenge{scalability}
\end{ThemeBoxA}

A promising direction for scientific inference is the hybridization of explicit, physics-based models with generative components that flexibly represent unknown or intractable distributions. In this framework, flow-based or diffusion models serve as probabilistic priors over complex latent variables, such as galaxy morphology or small-scale baryonic processes, while the rest of the model remains physically interpretable and simulation-driven. These generative priors have already proven powerful in astronomical inference, e.g., in the estimation of galaxy properties and photometric redshifts at scale (e.g.,  \href{https://github.com/Cosmo-Pop/pop-cosmos}{\tt pop-cosmos}; \citealp{Alsing:2024, Thorp:2024, Thorp:2025, Deger:2025}), and in the generation of high-fidelity, field-level H\,{\sc i} maps from dark matter simulations~\citep{Mishra:2025}. More broadly, generative models naturally support amortized inference frameworks, where neural posterior estimators are trained on samples from the generative process, enabling accurate Bayesian inference without \acrshort{mcmc} sampling but at the cost of greater upfront training effort.

\paragraph{Differentiable Programming} To fully integrate such probabilistic components with physical simulations, modern astrophysical codes are increasingly being reimplemented in automatic-differentiation libraries. Differentiable simulators eliminate the approximation errors of emulators and provide exact gradient information for optimization and uncertainty quantification. Examples include differentiable particle-mesh cosmological solvers~\citep{FlowPM,DISCO-DJ}, theoretical cosmology computations in \href{https://github.com/DifferentiableUniverseInitiative/jax_cosmo}{\texttt{jax-cosmo}}~\citep{JAX-COSMO}, galaxy–halo connection models in \href{https://github.com/ArgonneCPAC/diffsky}{\texttt{Diffsky}}/\href{https://github.com/ArgonneCPAC/diffstar/}{\texttt{Diffstar}}~\citep{Diffstar}, stellar population synthesis in \href{https://github.com/ArgonneCPAC/dsps/}{\texttt{DSPS}}~\citep{DSPS}, halo-model calculations in \href{https://github.com/fkeruzore/halox}{\texttt{halox}}~\citep{halox}, and differentiable image simulations in \href{https://github.com/GalSim-developers/JAX-GalSim}{\texttt{JAX-GalSim}}~\citep{jaxgalsim}. 

While the widely used \href{https://github.com/GalSim-developers/GalSim}{\texttt{GalSim}} library~\citep{ROWE2015121} produces realistic galaxy images, its non-differentiable design limits efficient gradient-based inference. In contrast, the emerging \href{https://github.com/GalSim-developers/JAX-GalSim}{\texttt{JAX-GalSim}} library reimplements core \texttt{GalSim} functionalities in JAX, yielding \acrshort{gpu}-accelerated, fully differentiable forward models that enable direct gradient computation for both population-level and individual-level parameters. Similarly, \href{https://github.com/pmelchior/scarlet2}{\tt scarlet2} offers a JAX-based, differentiable framework for non-parametric source morphologies and blended scenes observed by multiple instruments. Both libraries support vectorized batch simulations, crucial for large-scale hierarchical inference, and allow gradients of differentiable likelihoods to be computed automatically for maximum-likelihood estimation, variational inference, or gradient-based MCMC.


\paragraph{High-Dimensional Inverse Problems with Explicit Likelihood and Data-Driven Priors} 
High-dimensional inverse problems are central to cosmology, from galaxy deblending and strong-lensing source reconstruction to recovering the dark matter field from noisy data. In these settings, the forward process, such as instrumental response, noise model, or lensing distortion, is well understood and can be encoded in an explicit likelihood. The underlying components, however, such as galaxy morphologies or the non-Gaussian dark matter structure, lack closed-form descriptions and require expressive statistical models. Generative models, such as diffusion models, can learn realistic priors from high-dimensional observations or simulations. Combining such data-driven priors with explicit likelihoods yields a principled framework: the prior enforces realistic structure, while the likelihood anchors inference to the data, even under low \acrshort{snr} conditions where fully amortized approaches may drift. Recent works have demonstrated this hybrid approach for galaxy source reconstruction, strong lens modeling, and superresolution \citep{adam2022posterior,Barco2025blindinversion,2025Adam_SBProfiles} and dark matter field inference \citep{remy2023}. Moreover, the presence of an explicit likelihood enables learning data-driven priors directly from observations, through iterative refinement using posterior samples \citep{rozet2024learning, Barco_2025}, and can even allow for the correction of model misspecification \citep{Payot2025}. A remaining challenge is efficient posterior sampling, as inference with diffusion priors entails solving \acrfullpl{ode}, which is computationally demanding, although it can be performed practically at scale \citep[see, e.g.,][]{Thorp:2024, Thorp:2025}.



\subsubsection{Imposing Consistency with Physical Equations and Symmetries}
\label{sec4:physics-constraints}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{physics-informed} \\
\themebullet \themekey{Addresses.} \challenge{covariate-shift}, \challenge{metrics}
\end{ThemeBoxA}
%\note{This section can be fleshed out further, it's a bit thin right now.}

For trustworthy results, we additionally demand that the \acrshort{ai}/\acrshort{ml} outputs at various stages of the pipeline satisfy null tests (e.g., B-modes in gravitational lensing or rho-statistics for \acrshort{psf} modeling, \citealp{10.1111/j.1365-2966.2010.16277.x}) or obey the laws of physics of the corresponding analysis component rather than merely report final results with high fidelity. The modularity of the pipelines and multi-scale nature of the phenomena asks for validations at every analysis stage. Crucially, our knowledge of physical relations (in the universe, in the atmosphere, in the instrument) permits a form of validation that is typically omitted or impossible in industry applications of AI/ML. This motivates research in areas such as invariant/equivariant representation learning and geometric learning, with possible interdisciplinary implications beyond the scope of \acrshort{desc} \citep{2025arXiv250902661F}. Furthermore, the use of symmetry-aware \acrfullpl{enn} could help with the extraction of more robustness features from the data and, in combination with domain adaptation, enable easier mitigation of covariate shifts~\citep{Pandya2025}. E.g., with \acrfullpl{pinn} one can, in principle, find solutions for explicitly specified differential equations if their optimization could be made more robust \citep{2024arXiv240201868R}. 

\subsection{Novelty Detection and Discovery}
\label{sec:discovery}

\begin{ThemeBoxA}
\themebullet \themekey{Related Methodologies.} \meth{anomaly-detection}, \meth{self-supervised}, \meth{active-learning} \\
\themebullet \themekey{Addresses.} \challenge{covariate-shift}, \challenge{data-sparsity}
\end{ThemeBoxA}
A central scientific promise of \acrshort{lsst} lies in its potential for unexpected discovery. Many now-fundamental astrophysical phenomena, such as strong lenses, fast radio bursts, and pulsars, as well as singular systems like the Bullet Cluster, were first identified as anomalies. With an anticipated catalog exceeding 20 billion galaxies and roughly 10 million alerts per night, however, detecting novel phenomena in LSST data represents an unprecedented challenge.

Generative models offer a powerful framework for unsupervised discovery by learning the ensemble properties of galaxy images, spectra, photometry, and time-domain behavior, and by enabling the detection of statistically anomalous signals without labeled training data~\citep{2023AJ....166...75L}. In time-series and high-energy astronomy, representation learning has already proven effective for discovery-oriented analyses~\citep{Dillmann2025,Song2025}. However, a common failure mode of generative models, where atypical signals appear highly typical \citep{2018arXiv181009136N}, will mean that true outliers may not be recognized, a loss if we seek to find them (e.g., rare SN types such as pair instability \acrshort{sne}) and a problem if they contaminate carefully selected samples used in high-precision cosmology (e.g.,unrecognized blends in shape catalogs, \citealp{Dawson16ublends}; or catastrophic outliers in \acrshort{photoz} estimates). 
More specifically, standard unsupervised methods often struggle in the dense and homogeneous latent spaces produced by deep representations~\citep[e.g.,][]{Baron2025,StarEmbed2025}. E.g., while \citet{AstronomalyDecals2024} successfully combined Zoobot (a foundation model discussed in \autoref{sec:foundation_models}) features with the \href{https://github.com/MichelleLochner/astronomaly}{\tt Astronomaly} framework~\citep{2021Lochner_Astronomaly} to identify new sources, \citet{ZoobotApplications2022} found that tailored anomaly-detection techniques were necessary even within the well-studied Galaxy Zoo dataset. This line of work culminated in \texttt{Astronomaly} \texttt{Protege}~\citep{Protege2025}, a general-purpose, active anomaly detection system optimized for exploration in deep latent spaces.

The emergence of deep learning and foundation models (\autoref{sec:foundation_models}) further elevates the importance of active learning, the tight integration of human expertise and machine-driven pattern recognition~\citep{Protege2025}. Self-supervised methods and foundation models promise the ability to generate rich, general-purpose representations, but expert oversight remains essential to interpret their outputs and assess scientific relevance. Automated systems may flag outliers or cluster data effectively, yet human judgment is required to determine which patterns constitute genuine discovery. As \acrshort{ai} systems evolve toward agentic operation (\autoref{sec:llm_agentic}), the collaboration between human and machine will become increasingly intertwined. Embedding active-learning capabilities directly within AI infrastructures will therefore be critical to enable rapid, scalable, and participatory scientific discovery—bridging expert analysis and citizen science within the LSST era.