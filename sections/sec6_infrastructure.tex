\newpage
\section{Infrastructure Requirements to Support AI/ML in DESC}
\label{sec6:infra_requirements}
% \coordinator{Adam Bolton}
%\sectioninstr{Summary of the computing, data management, and infrastructure needed to deploy/scale existing and emerging AI/ML across DESC. Addresses benchmarking standards and frameworks for reproducibility. Subsections: Computing Resources, Data Management and Access, Benchmarking and Reproducibility.}

Infrastructure is the shared technology needed to realize the methods, models, and scientific opportunities outlined in the preceding sections. It may be deployed in a distributed mode with individuals or small teams using local or institutional resources, or in a coordinated mode within a shared platform or environment. This latter mode is most relevant for the largest-scale \acrshort{ai}/\acrshort{ml}-enabled model training and inference workflows at the scale of the entire Rubin-\acrshort{lsst} data set, including via core infrastructure through \acrshort{doe}-funded high performance computing facilities in the US. The following subsections review the infrastructure elements most relevant to the future of AI/ML in \acrshort{desc}.

\subsection{Software}
% What are the important points we want to surface here?
%  - There is a notion of building up a robust software stack for scientific AI.
%      - What is critical to the success of a large organization like DESC is strong 
%        Leadership on adoptingindustry standard solutions for that software stack. With a focus
%        on ensuring that our stack will survive a few years (e.g. frameworks disappearing)
%      - Notion of coordination with supporting facilities like CC/IN2P3 or NERSC on making 
%        these choices and deploying them for DESC members
%  - Then there is a notion that increasingly, DESC analysis pipelines may start to rely on
%    AI components. 
%       - Foundation models served as a service for X
%       - Dedicated models integrated in analysis workflows
%       - SBI and such 
%       - All the way to integrating and/or enabling agents to interface with our systems

Software is foundational to modern cosmology, especially for \acrshort{desc}, where scientific insight increasingly depends on sophisticated computational workflows. As \acrshort{ai} and \acrshort{ml} mature into core scientific technologies, software itself becomes a form of research infrastructure. In this context, two tightly connected priorities emerge: first, the development and long-term stewardship of a robust AI software stack that enables model development and experimentation; second, the strategic integration of AI methods into the DESC scientific pipelines, where they will ultimately shape how we reduce data, extract measurements, and perform inference.

\subsubsection{The AI Software Stack} 

DESC has long demonstrated leadership in scientific software development: from collaboration-wide development guidelines\footnote{\url{https://lsstdesc.org/assets/pdf/docs/DESC_Coding_Guidelines_latest.pdf}}
 and software-oriented publication policies, to a culture of reproducibility and sustained support for collaboration-wide software stacks. As AI becomes a first-class component of scientific analysis, extending that same discipline and strategic thinking to the AI software ecosystem is increasingly important. The goal is to define a modern, durable, and interoperable stack built on best practices for reproducibility and maintainability. This accelerates research while preserving the transparency that has always characterized DESC software.
 
To meet this goal, we recommend converging on a small set of shared practices and services that make ML development reproducible, portable to the \acrfull{cc-in2p3} and \acrfull{nersc}, and sustainable over the 10-year duration of the LSST survey. Key components include:

\begin{itemize}
\item \textbf{Frameworks for model development}, likely PyTorch for large models and JAX for differentiable physics.
\item \textbf{Experiment tracking}, capturing code/data versions, configurations, metrics, and compute environments to ensure reproducibility.
\item \textbf{Model and artifact registries} to version and archive trained models, datasets, and reports, mirroring survey data-release practices.
\item \textbf{Standardized export formats} such as \acrfull{onnx} so models integrate cleanly with Rubin/DESC pipelines and \acrfull{hpc} environments.
\item \textbf{\Acrfull{ci} and \acrfull{cd} for models} to test and validate training configurations, exported artifacts, and deployment environments.
\item \textbf{Observability} and drift monitoring so ML components used in production behave predictably and transparently.
\end{itemize}

These elements are not overhead; they are the operational foundations that allow AI to become reliable scientific machinery rather than episodic experimentation. They also reduce long-term maintenance burden by enforcing shared conventions, minimizing bespoke tooling, and allowing learned models to be audited, reused, and trusted throughout the \acrshort{lsst} decade.

In addition to these considerations for ML model development, DESC will increasingly rely on \acrshortpl{llm} as flexible interfaces to data, documentation, and tooling. LLMs are unusual compared to conventional models in that they are supplied by a rapidly changing ecosystem of commercial providers, open models, and self-hosted deployments. Versions change frequently, and some use cases may require on-premises or institutionally-hosted models---e.g., at NERSC, CC-IN2P3, the \acrfull{csd3}, or similar facilities---via serving stacks such as vLLM for data-governance or cost reasons. To remain agile in this landscape, DESC should treat LLMs as interchangeable backends behind a stable internal \acrshort{api}. Such an abstraction layer enables swapping models without rewriting pipelines and moving workloads between commercial services and collaboration-owned \acrshort{gpu} resources as needs evolve. Beyond the LLMs themselves, agentic frameworks (libraries that orchestrate multi-step LLM workflows, tool calls, and human-in-the-loop interactions) are even more volatile, with new options appearing and disappearing on timescales of months. Frameworks such as LangGraph exemplify the current direction, representing agents as graphs of tools, memory, and control logic, and providing execution and tracing engines around them. 

Framework sustainability and openness should play an important role in guiding tooling choices. Solutions with strong governance and broad adoption (e.g., PyTorch under the Linux Foundation, ONNX, MLflow or self-hostable experiment-tracking systems) provide long-term stability and avoid dependence on proprietary silos. Finally, coordination with computing partners is essential to ensure portability of the software stack and deployment on more HPC-aligned facilities.

\subsubsection{Integration of AI/ML within Analysis Pipelines}
% PRevious content
% The space of “AI for software” is extensive and rapidly evolving. A non-exhaustive selection of topics of interest to DESC science is:
% \begin{itemize}
% \item Integrated AI support within notebook environments enabling rapid development and iteration of scripting software for data exploration and analysis.
% \item New approaches to data-reduction pipelines that optimize AI models directly against observational data rather than operating sequentially in a classical ``raw-to-reduced'' data pipeline workflow.
% \item Foundation Model-based implementations of X-as-a-service, where X is, for example, photometry, astrometry, cross-matching, classification, or anomaly detection.
% \item AI-based ``digital twin'' models for experimental apparatus that enable more precise and accurate calibration and control of systematics than traditional approaches.
% \item AI/ML-based cosmological inference frameworks (e.g., FMs, simulation-based inference, field-level inference)
% \item Use of specialized agents to complement DESC’s analyses by querying and exploring the heterogeneous landscape of astronomical data sets available across different repositories.
% \end{itemize}


As AI components mature, DESC pipelines may evolve from purely sequential “raw-to-reduced” workflows to systems where learned models are first-class, production-grade elements. The emphasis is on embedding AI in ways that enhance measurement fidelity, calibration control, and inference efficiency, while preserving transparency, reproducibility, and smooth integration with existing practices.

\paragraph{Data reduction pipelines} AI/ML is already present in DESC workflows (e.g., \acrshort{photoz} estimation within \acrshort{rail}). Over time, more components are likely to incorporate learned models at defined stages such as deblending, \acrshort{psf} estimation, artifact rejection, and photometric calibration, tightly coupled to Rubin/DESC data structures and HPC execution. In parallel, DESC has interest in end-to-end approaches that optimize models directly against observational data and simulators, potentially replacing brittle stage boundaries while maintaining provenance through experiment tracking and model registries.

\paragraph{Foundation Models as Services} With the advent of the foundation model paradigm, we envision the possibility of providing “X-as-a-service” (photometry, astrometry, cross-matching, classification, anomaly detection) behind stable APIs, so models can evolve without churn in downstream code.

\paragraph{AI/ML-based cosmological inference} AI-based methods are shifting inference from sampler-centric workflows toward models that learn mappings from data to posteriors or summaries. Foundation-style surrogates can amortize computation and reduce \acrshort{mcmc}-heavy workloads, while simulation-based inference trains directly on forward models and field-level methods operate on minimally reduced data. Active-learning loops may trigger simulations on the fly to refine surrogates and concentrate the HPC budget where it most reduces uncertainty. For pipeline use, these elements remain tied to provenance systems and model registries.

\paragraph{Shared infrastructure for emulators and surrogate models} Consistency across emulator efforts can lower reuse costs. A lightweight scaffold could include common interfaces (inputs, units, cosmology/observational context, stochastic controls, uncertainty outputs), a minimal dataset schema for training/evaluation, embedded metadata for provenance and validity domains, and containerized artifacts with dual exports (native framework and ONNX where feasible). A small validation suite (accuracy, coverage, calibration under shift, throughput) running in CI on facility images would help with portability to DESC computing facilities, and hooks for active learning can keep surrogates co-evolving with forward models.

\paragraph{LLMs and agents} Large language models and specialized agents can contribute at multiple levels of DESC work: in notebook environments as integrated assistants that accelerate iteration on analysis code, diagnostics, and documentation/query synthesis; in data discovery and curation by searching heterogeneous archives and DESC repositories, proposing cross-matches, and flagging anomalies; and at facility scale by assembling templated workflows, submitting authenticated jobs, and recording outcomes into experiment tracking and model registries. Adoption pairs capability with governance: standardized interfaces for serving models, authenticated access, audit logs of prompts and actions, and human-in-the-loop checkpoints for any decision with scientific impact.


Likely, investment in general-purpose AI tooling across industry, open-source, and public efforts will continue to exceed resources available for cosmology-specific software. When such tools meet DESC’s scientific and operational needs, adopting them can leverage broader community advances while allowing collaboration effort to focus on domain-specific modeling and validation. Finally, AI will also influence our approach to engaging with computing in the future. Computational infrastructure will increasingly be harnessed with the assistance of LLMs and agentic frameworks. Depending on how this transition proceeds, it may enable a larger community of researchers within DESC to engage directly with large-scale scientific computing.


\subsection{Computing}

\subsubsection{Workflows and Scales}

Estimating the full scale of resources needed to support the range of \acrshort{desc} \acrshort{ai}/\acrshort{ml} use cases is beyond the scope of this white paper, and any estimates will evolve with time as new methodologies and science applications are developed. Major resource categories include \acrshort{gpu} and \acrshort{cpu} time, short- and long-term storage, and network bandwidth both between and within analysis facilities. AI-oriented workloads will range from small-scale \acrfull{rd} to training and serving \acrshortpl{fm}, serving tokens for agents/\acrshortpl{llm}, running on-the-fly simulations for active learning loops for \acrshort{acr:sbi}, up to potentially running simulations on the fly as part of explicit inference loops. All of these will bring their own requirements for computing cycles, data locality and throughput, interactivity, and orchestration.

At the low end of requirements, resource-augmented instances of the commercial cloud-based \acrfull{rsp} would provide an accessible route for individuals and small teams to scale up AI/ML workflows that require integration with the Rubin data and software stack. For cost efficiency, these resources would likely need to be managed through either a batch-processing system or through an elastic data-science workflow system. Allocating GPU-based interactive servers (virtual or physical) in the cloud-based RSP context is unlikely to be viable at scale for many users, given the typical idle time in interactive sessions and workflows. Larger AI workflows could also be accommodated through individual or DESC-wide allocations of CPU, GPU, and working storage at \acrshort{nersc} or through proposal-driven allocations under the 10\% of computing time reserved for Rubin science users at the Rubin \acrfull{usdf} at \acrfull{slac}.

On the high end, significant computing resources  may be needed for new simulations to train simulation-based inference approaches to large-scale cosmology analyses. This includes not only the computing needed for simulation but also the storage to save and share large simulation outputs. Another major driver of resource needs would be training of data-oriented FMs at the scale of the full Rubin data set. Again, an accurate estimate of the resources needed for this would require further study of an appropriate reference architecture and training strategy. A rough guide to the scale can be found from the (CPU-based) compute sizing model for Rubin--\acrshort{lsst} Data Release Production\footnote{\url{https://dmtn-135.lsst.io}} since it is operating on roughly the same scale of data as a full-scale LSST-based FM would train on. The operations compute model estimates a need for about 50M core-hours in year 1 of the survey, rising linearly year over year (assuming annual data releases) to roughly 10x this amount in year 10. Storage needs are estimated to increase from 30PB in year 1 to over 800PB in year 10 although not all of the associated data products would necessarily be needed for FM training.

In addition to ``offline'' computing needs, a number of time-critical AI/ML-driven DESC workflows will be driven by the nightly alert stream, including ML-driven algorithms for classification, anomaly detection, and intelligent follow-up observations. Many of these will be implemented within the broker and marshal systems that receive the LSST alert stream; depending on their level of resource-intensiveness, broker/marshal computing capacity may need to be further augmented, and/or integrated with elastic or preemptive compute allocations within research computing facilities or in commercial cloud.

\subsubsection{Computing Resource Providers}

In the US, DESC computing needs are primarily supported by \acrshort{doe} through its network of computing facilities under the \acrfull{ascr} program within the DOE Office of Science. This includes NERSC as the primary user facility for DESC science analysis, \acrfull{esnet} for advanced wide-area networking and data movement, and the \acrshort{alcf} and \acrshort{olcf} for larger-scale computational work. Recognizing the need for a facility dedicated to data-intensive computing, ASCR is also supporting the early stage of development for a future \acrshort{hpdf}. DOE is also advancing the development of an \acrfull{iri} to support flexible, powerful, and accessible implementation of scientific workflows across all these facilities.

Anticipating an increasingly prominent role for AI in the scientific exploitation of data created by Rubin and other DOE-funded facilities across all disciplines, the US Congress has funded the creation of \acrshort{amsc}\footnote{\url{https://science.osti.gov/-/media/grants/pdf/lab-announcements/2025/LAB-25-3555.pdf}} to develop and deploy the next generation of AI-oriented capabilities on the foundation of the DOE computing platforms noted above. AmSC development is underway now, with funding distributed across an AmSC infrastructure component, a core AI model-development consortium, pilot funding for discipline-specific data curation and science benchmarking activities, and seed funding for discipline-specific AI model teams. While it is still in an early ramp-up phase, computing and AI model development infrastructure within the AmSC represent a significant opportunity to address ambitious DESC AI/ML goals.

The joint \acrshort{nsf}--DOE nature of Rubin Observatory opens the possibility of leveraging significant NSF-supported computing facility resources, especially if pursued in coordination with other Science Collaborations working in areas typically supported by NSF. These resources include the \acrfull{lccf} being constructed at the Texas Advanced Computing Center. DESC members also have connections to both astrophysics-oriented National Artificial Intelligence Research Institutes, funded jointly by NSF and the Simons Foundation (\acrshort{skai}, led by Northwestern University; and CosmicAI, led by the University of Texas at Austin), and to their associated computing resource allocations. Considering interests in Rubin--\textit{Roman}--\textit{Euclid} joint analysis, \acrfull{nasa}-funded computing resources may also be a viable option.

More broadly, many DESC members have access to significant campus-level computing at their institutions. Members outside the US have access to their own networks of national resources, including national and regional initiatives prioritizing computing for AI in science. Through the in-kind contribution program that supports LSST data rights for scientists outside of the US and Chile, a network of \acrshortpl{idac} is being deployed, with some sites bringing additional CPU and GPU capabilities that DESC would be well positioned to make use of. The UK will host an IDAC, sized to satisfy the resource needs of 20\% of the global LSST community during survey operations. The UK IDAC will be connected to UK national research computing facilities, both traditional (simulation and modeling) supercomputing services and the coming generation of AI-focused Digital Research infrastructure currently being specified and prototyped through the \acrshort{ascend} process\footnote{\acrlong{ascend}; see \url{https://engagementhub.ukri.org/ukri-infrastructure/ascend-process/}}.

Hyperscale commercial enterprises may also offer a viable path for certain novel and ambitious Rubin-LSST AI/ML applications, provided that their resources can be engaged through partnership or at significant discount. Potential partners include major cloud providers such as Google, Amazon, and Microsoft; major AI players such as OpenAI, Anthropic, and (again) Google; and GPU manufacturers such as NVIDIA and AMD. Additional effort would be required to develop private-sector partnerships that are mutually beneficial and compatible with the proprietary and non-commercial requirements of the Rubin--LSST data policy\footnote{\url{http://ls.st/RDO-013}}. On the positive side, timescales in industry are typically much shorter than in academia: work with an engaged hyperscale commercial partner could potentially deliver large-scale results quickly.

% environmental impact text moved to section 8 -- risks & challenges

\subsection{Data}

The primary data products relevant for \acrshort{desc} \acrshort{ai}/\acrshort{ml} work fall into several categories:
\begin{enumerate}
\item \acrshort{lsst} data products delivered by Rubin Observatory
\item Derived data products produced through DESC collaboration efforts
\item Data from other major surveys that enhance and expand DESC AI/ML science
\item Simulation data
\item Model weights and biases from AI models trained on the above
\end{enumerate}

Rubin-LSST data products are organized into three categories distinguished by the timescale of their delivery. The most immediate data are the world-public alert packets that will be distributed within minutes of shutter-close, including difference-image detections of transient and variable objects along with associated postage stamps and (for repeat detections) a 1-year time series. ``Prompt products'' will be released to the LSST data-rights community after 24 hours (for catalogs) and 80 hours (for full focal plane images). On a longer cadence, uniform reprocessing and coaddition across all epochs will deliver annual data releases of catalogs and images. The alert stream will be distributed via the network of LSST Community Brokers, while the prompt products and annual data releases will be accessible via the \acrshort{rsp} and also available for bulk download to DESC via the Rubin \acrshort{usdf} at \acrshort{slac}. A workflow based on the Rucio data management software has been implemented to mirror LSST data to \acrshort{nersc} from the USDF, and could be employed for staging data at \acrshort{alcf} and \acrshort{olcf} as well.

Other major data sets of interest for cross-match, co-analysis, and multi-modal FM training in conjunction with Rubin data include: space-based surveys such as \textit{Roman}, \textit{Euclid}, the \acrfull{spherex}, and the \acrfull{wise}; spectroscopic surveys such as the \acrfull{sdss}, \acrshort{desi}, and \acrshort{4most}; precursor imaging and time-domain surveys such as \acrshort{des}, the \acrfull{decals}, \acrfull{pan-starrs}, and \acrfull{ztf}; and \acrshort{cmb} data from facilities such as Planck, \acrshort{act}, the \acrfull{spt}, and \acrfull{so}.

Given the multi-petabyte size of the LSST data (and of simulations and other survey data sets on similar scales), both network transfer and disk storage will be limiting factors. DESC would likely benefit from strategic coordination with other LSST science collaborations, \acrshort{lincc} (see \autoref{sec7:broader_coordination}), and \acrshortpl{idac} regarding which LSST data products are mirrored where, for how long, at what quality of service, in conjunction with which \acrshort{cpu} and \acrshort{gpu} allocations, and for which analysis purposes.

The current Rubin Data Management system was not primarily designed for large-scale AI/ML work. Hence the data will need to be fitted with additional data interfaces, APIs, and standards that enable efficient use in this new context, such as the following examples:
\begin{itemize}
\item Adoption of tools like the Hyrax framework\footnote{\url{https://hyrax.readthedocs.io/en/stable/}} that provides modular components for a full AI/ML workflow tailored to astrophysical data.
\item Large-scale cross-match capabilities such as the \acrfull{hats}\footnote{\url{https://hats.readthedocs.io/en/latest/}} and Fink Xmatch\footnote{\url{https://fink-portal.org/xmatch}} that are critical to multi-modal dataset construction.
\item Performant and scalable services for streaming large batches of image cutouts into AI/ML training and inference workflows.
\item Data tokenization and embedding strategies that are well matched to AI/ML model architectures and downstream science tasks.
\item Active learning frameworks for alerts and images that maximize the value of limited human expert labeling time with respect to relevant modeling objectives.
\end{itemize}
In some cases, standards developed by the International Virtual Observatory Alliance\footnote{\url{https://www.ivoa.net}} may be fit for these purposes in DESC although they are typically conceived around classical astronomy use cases that do not map onto survey-scale AI/ML.

\subsection{Benchmarking and Reproducibility}

Challenges of reproducibility will only increase as \acrshort{ai}/\acrshort{ml}-based analyses become more common. Full computational reproducibility requires infrastructure for versioned retention of all input data, any pre-trained models used for inference, all analysis code and frameworks, and all software and environment dependencies. Solutions that allow for some or all of the above elements to be mutable over time fall short of true reproducibility but may be acceptable (or even preferable) to the extent that significant scientific conclusions remain replicable.

Additional challenges posed by the increasing adoption of AI/ML methods include:
\begin{itemize}
\item Defining the role and framework of blind analysis.
\item Maintaining independence of different experiments in the context of multi-modal FM-based analyses.
\item Defining training, validation, and test data sets when we ultimately want to use the full data set for cosmological inference.
\item Ensuring that any agentic software used or developed to generate \acrshort{desc} science records the provenance of the operations done within its task in ways that are compatible with scientific standards of reproducibility (explainable AI).
\end{itemize}

While the above are primarily questions of methodology rather than infrastructure, their solutions will have implications for infrastructure requirements. Some of the required infrastructure elements may include
\begin{itemize}
\item Persistent, accessible storage for testbed datasets, SBI training simulations, and pretrained model weights.
\item Hosted frameworks for deploying and running against science benchmarks.
\item Minified production environments that facilitate use of small-scale development instances to develop methods and benchmarks.
\item Standardized \acrshortpl{api} and architectures for reproducible large-scale model training and deployment.
\item Agentic frameworks for analysis reproduction \citep[e.g.,][]{ye2025replicationbench}
\item Comprehensive provenance tracking \& support for long-term co-archiving of data and analysis.
\end{itemize}

Our traditional thinking about reproducibility will be further challenged if \acrshortpl{llm} and AI agents continue to move us toward a more natural-language approach to dynamically and interactively extracting understanding from data. This trend can bring about a shift away from the traditionally sequential and siloed model of ``data, pipeline, results as papers'' and toward a more dynamic and interactive world characterized by prompts such as ``I want to regenerate the plot in Figure 1 of so-and-so's paper, except I want the magnitude cut at $g=22$ rather than $g=23$\dots'' Other disciplines will be experiencing similar transitions and DESC should look for opportunities both to lead by example and to benefit from broader trends and investment.

% Infrastructure text from 
The vast data scale of \acrshort{lsst} has necessitated a fundamental evolution in scientific methodology. This paradigm shift moves from local data processing on individual researchers' computers toward analytical tools designed to operate on shared, high-performance compute platforms. This centralization, combined with the anticipated widespread adoption of general-purpose foundation models, compels the establishment of a common implementation framework to ensure scientific analyses are reproducible, consistent, and interoperable.

While the \acrshort{rsp} serves as the primary portal for LSST data access, it was not designed for resource-intensive, large-scale AI applications. To bridge this gap, the Hyrax framework provides a  unified platform for exploring the latent space of foundation models. However, their operational deployment remains a significant undertaking that requires dedicated computational resources and a community-governed process for selection and validation.

The rapid pace of innovation in ML means that \acrshortpl{fm} cannot be considered static, long-term solutions. They must be constructed for a dynamic life cycle that includes systematic processes for review, evaluation, and replacement. This agile strategy is critical for all modalities, especially for time-series models deployed on real-time alert brokers, which must reliably process transient astronomical events to enable rapid discovery.

%
% To ensure a robust and sustainable AI ecosystem for Rubin science, future development must adhere to three core principles:
%  \begin{enumerate}
%      \item Deployment Compatibility: New foundation models must be explicitly designed for deployment within Hyrax and/or relevant real-time alert brokers.

%      \item Reproducibility: The model, its training data, and all relevant code must be versioned and archived to permit the complete replication of any scientific analysis (***point to where this is discussed in infrastructure***).
     
%      \item \note{What is the third core principle ?}
% \end{enumerate}



% This sub-section is commented out for now, can be added back later with more content
%
% \subsection{Human Support}
% \note{please add content here if you have thoughts on this aspect, we might remove this subsection if not enough content}
% \textit{This can include both the human efforts needed to maintain the other infrastructure (software, computing, data, reproducibility), and also the human oversight that is needed for vetting AI uses and policy development etc.}


%\subsection{Hyrax Text-Dump}
%\label{sec:hyrax}
%Wider community-level adoption of advanced ML techniques in astronomy is often hampered by fragmented, project-specific infrastructure where each group develops bespoke pipelines, duplicating effort and limiting reproducibility. For foundation models and unsupervised learning specifically, standardized frameworks are needed for systematic comparison across architectures and for exploring the high-dimensional latent spaces they produce.
%The Hyrax framework\footnote{\url{https://hyrax.readthedocs.io/en/stable/}} (Ghosh et al., in prep) attempts to address these gaps by providing modular, astronomy-tailored components for the full ML workflow. While supporting both supervised and unsupervised learning across diverse applications, it includes specialized tools particularly relevant for foundation models: interactive 2D/3D latent space visualization linked to source data, and vector-database similarity search scaling to billions of objects. Users can adopt community-produced foundation models, compare performance on standardized benchmarks, and perform downstream fine-tuning - or deploy entirely custom models using the same infrastructure. Integration with MLFlow, TensorBoard and Optuna enables systematic model comparison, hyperparameter tuning, and experiment tracking. 

%For Rubin, Hyrax provides native Butler integration enabling on-the-fly image cutout generation without local storage, and can be deployed on the RSP or any platform with Butler access (e.g., USDF, IDACs). Combined with support for scaling from laptop prototypes to multi-node HPC environments, Hyrax is positioned as a key enabling technology for wider community-wide adoption of AI within DESC. However, realizing this vision will require coordinated and sustained investment in specialized software engineering talent -- embedded within LSST-DM, LINCC, and DESC-DM -- to ensure tight integration between training/visualization/exploration infrastructure and the LSST stack. Equally critical is dedicated computing infrastructure co-located with LSST data systems and focused specifically on training, deploying, and using LSST-scale ML models. These investments are essential both for developing models on petabyte-scale datasets and for lowering adoption barriers by providing researchers seamless access to models and data through familiar LSST interfaces.

